{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'cloudspace (Python 3.10.10)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/defaults.yaml \\\n",
    "model=visual_bert \\\n",
    "dataset=hateful_memes \\\n",
    "run_type=train_val training.batch_size=2 training.log_interval=1 training.wandb.enabled=True training.max_updates=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a pretrained visual bert model on COCO and evaluate the validation set on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/vikrant/.cache/torch/mmf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikrant17\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_roberta/defaults.yaml\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_roberta\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 10\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option training.seed to 2024\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 80\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 100\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option training.experiment_name to hateful_memes_lr3e5\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 2000\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 3e-5\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option training.fp16 to True\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 200\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option training.wandb.enabled to True\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_roberta/defaults.yaml', 'model=visual_roberta', 'dataset=hateful_memes', 'training.log_interval=10', 'training.seed=2024', 'training.batch_size=80', 'training.evaluation_interval=100', 'training.experiment_name=hateful_memes_lr3e5', 'training.max_updates=2000', 'optimizer.params.lr=3e-5', 'training.fp16=True', 'scheduler.params.num_warmup_steps=200', 'checkpoint.max_to_keep=1', 'training.wandb.enabled=True', 'run_type=train_val'])\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf_cli.run: \u001b[0mTorch version: 1.11.0+cu102\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf_cli.run: \u001b[0mUsing seed 2024\n",
      "\u001b[32m2024-07-23T07:52:26 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/zeus/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/zeus/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/zeus/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at /home/zeus/.cache/huggingface/transformers/dfe8f1ad04cb25b61a647e3d13620f9bf0a0f51d277897b232a5735297134132.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\u001b[32m2024-07-23T07:52:27 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T07:52:27 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T07:52:27 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T07:52:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2024-07-23T07:52:27 | torch.distributed.nn.jit.instantiator: \u001b[0mCreated a temporary directory at /tmp/tmpe1rgpj_h\n",
      "\u001b[32m2024-07-23T07:52:27 | torch.distributed.nn.jit.instantiator: \u001b[0mWriting /tmp/tmpe1rgpj_h/_remote_module_non_sriptable.py\n",
      "HI BRO {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': ['cross_entropy'], 'model': 'visual_roberta'}\n",
      "SELFIE {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': ['cross_entropy'], 'model': 'visual_roberta'}\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "SELF ROBERT CONFIG RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "roberta-base\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/zeus/.cache/torch/mmf/distributed_-1/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VisualRobertaBase: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing VisualRobertaBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualRobertaBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualRobertaBase were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.projection.bias', 'roberta.embeddings.token_type_embeddings_visual.weight', 'roberta.embeddings.position_ids', 'roberta.embeddings.position_embeddings_visual.weight', 'roberta.embeddings.projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-07-23T07:52:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2024-07-23T07:52:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikrant17\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/wandb/run-20240723_075234-4b9cv470\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhateful_memes_lr3e5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/4b9cv470\u001b[0m\n",
      "\u001b[32m2024-07-23T07:52:37 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2024-07-23T07:52:37 | mmf.trainers.mmf_trainer: \u001b[0mVisualRoberta(\n",
      "  (model): VisualRobertaForClassification(\n",
      "    (roberta): VisualRobertaBase(\n",
      "      (embeddings): RobertaVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(1, 768)\n",
      "        (position_embeddings_visual): Embedding(514, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2024-07-23T07:52:37 | mmf.utils.general: \u001b[0mTotal Parameters: 127208450. Trained Parameters: 127208450\n",
      "\u001b[32m2024-07-23T07:52:37 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2024-07-23T07:52:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10/2000, train/hateful_memes/cross_entropy: 0.7570, train/hateful_memes/cross_entropy/avg: 0.7570, train/total_loss: 0.7570, train/total_loss/avg: 0.7570, max mem: 13359.0, experiment: hateful_memes_lr3e5, epoch: 1, num_updates: 10, iterations: 10, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 772ms, time_since_start: 17s 900ms, eta: 48m 52s 492ms\n",
      "\u001b[32m2024-07-23T07:53:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20/2000, train/hateful_memes/cross_entropy: 0.6526, train/hateful_memes/cross_entropy/avg: 0.7048, train/total_loss: 0.6526, train/total_loss/avg: 0.7048, max mem: 13359.0, experiment: hateful_memes_lr3e5, epoch: 1, num_updates: 20, iterations: 20, max_updates: 2000, lr: 0., ups: 0.91, time: 11s 837ms, time_since_start: 29s 737ms, eta: 41m 47s 845ms\n",
      "\u001b[32m2024-07-23T07:53:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 30/2000, train/hateful_memes/cross_entropy: 0.6857, train/hateful_memes/cross_entropy/avg: 0.6984, train/total_loss: 0.6857, train/total_loss/avg: 0.6984, max mem: 13359.0, experiment: hateful_memes_lr3e5, epoch: 1, num_updates: 30, iterations: 30, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 224ms, time_since_start: 42s 961ms, eta: 46m 27s 494ms\n",
      "\u001b[32m2024-07-23T07:53:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 40/2000, train/hateful_memes/cross_entropy: 0.6722, train/hateful_memes/cross_entropy/avg: 0.6919, train/total_loss: 0.6722, train/total_loss/avg: 0.6919, max mem: 13359.0, experiment: hateful_memes_lr3e5, epoch: 1, num_updates: 40, iterations: 40, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 168ms, time_since_start: 55s 130ms, eta: 42m 32s 058ms\n",
      "\u001b[32m2024-07-23T07:53:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/2000, train/hateful_memes/cross_entropy: 0.6722, train/hateful_memes/cross_entropy/avg: 0.6749, train/total_loss: 0.6722, train/total_loss/avg: 0.6749, max mem: 13359.0, experiment: hateful_memes_lr3e5, epoch: 1, num_updates: 50, iterations: 50, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 197ms, time_since_start: 01m 08s 327ms, eta: 45m 53s 650ms\n",
      "\u001b[32m2024-07-23T07:53:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 60/2000, train/hateful_memes/cross_entropy: 0.6526, train/hateful_memes/cross_entropy/avg: 0.6657, train/total_loss: 0.6526, train/total_loss/avg: 0.6657, max mem: 13359.0, experiment: hateful_memes_lr3e5, epoch: 1, num_updates: 60, iterations: 60, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 486ms, time_since_start: 01m 20s 814ms, eta: 43m 11s 922ms\n",
      "\u001b[32m2024-07-23T07:54:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 70/2000, train/hateful_memes/cross_entropy: 0.6722, train/hateful_memes/cross_entropy/avg: 0.6685, train/total_loss: 0.6722, train/total_loss/avg: 0.6685, max mem: 13359.0, experiment: hateful_memes_lr3e5, epoch: 1, num_updates: 70, iterations: 70, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 070ms, time_since_start: 01m 33s 884ms, eta: 44m 59s 195ms\n",
      "\u001b[32m2024-07-23T07:54:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 80/2000, train/hateful_memes/cross_entropy: 0.6526, train/hateful_memes/cross_entropy/avg: 0.6644, train/total_loss: 0.6526, train/total_loss/avg: 0.6644, max mem: 13359.0, experiment: hateful_memes_lr3e5, epoch: 1, num_updates: 80, iterations: 80, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 237ms, time_since_start: 01m 46s 121ms, eta: 41m 54s 018ms\n",
      "\u001b[32m2024-07-23T07:54:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 90/2000, train/hateful_memes/cross_entropy: 0.6722, train/hateful_memes/cross_entropy/avg: 0.6666, train/total_loss: 0.6722, train/total_loss/avg: 0.6666, max mem: 13359.0, experiment: hateful_memes_lr3e5, epoch: 1, num_updates: 90, iterations: 90, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 385ms, time_since_start: 01m 59s 507ms, eta: 45m 35s 576ms\n",
      "\u001b[32m2024-07-23T07:54:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, train/hateful_memes/cross_entropy: 0.6526, train/hateful_memes/cross_entropy/avg: 0.6642, train/total_loss: 0.6526, train/total_loss/avg: 0.6642, max mem: 13359.0, experiment: hateful_memes_lr3e5, epoch: 1, num_updates: 100, iterations: 100, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 124ms, time_since_start: 02m 11s 631ms, eta: 41m 04s 884ms\n",
      "\u001b[32m2024-07-23T07:54:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T07:54:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T07:54:53 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T07:54:53 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T07:54:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T07:54:57 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T07:55:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T07:55:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T07:55:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, val/hateful_memes/cross_entropy: 0.7590, val/total_loss: 0.7590, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5368, num_updates: 100, epoch: 1, iterations: 100, max_updates: 2000, val_time: 35s 137ms, best_update: 100, best_iteration: 100, best_val/hateful_memes/roc_auc: 0.536800\n",
      "\u001b[32m2024-07-23T07:55:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 110/2000, train/hateful_memes/cross_entropy: 0.6526, train/hateful_memes/cross_entropy/avg: 0.6603, train/total_loss: 0.6526, train/total_loss/avg: 0.6603, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 2, num_updates: 110, iterations: 110, max_updates: 2000, lr: 0.00002, ups: 0.91, time: 11s 789ms, time_since_start: 02m 58s 566ms, eta: 39m 44s 276ms\n",
      "\u001b[32m2024-07-23T07:55:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 120/2000, train/hateful_memes/cross_entropy: 0.6526, train/hateful_memes/cross_entropy/avg: 0.6645, train/total_loss: 0.6526, train/total_loss/avg: 0.6645, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 2, num_updates: 120, iterations: 120, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 338ms, time_since_start: 03m 10s 905ms, eta: 41m 22s 105ms\n",
      "\u001b[32m2024-07-23T07:55:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 130/2000, train/hateful_memes/cross_entropy: 0.6526, train/hateful_memes/cross_entropy/avg: 0.6608, train/total_loss: 0.6526, train/total_loss/avg: 0.6608, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 2, num_updates: 130, iterations: 130, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 469ms, time_since_start: 03m 23s 374ms, eta: 41m 34s 946ms\n",
      "\u001b[32m2024-07-23T07:56:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 140/2000, train/hateful_memes/cross_entropy: 0.6425, train/hateful_memes/cross_entropy/avg: 0.6533, train/total_loss: 0.6425, train/total_loss/avg: 0.6533, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 2, num_updates: 140, iterations: 140, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 542ms, time_since_start: 03m 35s 916ms, eta: 41m 36s 129ms\n",
      "\u001b[32m2024-07-23T07:56:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/2000, train/hateful_memes/cross_entropy: 0.6425, train/hateful_memes/cross_entropy/avg: 0.6525, train/total_loss: 0.6425, train/total_loss/avg: 0.6525, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 2, num_updates: 150, iterations: 150, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 504ms, time_since_start: 03m 48s 421ms, eta: 41m 15s 353ms\n",
      "\u001b[32m2024-07-23T07:56:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 160/2000, train/hateful_memes/cross_entropy: 0.6421, train/hateful_memes/cross_entropy/avg: 0.6465, train/total_loss: 0.6421, train/total_loss/avg: 0.6465, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 2, num_updates: 160, iterations: 160, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 595ms, time_since_start: 04m 01s 017ms, eta: 41m 19s 773ms\n",
      "\u001b[32m2024-07-23T07:56:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 170/2000, train/hateful_memes/cross_entropy: 0.6421, train/hateful_memes/cross_entropy/avg: 0.6429, train/total_loss: 0.6421, train/total_loss/avg: 0.6429, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 2, num_updates: 170, iterations: 170, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 526ms, time_since_start: 04m 13s 543ms, eta: 40m 52s 754ms\n",
      "\u001b[32m2024-07-23T07:56:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 180/2000, train/hateful_memes/cross_entropy: 0.6351, train/hateful_memes/cross_entropy/avg: 0.6375, train/total_loss: 0.6351, train/total_loss/avg: 0.6375, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 2, num_updates: 180, iterations: 180, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 577ms, time_since_start: 04m 26s 120ms, eta: 40m 49s 364ms\n",
      "\u001b[32m2024-07-23T07:57:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 190/2000, train/hateful_memes/cross_entropy: 0.6351, train/hateful_memes/cross_entropy/avg: 0.6369, train/total_loss: 0.6351, train/total_loss/avg: 0.6369, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 2, num_updates: 190, iterations: 190, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 515ms, time_since_start: 04m 38s 636ms, eta: 40m 23s 923ms\n",
      "\u001b[32m2024-07-23T07:57:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, train/hateful_memes/cross_entropy: 0.6268, train/hateful_memes/cross_entropy/avg: 0.6350, train/total_loss: 0.6268, train/total_loss/avg: 0.6350, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 2, num_updates: 200, iterations: 200, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 622ms, time_since_start: 04m 51s 259ms, eta: 40m 31s 182ms\n",
      "\u001b[32m2024-07-23T07:57:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T07:57:24 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T07:57:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T07:57:32 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T07:57:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T07:57:41 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T07:57:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T07:57:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T07:57:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, val/hateful_memes/cross_entropy: 0.7884, val/total_loss: 0.7884, val/hateful_memes/accuracy: 0.5340, val/hateful_memes/binary_f1: 0.2508, val/hateful_memes/roc_auc: 0.6044, num_updates: 200, epoch: 2, iterations: 200, max_updates: 2000, val_time: 31s 148ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.604352\n",
      "\u001b[32m2024-07-23T07:58:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 210/2000, train/hateful_memes/cross_entropy: 0.6219, train/hateful_memes/cross_entropy/avg: 0.6326, train/total_loss: 0.6219, train/total_loss/avg: 0.6326, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 2, num_updates: 210, iterations: 210, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 419ms, time_since_start: 05m 34s 834ms, eta: 39m 38s 618ms\n",
      "\u001b[32m2024-07-23T07:58:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 220/2000, train/hateful_memes/cross_entropy: 0.6198, train/hateful_memes/cross_entropy/avg: 0.6294, train/total_loss: 0.6198, train/total_loss/avg: 0.6294, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 3, num_updates: 220, iterations: 220, max_updates: 2000, lr: 0.00003, ups: 0.91, time: 11s 203ms, time_since_start: 05m 46s 037ms, eta: 35m 33s 767ms\n",
      "\u001b[32m2024-07-23T07:58:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 230/2000, train/hateful_memes/cross_entropy: 0.6198, train/hateful_memes/cross_entropy/avg: 0.6300, train/total_loss: 0.6198, train/total_loss/avg: 0.6300, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 3, num_updates: 230, iterations: 230, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 553ms, time_since_start: 05m 58s 591ms, eta: 39m 37s 578ms\n",
      "\u001b[32m2024-07-23T07:58:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 240/2000, train/hateful_memes/cross_entropy: 0.6167, train/hateful_memes/cross_entropy/avg: 0.6262, train/total_loss: 0.6167, train/total_loss/avg: 0.6262, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 3, num_updates: 240, iterations: 240, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 458ms, time_since_start: 06m 11s 050ms, eta: 39m 06s 210ms\n",
      "\u001b[32m2024-07-23T07:58:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/2000, train/hateful_memes/cross_entropy: 0.6198, train/hateful_memes/cross_entropy/avg: 0.6266, train/total_loss: 0.6198, train/total_loss/avg: 0.6266, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 3, num_updates: 250, iterations: 250, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 219ms, time_since_start: 06m 23s 269ms, eta: 38m 08s 133ms\n",
      "\u001b[32m2024-07-23T07:59:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 260/2000, train/hateful_memes/cross_entropy: 0.6167, train/hateful_memes/cross_entropy/avg: 0.6251, train/total_loss: 0.6167, train/total_loss/avg: 0.6251, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 3, num_updates: 260, iterations: 260, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 614ms, time_since_start: 06m 35s 884ms, eta: 39m 08s 534ms\n",
      "\u001b[32m2024-07-23T07:59:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 270/2000, train/hateful_memes/cross_entropy: 0.5991, train/hateful_memes/cross_entropy/avg: 0.6210, train/total_loss: 0.5991, train/total_loss/avg: 0.6210, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 3, num_updates: 270, iterations: 270, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 247ms, time_since_start: 06m 48s 132ms, eta: 37m 47s 229ms\n",
      "\u001b[32m2024-07-23T07:59:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 280/2000, train/hateful_memes/cross_entropy: 0.5881, train/hateful_memes/cross_entropy/avg: 0.6154, train/total_loss: 0.5881, train/total_loss/avg: 0.6154, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 3, num_updates: 280, iterations: 280, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 445ms, time_since_start: 07m 577ms, eta: 38m 10s 418ms\n",
      "\u001b[32m2024-07-23T07:59:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 290/2000, train/hateful_memes/cross_entropy: 0.5861, train/hateful_memes/cross_entropy/avg: 0.6129, train/total_loss: 0.5861, train/total_loss/avg: 0.6129, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 3, num_updates: 290, iterations: 290, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 156ms, time_since_start: 07m 12s 734ms, eta: 37m 04s 337ms\n",
      "\u001b[32m2024-07-23T07:59:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, train/hateful_memes/cross_entropy: 0.5845, train/hateful_memes/cross_entropy/avg: 0.6074, train/total_loss: 0.5845, train/total_loss/avg: 0.6074, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 3, num_updates: 300, iterations: 300, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 571ms, time_since_start: 07m 25s 305ms, eta: 38m 06s 676ms\n",
      "\u001b[32m2024-07-23T07:59:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T07:59:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:00:06 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:00:06 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:00:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:00:10 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T08:00:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:00:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:00:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, val/hateful_memes/cross_entropy: 0.8619, val/total_loss: 0.8619, val/hateful_memes/accuracy: 0.5540, val/hateful_memes/binary_f1: 0.2965, val/hateful_memes/roc_auc: 0.6422, num_updates: 300, epoch: 3, iterations: 300, max_updates: 2000, val_time: 33s 365ms, best_update: 300, best_iteration: 300, best_val/hateful_memes/roc_auc: 0.642208\n",
      "\u001b[32m2024-07-23T08:00:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 310/2000, train/hateful_memes/cross_entropy: 0.5625, train/hateful_memes/cross_entropy/avg: 0.6053, train/total_loss: 0.5625, train/total_loss/avg: 0.6053, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 3, num_updates: 310, iterations: 310, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 879ms, time_since_start: 08m 11s 558ms, eta: 38m 48s 993ms\n",
      "\u001b[32m2024-07-23T08:00:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 320/2000, train/hateful_memes/cross_entropy: 0.5558, train/hateful_memes/cross_entropy/avg: 0.6013, train/total_loss: 0.5558, train/total_loss/avg: 0.6013, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 3, num_updates: 320, iterations: 320, max_updates: 2000, lr: 0.00003, ups: 0.91, time: 11s 753ms, time_since_start: 08m 23s 311ms, eta: 35m 12s 838ms\n",
      "\u001b[32m2024-07-23T08:01:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 330/2000, train/hateful_memes/cross_entropy: 0.5557, train/hateful_memes/cross_entropy/avg: 0.5986, train/total_loss: 0.5557, train/total_loss/avg: 0.5986, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 4, num_updates: 330, iterations: 330, max_updates: 2000, lr: 0.00003, ups: 0.91, time: 11s 603ms, time_since_start: 08m 34s 916ms, eta: 34m 33s 520ms\n",
      "\u001b[32m2024-07-23T08:01:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 340/2000, train/hateful_memes/cross_entropy: 0.5558, train/hateful_memes/cross_entropy/avg: 0.5989, train/total_loss: 0.5558, train/total_loss/avg: 0.5989, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 4, num_updates: 340, iterations: 340, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 001ms, time_since_start: 08m 46s 917ms, eta: 35m 31s 633ms\n",
      "\u001b[32m2024-07-23T08:01:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/2000, train/hateful_memes/cross_entropy: 0.5558, train/hateful_memes/cross_entropy/avg: 0.5984, train/total_loss: 0.5558, train/total_loss/avg: 0.5984, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 4, num_updates: 350, iterations: 350, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 289ms, time_since_start: 08m 59s 206ms, eta: 36m 09s 680ms\n",
      "\u001b[32m2024-07-23T08:01:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 360/2000, train/hateful_memes/cross_entropy: 0.5444, train/hateful_memes/cross_entropy/avg: 0.5957, train/total_loss: 0.5444, train/total_loss/avg: 0.5957, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 4, num_updates: 360, iterations: 360, max_updates: 2000, lr: 0.00003, ups: 0.91, time: 11s 949ms, time_since_start: 09m 11s 155ms, eta: 34m 56s 877ms\n",
      "\u001b[32m2024-07-23T08:01:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 370/2000, train/hateful_memes/cross_entropy: 0.5430, train/hateful_memes/cross_entropy/avg: 0.5922, train/total_loss: 0.5430, train/total_loss/avg: 0.5922, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 4, num_updates: 370, iterations: 370, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 365ms, time_since_start: 09m 23s 521ms, eta: 35m 56s 678ms\n",
      "\u001b[32m2024-07-23T08:02:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 380/2000, train/hateful_memes/cross_entropy: 0.5420, train/hateful_memes/cross_entropy/avg: 0.5890, train/total_loss: 0.5420, train/total_loss/avg: 0.5890, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 4, num_updates: 380, iterations: 380, max_updates: 2000, lr: 0.00003, ups: 0.91, time: 11s 983ms, time_since_start: 09m 35s 505ms, eta: 34m 37s 233ms\n",
      "\u001b[32m2024-07-23T08:02:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 390/2000, train/hateful_memes/cross_entropy: 0.5392, train/hateful_memes/cross_entropy/avg: 0.5856, train/total_loss: 0.5392, train/total_loss/avg: 0.5856, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 4, num_updates: 390, iterations: 390, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 343ms, time_since_start: 09m 47s 848ms, eta: 35m 26s 343ms\n",
      "\u001b[32m2024-07-23T08:02:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, train/hateful_memes/cross_entropy: 0.5183, train/hateful_memes/cross_entropy/avg: 0.5839, train/total_loss: 0.5183, train/total_loss/avg: 0.5839, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 4, num_updates: 400, iterations: 400, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 000ms, time_since_start: 09m 59s 848ms, eta: 34m 14s 538ms\n",
      "\u001b[32m2024-07-23T08:02:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:02:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:02:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:02:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:02:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:02:45 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T08:02:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:03:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:03:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, val/hateful_memes/cross_entropy: 0.7427, val/total_loss: 0.7427, val/hateful_memes/accuracy: 0.5920, val/hateful_memes/binary_f1: 0.4688, val/hateful_memes/roc_auc: 0.6750, num_updates: 400, epoch: 4, iterations: 400, max_updates: 2000, val_time: 34s 330ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.674960\n",
      "\u001b[32m2024-07-23T08:03:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 410/2000, train/hateful_memes/cross_entropy: 0.5133, train/hateful_memes/cross_entropy/avg: 0.5788, train/total_loss: 0.5133, train/total_loss/avg: 0.5788, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 4, num_updates: 410, iterations: 410, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 086ms, time_since_start: 10m 47s 273ms, eta: 37m 06s 418ms\n",
      "\u001b[32m2024-07-23T08:03:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 420/2000, train/hateful_memes/cross_entropy: 0.5125, train/hateful_memes/cross_entropy/avg: 0.5747, train/total_loss: 0.5125, train/total_loss/avg: 0.5747, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 4, num_updates: 420, iterations: 420, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 202ms, time_since_start: 10m 59s 476ms, eta: 34m 22s 975ms\n",
      "\u001b[32m2024-07-23T08:03:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 430/2000, train/hateful_memes/cross_entropy: 0.5045, train/hateful_memes/cross_entropy/avg: 0.5706, train/total_loss: 0.5045, train/total_loss/avg: 0.5706, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 5, num_updates: 430, iterations: 430, max_updates: 2000, lr: 0.00003, ups: 0.91, time: 11s 649ms, time_since_start: 11m 11s 125ms, eta: 32m 37s 035ms\n",
      "\u001b[32m2024-07-23T08:03:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 440/2000, train/hateful_memes/cross_entropy: 0.4773, train/hateful_memes/cross_entropy/avg: 0.5680, train/total_loss: 0.4773, train/total_loss/avg: 0.5680, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 5, num_updates: 440, iterations: 440, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 193ms, time_since_start: 11m 23s 319ms, eta: 33m 55s 370ms\n",
      "\u001b[32m2024-07-23T08:04:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/2000, train/hateful_memes/cross_entropy: 0.4678, train/hateful_memes/cross_entropy/avg: 0.5643, train/total_loss: 0.4678, train/total_loss/avg: 0.5643, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 5, num_updates: 450, iterations: 450, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 023ms, time_since_start: 11m 36s 343ms, eta: 35m 59s 965ms\n",
      "\u001b[32m2024-07-23T08:04:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 460/2000, train/hateful_memes/cross_entropy: 0.4658, train/hateful_memes/cross_entropy/avg: 0.5602, train/total_loss: 0.4658, train/total_loss/avg: 0.5602, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 5, num_updates: 460, iterations: 460, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 122ms, time_since_start: 11m 48s 465ms, eta: 33m 17s 525ms\n",
      "\u001b[32m2024-07-23T08:04:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 470/2000, train/hateful_memes/cross_entropy: 0.4647, train/hateful_memes/cross_entropy/avg: 0.5581, train/total_loss: 0.4647, train/total_loss/avg: 0.5581, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 5, num_updates: 470, iterations: 470, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 010ms, time_since_start: 12m 01s 476ms, eta: 35m 30s 007ms\n",
      "\u001b[32m2024-07-23T08:04:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 480/2000, train/hateful_memes/cross_entropy: 0.4639, train/hateful_memes/cross_entropy/avg: 0.5550, train/total_loss: 0.4639, train/total_loss/avg: 0.5550, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 5, num_updates: 480, iterations: 480, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 193ms, time_since_start: 12m 13s 670ms, eta: 33m 03s 141ms\n",
      "\u001b[32m2024-07-23T08:05:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 490/2000, train/hateful_memes/cross_entropy: 0.4567, train/hateful_memes/cross_entropy/avg: 0.5517, train/total_loss: 0.4567, train/total_loss/avg: 0.5517, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 5, num_updates: 490, iterations: 490, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 038ms, time_since_start: 12m 26s 708ms, eta: 35m 06s 575ms\n",
      "\u001b[32m2024-07-23T08:05:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, train/hateful_memes/cross_entropy: 0.4567, train/hateful_memes/cross_entropy/avg: 0.5482, train/total_loss: 0.4567, train/total_loss/avg: 0.5482, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 5, num_updates: 500, iterations: 500, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 259ms, time_since_start: 12m 38s 968ms, eta: 32m 47s 724ms\n",
      "\u001b[32m2024-07-23T08:05:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:05:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:05:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:05:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:05:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:05:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:05:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:05:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, val/hateful_memes/cross_entropy: 0.8126, val/total_loss: 0.8126, val/hateful_memes/accuracy: 0.5700, val/hateful_memes/binary_f1: 0.4267, val/hateful_memes/roc_auc: 0.6600, num_updates: 500, epoch: 5, iterations: 500, max_updates: 2000, val_time: 21s 331ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.674960\n",
      "\u001b[32m2024-07-23T08:05:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 510/2000, train/hateful_memes/cross_entropy: 0.4562, train/hateful_memes/cross_entropy/avg: 0.5460, train/total_loss: 0.4562, train/total_loss/avg: 0.5460, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 5, num_updates: 510, iterations: 510, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 280ms, time_since_start: 13m 13s 587ms, eta: 35m 17s 240ms\n",
      "\u001b[32m2024-07-23T08:05:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 520/2000, train/hateful_memes/cross_entropy: 0.4366, train/hateful_memes/cross_entropy/avg: 0.5438, train/total_loss: 0.4366, train/total_loss/avg: 0.5438, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 5, num_updates: 520, iterations: 520, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 312ms, time_since_start: 13m 25s 899ms, eta: 32m 29s 738ms\n",
      "\u001b[32m2024-07-23T08:06:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 530/2000, train/hateful_memes/cross_entropy: 0.4366, train/hateful_memes/cross_entropy/avg: 0.5426, train/total_loss: 0.4366, train/total_loss/avg: 0.5426, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 5, num_updates: 530, iterations: 530, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 809ms, time_since_start: 13m 38s 709ms, eta: 33m 34s 812ms\n",
      "\u001b[32m2024-07-23T08:06:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 540/2000, train/hateful_memes/cross_entropy: 0.4317, train/hateful_memes/cross_entropy/avg: 0.5397, train/total_loss: 0.4317, train/total_loss/avg: 0.5397, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 6, num_updates: 540, iterations: 540, max_updates: 2000, lr: 0.00002, ups: 1.00, time: 10s 968ms, time_since_start: 13m 49s 677ms, eta: 28m 33s 511ms\n",
      "\u001b[32m2024-07-23T08:06:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/2000, train/hateful_memes/cross_entropy: 0.4092, train/hateful_memes/cross_entropy/avg: 0.5364, train/total_loss: 0.4092, train/total_loss/avg: 0.5364, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 6, num_updates: 550, iterations: 550, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 695ms, time_since_start: 14m 02s 372ms, eta: 32m 49s 657ms\n",
      "\u001b[32m2024-07-23T08:06:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 560/2000, train/hateful_memes/cross_entropy: 0.4082, train/hateful_memes/cross_entropy/avg: 0.5328, train/total_loss: 0.4082, train/total_loss/avg: 0.5328, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 6, num_updates: 560, iterations: 560, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 238ms, time_since_start: 14m 14s 611ms, eta: 31m 25s 728ms\n",
      "\u001b[32m2024-07-23T08:07:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 570/2000, train/hateful_memes/cross_entropy: 0.4082, train/hateful_memes/cross_entropy/avg: 0.5310, train/total_loss: 0.4082, train/total_loss/avg: 0.5310, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 6, num_updates: 570, iterations: 570, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 662ms, time_since_start: 14m 27s 274ms, eta: 32m 17s 493ms\n",
      "\u001b[32m2024-07-23T08:07:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 580/2000, train/hateful_memes/cross_entropy: 0.4009, train/hateful_memes/cross_entropy/avg: 0.5276, train/total_loss: 0.4009, train/total_loss/avg: 0.5276, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 6, num_updates: 580, iterations: 580, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 319ms, time_since_start: 14m 39s 593ms, eta: 31m 11s 809ms\n",
      "\u001b[32m2024-07-23T08:07:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 590/2000, train/hateful_memes/cross_entropy: 0.3987, train/hateful_memes/cross_entropy/avg: 0.5250, train/total_loss: 0.3987, train/total_loss/avg: 0.5250, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 6, num_updates: 590, iterations: 590, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 651ms, time_since_start: 14m 52s 244ms, eta: 31m 48s 712ms\n",
      "\u001b[32m2024-07-23T08:07:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, train/hateful_memes/cross_entropy: 0.3987, train/hateful_memes/cross_entropy/avg: 0.5246, train/total_loss: 0.3987, train/total_loss/avg: 0.5246, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 6, num_updates: 600, iterations: 600, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 365ms, time_since_start: 15m 04s 610ms, eta: 30m 52s 396ms\n",
      "\u001b[32m2024-07-23T08:07:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:07:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:07:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:07:45 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:07:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:07:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:07:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:07:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, val/hateful_memes/cross_entropy: 1.0193, val/total_loss: 1.0193, val/hateful_memes/accuracy: 0.5720, val/hateful_memes/binary_f1: 0.3815, val/hateful_memes/roc_auc: 0.6629, num_updates: 600, epoch: 6, iterations: 600, max_updates: 2000, val_time: 20s 432ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.674960\n",
      "\u001b[32m2024-07-23T08:08:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 610/2000, train/hateful_memes/cross_entropy: 0.3987, train/hateful_memes/cross_entropy/avg: 0.5215, train/total_loss: 0.3987, train/total_loss/avg: 0.5215, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 6, num_updates: 610, iterations: 610, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 910ms, time_since_start: 15m 37s 961ms, eta: 32m 191ms\n",
      "\u001b[32m2024-07-23T08:08:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 620/2000, train/hateful_memes/cross_entropy: 0.3987, train/hateful_memes/cross_entropy/avg: 0.5207, train/total_loss: 0.3987, train/total_loss/avg: 0.5207, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 6, num_updates: 620, iterations: 620, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 716ms, time_since_start: 15m 50s 678ms, eta: 31m 17s 726ms\n",
      "\u001b[32m2024-07-23T08:08:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 630/2000, train/hateful_memes/cross_entropy: 0.3905, train/hateful_memes/cross_entropy/avg: 0.5183, train/total_loss: 0.3905, train/total_loss/avg: 0.5183, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 6, num_updates: 630, iterations: 630, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 785ms, time_since_start: 16m 03s 463ms, eta: 31m 14s 190ms\n",
      "\u001b[32m2024-07-23T08:08:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 640/2000, train/hateful_memes/cross_entropy: 0.3905, train/hateful_memes/cross_entropy/avg: 0.5179, train/total_loss: 0.3905, train/total_loss/avg: 0.5179, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 6, num_updates: 640, iterations: 640, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 042ms, time_since_start: 16m 15s 506ms, eta: 29m 12s 438ms\n",
      "\u001b[32m2024-07-23T08:09:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/2000, train/hateful_memes/cross_entropy: 0.3874, train/hateful_memes/cross_entropy/avg: 0.5150, train/total_loss: 0.3874, train/total_loss/avg: 0.5150, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 7, num_updates: 650, iterations: 650, max_updates: 2000, lr: 0.00002, ups: 0.91, time: 11s 655ms, time_since_start: 16m 27s 162ms, eta: 28m 03s 696ms\n",
      "\u001b[32m2024-07-23T08:09:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 660/2000, train/hateful_memes/cross_entropy: 0.3874, train/hateful_memes/cross_entropy/avg: 0.5125, train/total_loss: 0.3874, train/total_loss/avg: 0.5125, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 7, num_updates: 660, iterations: 660, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 773ms, time_since_start: 16m 39s 935ms, eta: 30m 31s 498ms\n",
      "\u001b[32m2024-07-23T08:09:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 670/2000, train/hateful_memes/cross_entropy: 0.3789, train/hateful_memes/cross_entropy/avg: 0.5093, train/total_loss: 0.3789, train/total_loss/avg: 0.5093, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 7, num_updates: 670, iterations: 670, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 333ms, time_since_start: 16m 52s 269ms, eta: 29m 15s 234ms\n",
      "\u001b[32m2024-07-23T08:09:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 680/2000, train/hateful_memes/cross_entropy: 0.3757, train/hateful_memes/cross_entropy/avg: 0.5073, train/total_loss: 0.3757, train/total_loss/avg: 0.5073, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 7, num_updates: 680, iterations: 680, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 660ms, time_since_start: 17m 04s 930ms, eta: 29m 48s 217ms\n",
      "\u001b[32m2024-07-23T08:09:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 690/2000, train/hateful_memes/cross_entropy: 0.3756, train/hateful_memes/cross_entropy/avg: 0.5050, train/total_loss: 0.3756, train/total_loss/avg: 0.5050, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 7, num_updates: 690, iterations: 690, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 377ms, time_since_start: 17m 17s 308ms, eta: 28m 54s 964ms\n",
      "\u001b[32m2024-07-23T08:10:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, train/hateful_memes/cross_entropy: 0.3680, train/hateful_memes/cross_entropy/avg: 0.5016, train/total_loss: 0.3680, train/total_loss/avg: 0.5016, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 7, num_updates: 700, iterations: 700, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 860ms, time_since_start: 17m 30s 168ms, eta: 29m 48s 856ms\n",
      "\u001b[32m2024-07-23T08:10:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:10:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:10:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:10:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:10:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:10:15 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T08:10:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:10:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:10:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, val/hateful_memes/cross_entropy: 0.8862, val/total_loss: 0.8862, val/hateful_memes/accuracy: 0.6040, val/hateful_memes/binary_f1: 0.5194, val/hateful_memes/roc_auc: 0.6833, num_updates: 700, epoch: 7, iterations: 700, max_updates: 2000, val_time: 33s 270ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.683296\n",
      "\u001b[32m2024-07-23T08:10:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 710/2000, train/hateful_memes/cross_entropy: 0.3680, train/hateful_memes/cross_entropy/avg: 0.5004, train/total_loss: 0.3680, train/total_loss/avg: 0.5004, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 7, num_updates: 710, iterations: 710, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 355ms, time_since_start: 18m 16s 801ms, eta: 30m 43s 421ms\n",
      "\u001b[32m2024-07-23T08:11:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 720/2000, train/hateful_memes/cross_entropy: 0.3680, train/hateful_memes/cross_entropy/avg: 0.4994, train/total_loss: 0.3680, train/total_loss/avg: 0.4994, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 7, num_updates: 720, iterations: 720, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 823ms, time_since_start: 18m 29s 625ms, eta: 29m 16s 376ms\n",
      "\u001b[32m2024-07-23T08:11:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 730/2000, train/hateful_memes/cross_entropy: 0.3611, train/hateful_memes/cross_entropy/avg: 0.4965, train/total_loss: 0.3611, train/total_loss/avg: 0.4965, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 7, num_updates: 730, iterations: 730, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 389ms, time_since_start: 18m 42s 014ms, eta: 28m 03s 622ms\n",
      "\u001b[32m2024-07-23T08:11:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 740/2000, train/hateful_memes/cross_entropy: 0.3465, train/hateful_memes/cross_entropy/avg: 0.4938, train/total_loss: 0.3465, train/total_loss/avg: 0.4938, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 7, num_updates: 740, iterations: 740, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 827ms, time_since_start: 18m 54s 842ms, eta: 28m 49s 454ms\n",
      "\u001b[32m2024-07-23T08:11:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/2000, train/hateful_memes/cross_entropy: 0.3446, train/hateful_memes/cross_entropy/avg: 0.4902, train/total_loss: 0.3446, train/total_loss/avg: 0.4902, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 8, num_updates: 750, iterations: 750, max_updates: 2000, lr: 0.00002, ups: 0.91, time: 11s 580ms, time_since_start: 19m 06s 423ms, eta: 25m 48s 885ms\n",
      "\u001b[32m2024-07-23T08:11:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 760/2000, train/hateful_memes/cross_entropy: 0.3446, train/hateful_memes/cross_entropy/avg: 0.4862, train/total_loss: 0.3446, train/total_loss/avg: 0.4862, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 8, num_updates: 760, iterations: 760, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 120ms, time_since_start: 19m 18s 543ms, eta: 26m 48s 097ms\n",
      "\u001b[32m2024-07-23T08:12:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 770/2000, train/hateful_memes/cross_entropy: 0.3361, train/hateful_memes/cross_entropy/avg: 0.4836, train/total_loss: 0.3361, train/total_loss/avg: 0.4836, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 8, num_updates: 770, iterations: 770, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 888ms, time_since_start: 19m 31s 431ms, eta: 28m 16s 254ms\n",
      "\u001b[32m2024-07-23T08:12:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 780/2000, train/hateful_memes/cross_entropy: 0.3361, train/hateful_memes/cross_entropy/avg: 0.4804, train/total_loss: 0.3361, train/total_loss/avg: 0.4804, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 8, num_updates: 780, iterations: 780, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 080ms, time_since_start: 19m 43s 512ms, eta: 26m 16s 941ms\n",
      "\u001b[32m2024-07-23T08:12:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 790/2000, train/hateful_memes/cross_entropy: 0.3323, train/hateful_memes/cross_entropy/avg: 0.4780, train/total_loss: 0.3323, train/total_loss/avg: 0.4780, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 8, num_updates: 790, iterations: 790, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 930ms, time_since_start: 19m 56s 442ms, eta: 27m 54s 078ms\n",
      "\u001b[32m2024-07-23T08:12:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, train/hateful_memes/cross_entropy: 0.3323, train/hateful_memes/cross_entropy/avg: 0.4774, train/total_loss: 0.3323, train/total_loss/avg: 0.4774, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 8, num_updates: 800, iterations: 800, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 062ms, time_since_start: 20m 08s 504ms, eta: 25m 48s 788ms\n",
      "\u001b[32m2024-07-23T08:12:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:12:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:12:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:12:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:12:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:12:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:13:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:13:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, val/hateful_memes/cross_entropy: 0.9354, val/total_loss: 0.9354, val/hateful_memes/accuracy: 0.6180, val/hateful_memes/binary_f1: 0.5629, val/hateful_memes/roc_auc: 0.6795, num_updates: 800, epoch: 8, iterations: 800, max_updates: 2000, val_time: 21s 846ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.683296\n",
      "\u001b[32m2024-07-23T08:13:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 810/2000, train/hateful_memes/cross_entropy: 0.3323, train/hateful_memes/cross_entropy/avg: 0.4771, train/total_loss: 0.3323, train/total_loss/avg: 0.4771, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 8, num_updates: 810, iterations: 810, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 111ms, time_since_start: 20m 43s 470ms, eta: 27m 49s 530ms\n",
      "\u001b[32m2024-07-23T08:13:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 820/2000, train/hateful_memes/cross_entropy: 0.3017, train/hateful_memes/cross_entropy/avg: 0.4739, train/total_loss: 0.3017, train/total_loss/avg: 0.4739, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 8, num_updates: 820, iterations: 820, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 556ms, time_since_start: 20m 56s 027ms, eta: 26m 25s 441ms\n",
      "\u001b[32m2024-07-23T08:13:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 830/2000, train/hateful_memes/cross_entropy: 0.2975, train/hateful_memes/cross_entropy/avg: 0.4712, train/total_loss: 0.2975, train/total_loss/avg: 0.4712, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 8, num_updates: 830, iterations: 830, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 102ms, time_since_start: 21m 09s 130ms, eta: 27m 20s 348ms\n",
      "\u001b[32m2024-07-23T08:13:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 840/2000, train/hateful_memes/cross_entropy: 0.2899, train/hateful_memes/cross_entropy/avg: 0.4685, train/total_loss: 0.2899, train/total_loss/avg: 0.4685, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 8, num_updates: 840, iterations: 840, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 106ms, time_since_start: 21m 21s 236ms, eta: 25m 02s 620ms\n",
      "\u001b[32m2024-07-23T08:14:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/2000, train/hateful_memes/cross_entropy: 0.2899, train/hateful_memes/cross_entropy/avg: 0.4669, train/total_loss: 0.2899, train/total_loss/avg: 0.4669, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 8, num_updates: 850, iterations: 850, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 886ms, time_since_start: 21m 34s 123ms, eta: 26m 25s 688ms\n",
      "\u001b[32m2024-07-23T08:14:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 860/2000, train/hateful_memes/cross_entropy: 0.2899, train/hateful_memes/cross_entropy/avg: 0.4649, train/total_loss: 0.2899, train/total_loss/avg: 0.4649, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 9, num_updates: 860, iterations: 860, max_updates: 2000, lr: 0.00002, ups: 1.00, time: 10s 939ms, time_since_start: 21m 45s 063ms, eta: 22m 14s 458ms\n",
      "\u001b[32m2024-07-23T08:14:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 870/2000, train/hateful_memes/cross_entropy: 0.2897, train/hateful_memes/cross_entropy/avg: 0.4614, train/total_loss: 0.2897, train/total_loss/avg: 0.4614, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 9, num_updates: 870, iterations: 870, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 639ms, time_since_start: 21m 57s 702ms, eta: 25m 28s 288ms\n",
      "\u001b[32m2024-07-23T08:14:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 880/2000, train/hateful_memes/cross_entropy: 0.2897, train/hateful_memes/cross_entropy/avg: 0.4595, train/total_loss: 0.2897, train/total_loss/avg: 0.4595, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 9, num_updates: 880, iterations: 880, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 532ms, time_since_start: 22m 10s 235ms, eta: 25m 01s 916ms\n",
      "\u001b[32m2024-07-23T08:14:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 890/2000, train/hateful_memes/cross_entropy: 0.2897, train/hateful_memes/cross_entropy/avg: 0.4578, train/total_loss: 0.2897, train/total_loss/avg: 0.4578, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 9, num_updates: 890, iterations: 890, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 579ms, time_since_start: 22m 22s 815ms, eta: 24m 54s 122ms\n",
      "\u001b[32m2024-07-23T08:15:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, train/hateful_memes/cross_entropy: 0.2897, train/hateful_memes/cross_entropy/avg: 0.4558, train/total_loss: 0.2897, train/total_loss/avg: 0.4558, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 9, num_updates: 900, iterations: 900, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 641ms, time_since_start: 22m 35s 457ms, eta: 24m 47s 943ms\n",
      "\u001b[32m2024-07-23T08:15:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:15:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:15:17 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:15:17 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:15:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:15:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:15:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:15:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, val/hateful_memes/cross_entropy: 1.3684, val/total_loss: 1.3684, val/hateful_memes/accuracy: 0.5900, val/hateful_memes/binary_f1: 0.4414, val/hateful_memes/roc_auc: 0.6808, num_updates: 900, epoch: 9, iterations: 900, max_updates: 2000, val_time: 42s 469ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.683296\n",
      "\u001b[32m2024-07-23T08:16:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 910/2000, train/hateful_memes/cross_entropy: 0.2868, train/hateful_memes/cross_entropy/avg: 0.4524, train/total_loss: 0.2868, train/total_loss/avg: 0.4524, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 9, num_updates: 910, iterations: 910, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 769ms, time_since_start: 23m 30s 703ms, eta: 24m 49s 334ms\n",
      "\u001b[32m2024-07-23T08:16:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 920/2000, train/hateful_memes/cross_entropy: 0.2794, train/hateful_memes/cross_entropy/avg: 0.4502, train/total_loss: 0.2794, train/total_loss/avg: 0.4502, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 9, num_updates: 920, iterations: 920, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 881ms, time_since_start: 23m 43s 584ms, eta: 24m 48s 538ms\n",
      "\u001b[32m2024-07-23T08:16:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 930/2000, train/hateful_memes/cross_entropy: 0.2564, train/hateful_memes/cross_entropy/avg: 0.4480, train/total_loss: 0.2564, train/total_loss/avg: 0.4480, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 9, num_updates: 930, iterations: 930, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 636ms, time_since_start: 23m 56s 221ms, eta: 24m 06s 775ms\n",
      "\u001b[32m2024-07-23T08:16:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 940/2000, train/hateful_memes/cross_entropy: 0.2450, train/hateful_memes/cross_entropy/avg: 0.4457, train/total_loss: 0.2450, train/total_loss/avg: 0.4457, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 9, num_updates: 940, iterations: 940, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 615ms, time_since_start: 24m 08s 837ms, eta: 23m 50s 834ms\n",
      "\u001b[32m2024-07-23T08:16:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/2000, train/hateful_memes/cross_entropy: 0.2450, train/hateful_memes/cross_entropy/avg: 0.4433, train/total_loss: 0.2450, train/total_loss/avg: 0.4433, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 9, num_updates: 950, iterations: 950, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 699ms, time_since_start: 24m 21s 536ms, eta: 23m 46s 825ms\n",
      "\u001b[32m2024-07-23T08:17:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 960/2000, train/hateful_memes/cross_entropy: 0.2450, train/hateful_memes/cross_entropy/avg: 0.4412, train/total_loss: 0.2450, train/total_loss/avg: 0.4412, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 9, num_updates: 960, iterations: 960, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 417ms, time_since_start: 24m 33s 954ms, eta: 23m 01s 800ms\n",
      "\u001b[32m2024-07-23T08:17:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 970/2000, train/hateful_memes/cross_entropy: 0.2446, train/hateful_memes/cross_entropy/avg: 0.4376, train/total_loss: 0.2446, train/total_loss/avg: 0.4376, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 10, num_updates: 970, iterations: 970, max_updates: 2000, lr: 0.00002, ups: 0.91, time: 11s 841ms, time_since_start: 24m 45s 795ms, eta: 21m 45s 022ms\n",
      "\u001b[32m2024-07-23T08:17:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 980/2000, train/hateful_memes/cross_entropy: 0.2446, train/hateful_memes/cross_entropy/avg: 0.4354, train/total_loss: 0.2446, train/total_loss/avg: 0.4354, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 10, num_updates: 980, iterations: 980, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 602ms, time_since_start: 24m 58s 398ms, eta: 22m 55s 479ms\n",
      "\u001b[32m2024-07-23T08:17:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 990/2000, train/hateful_memes/cross_entropy: 0.2445, train/hateful_memes/cross_entropy/avg: 0.4319, train/total_loss: 0.2445, train/total_loss/avg: 0.4319, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 10, num_updates: 990, iterations: 990, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 563ms, time_since_start: 25m 10s 962ms, eta: 22m 37s 776ms\n",
      "\u001b[32m2024-07-23T08:17:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-23T08:17:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:18:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:18:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:18:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, train/hateful_memes/cross_entropy: 0.2332, train/hateful_memes/cross_entropy/avg: 0.4294, train/total_loss: 0.2332, train/total_loss/avg: 0.4294, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 10, num_updates: 1000, iterations: 1000, max_updates: 2000, lr: 0.00002, ups: 0.40, time: 25s 829ms, time_since_start: 25m 36s 791ms, eta: 46m 03s 709ms\n",
      "\u001b[32m2024-07-23T08:18:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:18:10 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:18:18 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:18:18 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:18:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:18:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:18:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:18:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, val/hateful_memes/cross_entropy: 1.4000, val/total_loss: 1.4000, val/hateful_memes/accuracy: 0.5760, val/hateful_memes/binary_f1: 0.4479, val/hateful_memes/roc_auc: 0.6672, num_updates: 1000, epoch: 10, iterations: 1000, max_updates: 2000, val_time: 22s 748ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.683296\n",
      "\u001b[32m2024-07-23T08:18:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1010/2000, train/hateful_memes/cross_entropy: 0.2332, train/hateful_memes/cross_entropy/avg: 0.4277, train/total_loss: 0.2332, train/total_loss/avg: 0.4277, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 10, num_updates: 1010, iterations: 1010, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 741ms, time_since_start: 26m 12s 283ms, eta: 22m 29s 728ms\n",
      "\u001b[32m2024-07-23T08:18:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1020/2000, train/hateful_memes/cross_entropy: 0.2332, train/hateful_memes/cross_entropy/avg: 0.4248, train/total_loss: 0.2332, train/total_loss/avg: 0.4248, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 10, num_updates: 1020, iterations: 1020, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 750ms, time_since_start: 26m 25s 033ms, eta: 22m 16s 980ms\n",
      "\u001b[32m2024-07-23T08:19:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1030/2000, train/hateful_memes/cross_entropy: 0.2332, train/hateful_memes/cross_entropy/avg: 0.4223, train/total_loss: 0.2332, train/total_loss/avg: 0.4223, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 10, num_updates: 1030, iterations: 1030, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 504ms, time_since_start: 26m 37s 538ms, eta: 21m 37s 877ms\n",
      "\u001b[32m2024-07-23T08:19:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1040/2000, train/hateful_memes/cross_entropy: 0.2332, train/hateful_memes/cross_entropy/avg: 0.4206, train/total_loss: 0.2332, train/total_loss/avg: 0.4206, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 10, num_updates: 1040, iterations: 1040, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 659ms, time_since_start: 26m 50s 198ms, eta: 21m 40s 385ms\n",
      "\u001b[32m2024-07-23T08:19:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/2000, train/hateful_memes/cross_entropy: 0.2332, train/hateful_memes/cross_entropy/avg: 0.4197, train/total_loss: 0.2332, train/total_loss/avg: 0.4197, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 10, num_updates: 1050, iterations: 1050, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 592ms, time_since_start: 27m 02s 790ms, eta: 21m 19s 994ms\n",
      "\u001b[32m2024-07-23T08:19:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1060/2000, train/hateful_memes/cross_entropy: 0.2248, train/hateful_memes/cross_entropy/avg: 0.4177, train/total_loss: 0.2248, train/total_loss/avg: 0.4177, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 10, num_updates: 1060, iterations: 1060, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 662ms, time_since_start: 27m 15s 453ms, eta: 21m 13s 633ms\n",
      "\u001b[32m2024-07-23T08:19:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1070/2000, train/hateful_memes/cross_entropy: 0.2248, train/hateful_memes/cross_entropy/avg: 0.4169, train/total_loss: 0.2248, train/total_loss/avg: 0.4169, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 10, num_updates: 1070, iterations: 1070, max_updates: 2000, lr: 0.00002, ups: 1.00, time: 10s 992ms, time_since_start: 27m 26s 445ms, eta: 18m 13s 817ms\n",
      "\u001b[32m2024-07-23T08:20:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1080/2000, train/hateful_memes/cross_entropy: 0.2248, train/hateful_memes/cross_entropy/avg: 0.4160, train/total_loss: 0.2248, train/total_loss/avg: 0.4160, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 11, num_updates: 1080, iterations: 1080, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 170ms, time_since_start: 27m 39s 616ms, eta: 21m 36s 512ms\n",
      "\u001b[32m2024-07-23T08:20:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1090/2000, train/hateful_memes/cross_entropy: 0.2169, train/hateful_memes/cross_entropy/avg: 0.4138, train/total_loss: 0.2169, train/total_loss/avg: 0.4138, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 11, num_updates: 1090, iterations: 1090, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 275ms, time_since_start: 27m 51s 891ms, eta: 19m 55s 308ms\n",
      "\u001b[32m2024-07-23T08:20:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, train/hateful_memes/cross_entropy: 0.2129, train/hateful_memes/cross_entropy/avg: 0.4109, train/total_loss: 0.2129, train/total_loss/avg: 0.4109, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 11, num_updates: 1100, iterations: 1100, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 091ms, time_since_start: 28m 04s 983ms, eta: 21m 665ms\n",
      "\u001b[32m2024-07-23T08:20:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:20:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:20:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:20:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:20:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:20:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:21:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:21:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, val/hateful_memes/cross_entropy: 1.5270, val/total_loss: 1.5270, val/hateful_memes/accuracy: 0.5880, val/hateful_memes/binary_f1: 0.4521, val/hateful_memes/roc_auc: 0.6758, num_updates: 1100, epoch: 11, iterations: 1100, max_updates: 2000, val_time: 22s 616ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.683296\n",
      "\u001b[32m2024-07-23T08:21:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1110/2000, train/hateful_memes/cross_entropy: 0.2169, train/hateful_memes/cross_entropy/avg: 0.4092, train/total_loss: 0.2169, train/total_loss/avg: 0.4092, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 11, num_updates: 1110, iterations: 1110, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 516ms, time_since_start: 28m 40s 123ms, eta: 19m 51s 957ms\n",
      "\u001b[32m2024-07-23T08:21:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1120/2000, train/hateful_memes/cross_entropy: 0.2169, train/hateful_memes/cross_entropy/avg: 0.4076, train/total_loss: 0.2169, train/total_loss/avg: 0.4076, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 11, num_updates: 1120, iterations: 1120, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 091ms, time_since_start: 28m 53s 215ms, eta: 20m 32s 719ms\n",
      "\u001b[32m2024-07-23T08:21:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1130/2000, train/hateful_memes/cross_entropy: 0.2129, train/hateful_memes/cross_entropy/avg: 0.4056, train/total_loss: 0.2129, train/total_loss/avg: 0.4056, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 11, num_updates: 1130, iterations: 1130, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 224ms, time_since_start: 29m 05s 439ms, eta: 18m 58s 007ms\n",
      "\u001b[32m2024-07-23T08:21:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1140/2000, train/hateful_memes/cross_entropy: 0.1965, train/hateful_memes/cross_entropy/avg: 0.4038, train/total_loss: 0.1965, train/total_loss/avg: 0.4038, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 11, num_updates: 1140, iterations: 1140, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 132ms, time_since_start: 29m 18s 572ms, eta: 20m 08s 449ms\n",
      "\u001b[32m2024-07-23T08:22:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/2000, train/hateful_memes/cross_entropy: 0.1965, train/hateful_memes/cross_entropy/avg: 0.4026, train/total_loss: 0.1965, train/total_loss/avg: 0.4026, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 11, num_updates: 1150, iterations: 1150, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 249ms, time_since_start: 29m 30s 821ms, eta: 18m 34s 075ms\n",
      "\u001b[32m2024-07-23T08:22:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1160/2000, train/hateful_memes/cross_entropy: 0.1890, train/hateful_memes/cross_entropy/avg: 0.4008, train/total_loss: 0.1890, train/total_loss/avg: 0.4008, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 11, num_updates: 1160, iterations: 1160, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 180ms, time_since_start: 29m 44s 001ms, eta: 19m 44s 643ms\n",
      "\u001b[32m2024-07-23T08:22:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1170/2000, train/hateful_memes/cross_entropy: 0.1890, train/hateful_memes/cross_entropy/avg: 0.3988, train/total_loss: 0.1890, train/total_loss/avg: 0.3988, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 11, num_updates: 1170, iterations: 1170, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 257ms, time_since_start: 29m 56s 259ms, eta: 18m 08s 601ms\n",
      "\u001b[32m2024-07-23T08:22:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1180/2000, train/hateful_memes/cross_entropy: 0.1886, train/hateful_memes/cross_entropy/avg: 0.3958, train/total_loss: 0.1886, train/total_loss/avg: 0.3958, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 12, num_updates: 1180, iterations: 1180, max_updates: 2000, lr: 0.00001, ups: 0.91, time: 11s 255ms, time_since_start: 30m 07s 515ms, eta: 16m 27s 574ms\n",
      "\u001b[32m2024-07-23T08:22:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1190/2000, train/hateful_memes/cross_entropy: 0.1886, train/hateful_memes/cross_entropy/avg: 0.3936, train/total_loss: 0.1886, train/total_loss/avg: 0.3936, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 12, num_updates: 1190, iterations: 1190, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 659ms, time_since_start: 30m 20s 175ms, eta: 18m 17s 235ms\n",
      "\u001b[32m2024-07-23T08:23:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, train/hateful_memes/cross_entropy: 0.1886, train/hateful_memes/cross_entropy/avg: 0.3906, train/total_loss: 0.1886, train/total_loss/avg: 0.3906, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 12, num_updates: 1200, iterations: 1200, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 326ms, time_since_start: 30m 32s 501ms, eta: 17m 35s 160ms\n",
      "\u001b[32m2024-07-23T08:23:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:23:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:23:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:23:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:23:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:23:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:23:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:23:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, val/hateful_memes/cross_entropy: 2.1074, val/total_loss: 2.1074, val/hateful_memes/accuracy: 0.5860, val/hateful_memes/binary_f1: 0.4103, val/hateful_memes/roc_auc: 0.6794, num_updates: 1200, epoch: 12, iterations: 1200, max_updates: 2000, val_time: 42s 571ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.683296\n",
      "\u001b[32m2024-07-23T08:24:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1210/2000, train/hateful_memes/cross_entropy: 0.1826, train/hateful_memes/cross_entropy/avg: 0.3883, train/total_loss: 0.1826, train/total_loss/avg: 0.3883, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 12, num_updates: 1210, iterations: 1210, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 235ms, time_since_start: 31m 28s 315ms, eta: 18m 38s 788ms\n",
      "\u001b[32m2024-07-23T08:24:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1220/2000, train/hateful_memes/cross_entropy: 0.1826, train/hateful_memes/cross_entropy/avg: 0.3865, train/total_loss: 0.1826, train/total_loss/avg: 0.3865, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 12, num_updates: 1220, iterations: 1220, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 416ms, time_since_start: 31m 40s 731ms, eta: 17m 16s 242ms\n",
      "\u001b[32m2024-07-23T08:24:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1230/2000, train/hateful_memes/cross_entropy: 0.1826, train/hateful_memes/cross_entropy/avg: 0.3846, train/total_loss: 0.1826, train/total_loss/avg: 0.3846, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 12, num_updates: 1230, iterations: 1230, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 755ms, time_since_start: 31m 53s 486ms, eta: 17m 30s 904ms\n",
      "\u001b[32m2024-07-23T08:24:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1240/2000, train/hateful_memes/cross_entropy: 0.1720, train/hateful_memes/cross_entropy/avg: 0.3821, train/total_loss: 0.1720, train/total_loss/avg: 0.3821, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 12, num_updates: 1240, iterations: 1240, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 322ms, time_since_start: 32m 05s 809ms, eta: 16m 42s 081ms\n",
      "\u001b[32m2024-07-23T08:24:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/2000, train/hateful_memes/cross_entropy: 0.1710, train/hateful_memes/cross_entropy/avg: 0.3798, train/total_loss: 0.1710, train/total_loss/avg: 0.3798, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 12, num_updates: 1250, iterations: 1250, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 753ms, time_since_start: 32m 18s 563ms, eta: 17m 03s 478ms\n",
      "\u001b[32m2024-07-23T08:25:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1260/2000, train/hateful_memes/cross_entropy: 0.1710, train/hateful_memes/cross_entropy/avg: 0.3788, train/total_loss: 0.1710, train/total_loss/avg: 0.3788, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 12, num_updates: 1260, iterations: 1260, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 389ms, time_since_start: 32m 30s 952ms, eta: 16m 20s 975ms\n",
      "\u001b[32m2024-07-23T08:25:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1270/2000, train/hateful_memes/cross_entropy: 0.1710, train/hateful_memes/cross_entropy/avg: 0.3767, train/total_loss: 0.1710, train/total_loss/avg: 0.3767, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 12, num_updates: 1270, iterations: 1270, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 826ms, time_since_start: 32m 43s 778ms, eta: 16m 41s 865ms\n",
      "\u001b[32m2024-07-23T08:25:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1280/2000, train/hateful_memes/cross_entropy: 0.1526, train/hateful_memes/cross_entropy/avg: 0.3741, train/total_loss: 0.1526, train/total_loss/avg: 0.3741, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 12, num_updates: 1280, iterations: 1280, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 401ms, time_since_start: 32m 56s 180ms, eta: 15m 55s 390ms\n",
      "\u001b[32m2024-07-23T08:25:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1290/2000, train/hateful_memes/cross_entropy: 0.1331, train/hateful_memes/cross_entropy/avg: 0.3717, train/total_loss: 0.1331, train/total_loss/avg: 0.3717, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 13, num_updates: 1290, iterations: 1290, max_updates: 2000, lr: 0.00001, ups: 0.91, time: 11s 977ms, time_since_start: 33m 08s 157ms, eta: 15m 09s 935ms\n",
      "\u001b[32m2024-07-23T08:25:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, train/hateful_memes/cross_entropy: 0.1331, train/hateful_memes/cross_entropy/avg: 0.3694, train/total_loss: 0.1331, train/total_loss/avg: 0.3694, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 13, num_updates: 1300, iterations: 1300, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 174ms, time_since_start: 33m 20s 331ms, eta: 15m 11s 850ms\n",
      "\u001b[32m2024-07-23T08:25:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:25:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:26:01 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:26:01 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:26:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:26:05 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T08:26:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:26:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:26:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, val/hateful_memes/cross_entropy: 1.9012, val/total_loss: 1.9012, val/hateful_memes/accuracy: 0.6060, val/hateful_memes/binary_f1: 0.4690, val/hateful_memes/roc_auc: 0.6910, num_updates: 1300, epoch: 13, iterations: 1300, max_updates: 2000, val_time: 34s 168ms, best_update: 1300, best_iteration: 1300, best_val/hateful_memes/roc_auc: 0.690980\n",
      "\u001b[32m2024-07-23T08:26:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1310/2000, train/hateful_memes/cross_entropy: 0.1160, train/hateful_memes/cross_entropy/avg: 0.3673, train/total_loss: 0.1160, train/total_loss/avg: 0.3673, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 13, num_updates: 1310, iterations: 1310, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 483ms, time_since_start: 34m 07s 990ms, eta: 16m 35s 451ms\n",
      "\u001b[32m2024-07-23T08:26:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1320/2000, train/hateful_memes/cross_entropy: 0.1153, train/hateful_memes/cross_entropy/avg: 0.3653, train/total_loss: 0.1153, train/total_loss/avg: 0.3653, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 13, num_updates: 1320, iterations: 1320, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 156ms, time_since_start: 34m 20s 147ms, eta: 14m 44s 486ms\n",
      "\u001b[32m2024-07-23T08:27:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1330/2000, train/hateful_memes/cross_entropy: 0.1128, train/hateful_memes/cross_entropy/avg: 0.3630, train/total_loss: 0.1128, train/total_loss/avg: 0.3630, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 13, num_updates: 1330, iterations: 1330, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 043ms, time_since_start: 34m 33s 191ms, eta: 15m 35s 122ms\n",
      "\u001b[32m2024-07-23T08:27:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1340/2000, train/hateful_memes/cross_entropy: 0.1027, train/hateful_memes/cross_entropy/avg: 0.3608, train/total_loss: 0.1027, train/total_loss/avg: 0.3608, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 13, num_updates: 1340, iterations: 1340, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 124ms, time_since_start: 34m 45s 315ms, eta: 14m 16s 203ms\n",
      "\u001b[32m2024-07-23T08:27:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/2000, train/hateful_memes/cross_entropy: 0.1013, train/hateful_memes/cross_entropy/avg: 0.3589, train/total_loss: 0.1013, train/total_loss/avg: 0.3589, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 13, num_updates: 1350, iterations: 1350, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 050ms, time_since_start: 34m 58s 365ms, eta: 15m 07s 631ms\n",
      "\u001b[32m2024-07-23T08:27:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1360/2000, train/hateful_memes/cross_entropy: 0.1013, train/hateful_memes/cross_entropy/avg: 0.3574, train/total_loss: 0.1013, train/total_loss/avg: 0.3574, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 13, num_updates: 1360, iterations: 1360, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 224ms, time_since_start: 35m 10s 589ms, eta: 13m 57s 107ms\n",
      "\u001b[32m2024-07-23T08:27:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1370/2000, train/hateful_memes/cross_entropy: 0.1012, train/hateful_memes/cross_entropy/avg: 0.3556, train/total_loss: 0.1012, train/total_loss/avg: 0.3556, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 13, num_updates: 1370, iterations: 1370, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 054ms, time_since_start: 35m 23s 644ms, eta: 14m 40s 016ms\n",
      "\u001b[32m2024-07-23T08:28:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1380/2000, train/hateful_memes/cross_entropy: 0.1013, train/hateful_memes/cross_entropy/avg: 0.3541, train/total_loss: 0.1013, train/total_loss/avg: 0.3541, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 13, num_updates: 1380, iterations: 1380, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 160ms, time_since_start: 35m 35s 804ms, eta: 13m 26s 752ms\n",
      "\u001b[32m2024-07-23T08:28:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1390/2000, train/hateful_memes/cross_entropy: 0.1013, train/hateful_memes/cross_entropy/avg: 0.3523, train/total_loss: 0.1013, train/total_loss/avg: 0.3523, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 13, num_updates: 1390, iterations: 1390, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 156ms, time_since_start: 35m 47s 961ms, eta: 13m 13s 451ms\n",
      "\u001b[32m2024-07-23T08:28:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, train/hateful_memes/cross_entropy: 0.1013, train/hateful_memes/cross_entropy/avg: 0.3503, train/total_loss: 0.1013, train/total_loss/avg: 0.3503, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 14, num_updates: 1400, iterations: 1400, max_updates: 2000, lr: 0.00001, ups: 0.91, time: 11s 652ms, time_since_start: 35m 59s 613ms, eta: 12m 28s 091ms\n",
      "\u001b[32m2024-07-23T08:28:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:28:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:28:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:28:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:28:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:28:45 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T08:28:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:29:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:29:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, val/hateful_memes/cross_entropy: 2.2042, val/total_loss: 2.2042, val/hateful_memes/accuracy: 0.6000, val/hateful_memes/binary_f1: 0.4413, val/hateful_memes/roc_auc: 0.6942, num_updates: 1400, epoch: 14, iterations: 1400, max_updates: 2000, val_time: 35s 070ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.694216\n",
      "\u001b[32m2024-07-23T08:29:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1410/2000, train/hateful_memes/cross_entropy: 0.1012, train/hateful_memes/cross_entropy/avg: 0.3482, train/total_loss: 0.1012, train/total_loss/avg: 0.3482, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 14, num_updates: 1410, iterations: 1410, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 435ms, time_since_start: 36m 48s 128ms, eta: 14m 08s 208ms\n",
      "\u001b[32m2024-07-23T08:29:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1420/2000, train/hateful_memes/cross_entropy: 0.1012, train/hateful_memes/cross_entropy/avg: 0.3466, train/total_loss: 0.1012, train/total_loss/avg: 0.3466, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 14, num_updates: 1420, iterations: 1420, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 817ms, time_since_start: 37m 946ms, eta: 13m 15s 483ms\n",
      "\u001b[32m2024-07-23T08:29:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1430/2000, train/hateful_memes/cross_entropy: 0.0851, train/hateful_memes/cross_entropy/avg: 0.3445, train/total_loss: 0.0851, train/total_loss/avg: 0.3445, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 14, num_updates: 1430, iterations: 1430, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 669ms, time_since_start: 37m 13s 616ms, eta: 12m 52s 728ms\n",
      "\u001b[32m2024-07-23T08:29:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1440/2000, train/hateful_memes/cross_entropy: 0.1012, train/hateful_memes/cross_entropy/avg: 0.3429, train/total_loss: 0.1012, train/total_loss/avg: 0.3429, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 14, num_updates: 1440, iterations: 1440, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 712ms, time_since_start: 37m 26s 328ms, eta: 12m 41s 719ms\n",
      "\u001b[32m2024-07-23T08:30:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/2000, train/hateful_memes/cross_entropy: 0.0863, train/hateful_memes/cross_entropy/avg: 0.3412, train/total_loss: 0.0863, train/total_loss/avg: 0.3412, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 14, num_updates: 1450, iterations: 1450, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 745ms, time_since_start: 37m 39s 073ms, eta: 12m 30s 069ms\n",
      "\u001b[32m2024-07-23T08:30:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1460/2000, train/hateful_memes/cross_entropy: 0.0863, train/hateful_memes/cross_entropy/avg: 0.3397, train/total_loss: 0.0863, train/total_loss/avg: 0.3397, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 14, num_updates: 1460, iterations: 1460, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 696ms, time_since_start: 37m 51s 770ms, eta: 12m 13s 624ms\n",
      "\u001b[32m2024-07-23T08:30:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1470/2000, train/hateful_memes/cross_entropy: 0.0851, train/hateful_memes/cross_entropy/avg: 0.3378, train/total_loss: 0.0851, train/total_loss/avg: 0.3378, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 14, num_updates: 1470, iterations: 1470, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 789ms, time_since_start: 38m 04s 560ms, eta: 12m 05s 305ms\n",
      "\u001b[32m2024-07-23T08:30:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1480/2000, train/hateful_memes/cross_entropy: 0.0863, train/hateful_memes/cross_entropy/avg: 0.3361, train/total_loss: 0.0863, train/total_loss/avg: 0.3361, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 14, num_updates: 1480, iterations: 1480, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 673ms, time_since_start: 38m 17s 234ms, eta: 11m 45s 157ms\n",
      "\u001b[32m2024-07-23T08:31:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1490/2000, train/hateful_memes/cross_entropy: 0.0898, train/hateful_memes/cross_entropy/avg: 0.3348, train/total_loss: 0.0898, train/total_loss/avg: 0.3348, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 14, num_updates: 1490, iterations: 1490, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 722ms, time_since_start: 38m 29s 956ms, eta: 11m 34s 269ms\n",
      "\u001b[32m2024-07-23T08:31:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, train/hateful_memes/cross_entropy: 0.0898, train/hateful_memes/cross_entropy/avg: 0.3328, train/total_loss: 0.0898, train/total_loss/avg: 0.3328, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 15, num_updates: 1500, iterations: 1500, max_updates: 2000, lr: 0.00001, ups: 0.91, time: 11s 601ms, time_since_start: 38m 41s 558ms, eta: 10m 20s 705ms\n",
      "\u001b[32m2024-07-23T08:31:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:31:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:31:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:31:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:31:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:31:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:31:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:31:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, val/hateful_memes/cross_entropy: 2.2494, val/total_loss: 2.2494, val/hateful_memes/accuracy: 0.6060, val/hateful_memes/binary_f1: 0.4747, val/hateful_memes/roc_auc: 0.6914, num_updates: 1500, epoch: 15, iterations: 1500, max_updates: 2000, val_time: 22s 345ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.694216\n",
      "\u001b[32m2024-07-23T08:31:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1510/2000, train/hateful_memes/cross_entropy: 0.0898, train/hateful_memes/cross_entropy/avg: 0.3311, train/total_loss: 0.0898, train/total_loss/avg: 0.3311, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 15, num_updates: 1510, iterations: 1510, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 175ms, time_since_start: 39m 17s 087ms, eta: 11m 30s 802ms\n",
      "\u001b[32m2024-07-23T08:32:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1520/2000, train/hateful_memes/cross_entropy: 0.0863, train/hateful_memes/cross_entropy/avg: 0.3292, train/total_loss: 0.0863, train/total_loss/avg: 0.3292, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 15, num_updates: 1520, iterations: 1520, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 532ms, time_since_start: 39m 29s 620ms, eta: 10m 43s 693ms\n",
      "\u001b[32m2024-07-23T08:32:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1530/2000, train/hateful_memes/cross_entropy: 0.0863, train/hateful_memes/cross_entropy/avg: 0.3274, train/total_loss: 0.0863, train/total_loss/avg: 0.3274, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 15, num_updates: 1530, iterations: 1530, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 620ms, time_since_start: 39m 42s 241ms, eta: 10m 34s 699ms\n",
      "\u001b[32m2024-07-23T08:32:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1540/2000, train/hateful_memes/cross_entropy: 0.0863, train/hateful_memes/cross_entropy/avg: 0.3257, train/total_loss: 0.0863, train/total_loss/avg: 0.3257, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 15, num_updates: 1540, iterations: 1540, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 457ms, time_since_start: 39m 54s 698ms, eta: 10m 13s 155ms\n",
      "\u001b[32m2024-07-23T08:32:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/2000, train/hateful_memes/cross_entropy: 0.0816, train/hateful_memes/cross_entropy/avg: 0.3239, train/total_loss: 0.0816, train/total_loss/avg: 0.3239, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 15, num_updates: 1550, iterations: 1550, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 592ms, time_since_start: 40m 07s 291ms, eta: 10m 06s 326ms\n",
      "\u001b[32m2024-07-23T08:32:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1560/2000, train/hateful_memes/cross_entropy: 0.0741, train/hateful_memes/cross_entropy/avg: 0.3220, train/total_loss: 0.0741, train/total_loss/avg: 0.3220, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 15, num_updates: 1560, iterations: 1560, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 474ms, time_since_start: 40m 19s 765ms, eta: 09m 47s 293ms\n",
      "\u001b[32m2024-07-23T08:33:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1570/2000, train/hateful_memes/cross_entropy: 0.0741, train/hateful_memes/cross_entropy/avg: 0.3205, train/total_loss: 0.0741, train/total_loss/avg: 0.3205, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 15, num_updates: 1570, iterations: 1570, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 631ms, time_since_start: 40m 32s 396ms, eta: 09m 41s 160ms\n",
      "\u001b[32m2024-07-23T08:33:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1580/2000, train/hateful_memes/cross_entropy: 0.0637, train/hateful_memes/cross_entropy/avg: 0.3186, train/total_loss: 0.0637, train/total_loss/avg: 0.3186, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 15, num_updates: 1580, iterations: 1580, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 545ms, time_since_start: 40m 44s 942ms, eta: 09m 23s 803ms\n",
      "\u001b[32m2024-07-23T08:33:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1590/2000, train/hateful_memes/cross_entropy: 0.0637, train/hateful_memes/cross_entropy/avg: 0.3170, train/total_loss: 0.0637, train/total_loss/avg: 0.3170, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 15, num_updates: 1590, iterations: 1590, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 561ms, time_since_start: 40m 57s 504ms, eta: 09m 11s 084ms\n",
      "\u001b[32m2024-07-23T08:33:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, train/hateful_memes/cross_entropy: 0.0637, train/hateful_memes/cross_entropy/avg: 0.3156, train/total_loss: 0.0637, train/total_loss/avg: 0.3156, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 15, num_updates: 1600, iterations: 1600, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 564ms, time_since_start: 41m 10s 068ms, eta: 08m 57s 770ms\n",
      "\u001b[32m2024-07-23T08:33:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:33:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:33:51 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:33:51 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:33:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:33:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:34:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:34:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, val/hateful_memes/cross_entropy: 2.2175, val/total_loss: 2.2175, val/hateful_memes/accuracy: 0.5920, val/hateful_memes/binary_f1: 0.4688, val/hateful_memes/roc_auc: 0.6876, num_updates: 1600, epoch: 15, iterations: 1600, max_updates: 2000, val_time: 22s 872ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.694216\n",
      "\u001b[32m2024-07-23T08:34:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1610/2000, train/hateful_memes/cross_entropy: 0.0637, train/hateful_memes/cross_entropy/avg: 0.3139, train/total_loss: 0.0637, train/total_loss/avg: 0.3139, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 16, num_updates: 1610, iterations: 1610, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 413ms, time_since_start: 41m 45s 361ms, eta: 08m 38s 011ms\n",
      "\u001b[32m2024-07-23T08:34:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1620/2000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.3121, train/total_loss: 0.0621, train/total_loss/avg: 0.3121, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 16, num_updates: 1620, iterations: 1620, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 184ms, time_since_start: 41m 57s 546ms, eta: 08m 15s 441ms\n",
      "\u001b[32m2024-07-23T08:34:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1630/2000, train/hateful_memes/cross_entropy: 0.0637, train/hateful_memes/cross_entropy/avg: 0.3107, train/total_loss: 0.0637, train/total_loss/avg: 0.3107, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 16, num_updates: 1630, iterations: 1630, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 044ms, time_since_start: 42m 10s 591ms, eta: 08m 36s 449ms\n",
      "\u001b[32m2024-07-23T08:34:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1640/2000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.3090, train/total_loss: 0.0621, train/total_loss/avg: 0.3090, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 16, num_updates: 1640, iterations: 1640, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 169ms, time_since_start: 42m 22s 761ms, eta: 07m 48s 775ms\n",
      "\u001b[32m2024-07-23T08:35:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/2000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.3076, train/total_loss: 0.0621, train/total_loss/avg: 0.3076, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 16, num_updates: 1650, iterations: 1650, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 999ms, time_since_start: 42m 35s 760ms, eta: 08m 06s 820ms\n",
      "\u001b[32m2024-07-23T08:35:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1660/2000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.3067, train/total_loss: 0.0621, train/total_loss/avg: 0.3067, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 16, num_updates: 1660, iterations: 1660, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 173ms, time_since_start: 42m 47s 933ms, eta: 07m 22s 854ms\n",
      "\u001b[32m2024-07-23T08:35:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1670/2000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.3051, train/total_loss: 0.0621, train/total_loss/avg: 0.3051, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 16, num_updates: 1670, iterations: 1670, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 981ms, time_since_start: 43m 915ms, eta: 07m 38s 372ms\n",
      "\u001b[32m2024-07-23T08:35:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1680/2000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.3040, train/total_loss: 0.0621, train/total_loss/avg: 0.3040, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 16, num_updates: 1680, iterations: 1680, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 097ms, time_since_start: 43m 13s 013ms, eta: 06m 54s 228ms\n",
      "\u001b[32m2024-07-23T08:35:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1690/2000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.3027, train/total_loss: 0.0621, train/total_loss/avg: 0.3027, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 16, num_updates: 1690, iterations: 1690, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 064ms, time_since_start: 43m 26s 077ms, eta: 07m 13s 352ms\n",
      "\u001b[32m2024-07-23T08:36:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, train/hateful_memes/cross_entropy: 0.0637, train/hateful_memes/cross_entropy/avg: 0.3015, train/total_loss: 0.0637, train/total_loss/avg: 0.3015, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 16, num_updates: 1700, iterations: 1700, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 169ms, time_since_start: 43m 38s 247ms, eta: 06m 30s 642ms\n",
      "\u001b[32m2024-07-23T08:36:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:36:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:36:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:36:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:36:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:36:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:36:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:36:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, val/hateful_memes/cross_entropy: 2.6216, val/total_loss: 2.6216, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.4674, val/hateful_memes/roc_auc: 0.6882, num_updates: 1700, epoch: 16, iterations: 1700, max_updates: 2000, val_time: 21s 915ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.694216\n",
      "\u001b[32m2024-07-23T08:36:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1710/2000, train/hateful_memes/cross_entropy: 0.0637, train/hateful_memes/cross_entropy/avg: 0.3004, train/total_loss: 0.0637, train/total_loss/avg: 0.3004, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 16, num_updates: 1710, iterations: 1710, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 620ms, time_since_start: 44m 12s 790ms, eta: 06m 31s 621ms\n",
      "\u001b[32m2024-07-23T08:36:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1720/2000, train/hateful_memes/cross_entropy: 0.0637, train/hateful_memes/cross_entropy/avg: 0.2989, train/total_loss: 0.0637, train/total_loss/avg: 0.2989, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 17, num_updates: 1720, iterations: 1720, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 031ms, time_since_start: 44m 24s 822ms, eta: 06m 474ms\n",
      "\u001b[32m2024-07-23T08:37:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1730/2000, train/hateful_memes/cross_entropy: 0.0637, train/hateful_memes/cross_entropy/avg: 0.2974, train/total_loss: 0.0637, train/total_loss/avg: 0.2974, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 17, num_updates: 1730, iterations: 1730, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 850ms, time_since_start: 44m 37s 673ms, eta: 06m 11s 264ms\n",
      "\u001b[32m2024-07-23T08:37:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1740/2000, train/hateful_memes/cross_entropy: 0.0663, train/hateful_memes/cross_entropy/avg: 0.2961, train/total_loss: 0.0663, train/total_loss/avg: 0.2961, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 17, num_updates: 1740, iterations: 1740, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 018ms, time_since_start: 44m 49s 691ms, eta: 05m 34s 341ms\n",
      "\u001b[32m2024-07-23T08:37:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/2000, train/hateful_memes/cross_entropy: 0.0663, train/hateful_memes/cross_entropy/avg: 0.2947, train/total_loss: 0.0663, train/total_loss/avg: 0.2947, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 17, num_updates: 1750, iterations: 1750, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 864ms, time_since_start: 45m 02s 555ms, eta: 05m 44s 112ms\n",
      "\u001b[32m2024-07-23T08:37:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1760/2000, train/hateful_memes/cross_entropy: 0.0663, train/hateful_memes/cross_entropy/avg: 0.2931, train/total_loss: 0.0663, train/total_loss/avg: 0.2931, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 17, num_updates: 1760, iterations: 1760, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 129ms, time_since_start: 45m 14s 684ms, eta: 05m 11s 480ms\n",
      "\u001b[32m2024-07-23T08:38:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1770/2000, train/hateful_memes/cross_entropy: 0.0625, train/hateful_memes/cross_entropy/avg: 0.2916, train/total_loss: 0.0625, train/total_loss/avg: 0.2916, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 17, num_updates: 1770, iterations: 1770, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 943ms, time_since_start: 45m 27s 627ms, eta: 05m 18s 528ms\n",
      "\u001b[32m2024-07-23T08:38:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1780/2000, train/hateful_memes/cross_entropy: 0.0625, train/hateful_memes/cross_entropy/avg: 0.2901, train/total_loss: 0.0625, train/total_loss/avg: 0.2901, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 17, num_updates: 1780, iterations: 1780, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 072ms, time_since_start: 45m 39s 700ms, eta: 04m 44s 196ms\n",
      "\u001b[32m2024-07-23T08:38:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1790/2000, train/hateful_memes/cross_entropy: 0.0625, train/hateful_memes/cross_entropy/avg: 0.2889, train/total_loss: 0.0625, train/total_loss/avg: 0.2889, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 17, num_updates: 1790, iterations: 1790, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 931ms, time_since_start: 45m 52s 631ms, eta: 04m 50s 562ms\n",
      "\u001b[32m2024-07-23T08:38:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, train/hateful_memes/cross_entropy: 0.0584, train/hateful_memes/cross_entropy/avg: 0.2876, train/total_loss: 0.0584, train/total_loss/avg: 0.2876, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 17, num_updates: 1800, iterations: 1800, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 139ms, time_since_start: 46m 04s 771ms, eta: 04m 19s 786ms\n",
      "\u001b[32m2024-07-23T08:38:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:38:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:38:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:38:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:38:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:38:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:38:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:38:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, val/hateful_memes/cross_entropy: 2.8457, val/total_loss: 2.8457, val/hateful_memes/accuracy: 0.6000, val/hateful_memes/binary_f1: 0.4565, val/hateful_memes/roc_auc: 0.6879, num_updates: 1800, epoch: 17, iterations: 1800, max_updates: 2000, val_time: 21s 626ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.694216\n",
      "\u001b[32m2024-07-23T08:39:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1810/2000, train/hateful_memes/cross_entropy: 0.0625, train/hateful_memes/cross_entropy/avg: 0.2865, train/total_loss: 0.0625, train/total_loss/avg: 0.2865, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 17, num_updates: 1810, iterations: 1810, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 587ms, time_since_start: 46m 39s 992ms, eta: 04m 36s 241ms\n",
      "\u001b[32m2024-07-23T08:39:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1820/2000, train/hateful_memes/cross_entropy: 0.0663, train/hateful_memes/cross_entropy/avg: 0.2857, train/total_loss: 0.0663, train/total_loss/avg: 0.2857, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 18, num_updates: 1820, iterations: 1820, max_updates: 2000, lr: 0., ups: 0.91, time: 11s 569ms, time_since_start: 46m 51s 562ms, eta: 03m 42s 837ms\n",
      "\u001b[32m2024-07-23T08:39:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1830/2000, train/hateful_memes/cross_entropy: 0.0663, train/hateful_memes/cross_entropy/avg: 0.2846, train/total_loss: 0.0663, train/total_loss/avg: 0.2846, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 18, num_updates: 1830, iterations: 1830, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 399ms, time_since_start: 47m 03s 962ms, eta: 03m 45s 554ms\n",
      "\u001b[32m2024-07-23T08:39:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1840/2000, train/hateful_memes/cross_entropy: 0.0742, train/hateful_memes/cross_entropy/avg: 0.2836, train/total_loss: 0.0742, train/total_loss/avg: 0.2836, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 18, num_updates: 1840, iterations: 1840, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 830ms, time_since_start: 47m 16s 793ms, eta: 03m 39s 662ms\n",
      "\u001b[32m2024-07-23T08:40:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/2000, train/hateful_memes/cross_entropy: 0.0779, train/hateful_memes/cross_entropy/avg: 0.2826, train/total_loss: 0.0779, train/total_loss/avg: 0.2826, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 18, num_updates: 1850, iterations: 1850, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 482ms, time_since_start: 47m 29s 276ms, eta: 03m 20s 342ms\n",
      "\u001b[32m2024-07-23T08:40:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1860/2000, train/hateful_memes/cross_entropy: 0.0663, train/hateful_memes/cross_entropy/avg: 0.2812, train/total_loss: 0.0663, train/total_loss/avg: 0.2812, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 18, num_updates: 1860, iterations: 1860, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 776ms, time_since_start: 47m 42s 052ms, eta: 03m 11s 390ms\n",
      "\u001b[32m2024-07-23T08:40:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1870/2000, train/hateful_memes/cross_entropy: 0.0663, train/hateful_memes/cross_entropy/avg: 0.2799, train/total_loss: 0.0663, train/total_loss/avg: 0.2799, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 18, num_updates: 1870, iterations: 1870, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 496ms, time_since_start: 47m 54s 549ms, eta: 02m 53s 827ms\n",
      "\u001b[32m2024-07-23T08:40:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1880/2000, train/hateful_memes/cross_entropy: 0.0663, train/hateful_memes/cross_entropy/avg: 0.2791, train/total_loss: 0.0663, train/total_loss/avg: 0.2791, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 18, num_updates: 1880, iterations: 1880, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 830ms, time_since_start: 48m 07s 379ms, eta: 02m 44s 748ms\n",
      "\u001b[32m2024-07-23T08:40:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1890/2000, train/hateful_memes/cross_entropy: 0.0625, train/hateful_memes/cross_entropy/avg: 0.2777, train/total_loss: 0.0625, train/total_loss/avg: 0.2777, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 18, num_updates: 1890, iterations: 1890, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 519ms, time_since_start: 48m 19s 899ms, eta: 02m 27s 356ms\n",
      "\u001b[32m2024-07-23T08:41:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, train/hateful_memes/cross_entropy: 0.0584, train/hateful_memes/cross_entropy/avg: 0.2764, train/total_loss: 0.0584, train/total_loss/avg: 0.2764, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 18, num_updates: 1900, iterations: 1900, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 811ms, time_since_start: 48m 32s 711ms, eta: 02m 17s 087ms\n",
      "\u001b[32m2024-07-23T08:41:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:41:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:41:14 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:41:14 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:41:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:41:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:41:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:41:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, val/hateful_memes/cross_entropy: 2.7606, val/total_loss: 2.7606, val/hateful_memes/accuracy: 0.6040, val/hateful_memes/binary_f1: 0.4590, val/hateful_memes/roc_auc: 0.6888, num_updates: 1900, epoch: 18, iterations: 1900, max_updates: 2000, val_time: 22s 569ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.694216\n",
      "\u001b[32m2024-07-23T08:41:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1910/2000, train/hateful_memes/cross_entropy: 0.0515, train/hateful_memes/cross_entropy/avg: 0.2750, train/total_loss: 0.0515, train/total_loss/avg: 0.2750, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 18, num_updates: 1910, iterations: 1910, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 916ms, time_since_start: 49m 08s 204ms, eta: 02m 04s 386ms\n",
      "\u001b[32m2024-07-23T08:41:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1920/2000, train/hateful_memes/cross_entropy: 0.0584, train/hateful_memes/cross_entropy/avg: 0.2740, train/total_loss: 0.0584, train/total_loss/avg: 0.2740, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 18, num_updates: 1920, iterations: 1920, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 738ms, time_since_start: 49m 20s 942ms, eta: 01m 49s 039ms\n",
      "\u001b[32m2024-07-23T08:42:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1930/2000, train/hateful_memes/cross_entropy: 0.0584, train/hateful_memes/cross_entropy/avg: 0.2728, train/total_loss: 0.0584, train/total_loss/avg: 0.2728, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 19, num_updates: 1930, iterations: 1930, max_updates: 2000, lr: 0., ups: 0.91, time: 11s 246ms, time_since_start: 49m 32s 189ms, eta: 01m 24s 237ms\n",
      "\u001b[32m2024-07-23T08:42:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1940/2000, train/hateful_memes/cross_entropy: 0.0515, train/hateful_memes/cross_entropy/avg: 0.2716, train/total_loss: 0.0515, train/total_loss/avg: 0.2716, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 19, num_updates: 1940, iterations: 1940, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 696ms, time_since_start: 49m 44s 885ms, eta: 01m 21s 511ms\n",
      "\u001b[32m2024-07-23T08:42:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/2000, train/hateful_memes/cross_entropy: 0.0515, train/hateful_memes/cross_entropy/avg: 0.2706, train/total_loss: 0.0515, train/total_loss/avg: 0.2706, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 19, num_updates: 1950, iterations: 1950, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 306ms, time_since_start: 49m 57s 192ms, eta: 01m 05s 841ms\n",
      "\u001b[32m2024-07-23T08:42:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1960/2000, train/hateful_memes/cross_entropy: 0.0515, train/hateful_memes/cross_entropy/avg: 0.2695, train/total_loss: 0.0515, train/total_loss/avg: 0.2695, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 19, num_updates: 1960, iterations: 1960, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 671ms, time_since_start: 50m 09s 864ms, eta: 54s 234ms\n",
      "\u001b[32m2024-07-23T08:42:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1970/2000, train/hateful_memes/cross_entropy: 0.0515, train/hateful_memes/cross_entropy/avg: 0.2683, train/total_loss: 0.0515, train/total_loss/avg: 0.2683, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 19, num_updates: 1970, iterations: 1970, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 292ms, time_since_start: 50m 22s 156ms, eta: 39s 458ms\n",
      "\u001b[32m2024-07-23T08:43:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1980/2000, train/hateful_memes/cross_entropy: 0.0515, train/hateful_memes/cross_entropy/avg: 0.2671, train/total_loss: 0.0515, train/total_loss/avg: 0.2671, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 19, num_updates: 1980, iterations: 1980, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 734ms, time_since_start: 50m 34s 890ms, eta: 27s 251ms\n",
      "\u001b[32m2024-07-23T08:43:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1990/2000, train/hateful_memes/cross_entropy: 0.0506, train/hateful_memes/cross_entropy/avg: 0.2658, train/total_loss: 0.0506, train/total_loss/avg: 0.2658, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 19, num_updates: 1990, iterations: 1990, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 276ms, time_since_start: 50m 47s 167ms, eta: 13s 136ms\n",
      "\u001b[32m2024-07-23T08:43:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-23T08:43:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:43:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:43:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:43:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, train/hateful_memes/cross_entropy: 0.0506, train/hateful_memes/cross_entropy/avg: 0.2648, train/total_loss: 0.0506, train/total_loss/avg: 0.2648, max mem: 13427.0, experiment: hateful_memes_lr3e5, epoch: 19, num_updates: 2000, iterations: 2000, max_updates: 2000, lr: 0., ups: 0.37, time: 27s 315ms, time_since_start: 51m 14s 482ms, eta: 0ms\n",
      "\u001b[32m2024-07-23T08:43:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:43:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:43:55 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:43:55 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:43:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:44:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:44:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:44:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, val/hateful_memes/cross_entropy: 2.8154, val/total_loss: 2.8154, val/hateful_memes/accuracy: 0.6100, val/hateful_memes/binary_f1: 0.4744, val/hateful_memes/roc_auc: 0.6899, num_updates: 2000, epoch: 19, iterations: 2000, max_updates: 2000, val_time: 26s 330ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.694216\n",
      "\u001b[32m2024-07-23T08:44:14 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2024-07-23T08:44:14 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2024-07-23T08:44:14 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2024-07-23T08:44:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2024-07-23T08:44:20 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1400\n",
      "\u001b[32m2024-07-23T08:44:20 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1400\n",
      "\u001b[32m2024-07-23T08:44:20 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 14\n",
      "\u001b[32m2024-07-23T08:44:21 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2024-07-23T08:44:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.11s/it]\n",
      "\u001b[32m2024-07-23T08:44:29 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:44:29 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:44:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, val/hateful_memes/cross_entropy: 2.2042, val/total_loss: 2.2042, val/hateful_memes/accuracy: 0.6000, val/hateful_memes/binary_f1: 0.4413, val/hateful_memes/roc_auc: 0.6942\n",
      "\u001b[32m2024-07-23T08:44:29 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 51m 56s 498ms\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.139 MB of 0.139 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/cross_entropy ▇▇█▆▇▇▆▆▅▅▅▄▆▅▅▃▃▄▃▃▂▄▃▁▃▁▂▂▂▂▁▁▁▂▂▁▂▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/learning_rate ▁▃▆▆██████▆▆▆▆▆▆▆▆▆▆▆▆▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/total_loss ▇▇█▆▇▇▆▆▅▅▅▄▆▅▅▃▃▄▃▃▂▄▃▁▃▁▂▂▂▂▁▁▁▂▂▁▂▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val/hateful_memes/accuracy ▁▃▄▆▅▅▇█▆▆▆▆▇▇▇▆▇▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val/hateful_memes/binary_f1 ▁▄▅▇▆▆▇█▆▇▇▆▇▆▇▇▇▇▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/cross_entropy ▁▂▁▂▂▃▁▂▃▃▂▆▄▆▅▄█▆▆▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/hateful_memes/roc_auc ▁▄▆▇▆▇█▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val/total_loss ▁▂▁▂▂▃▁▂▃▃▂▆▄▆▅▄█▆▆▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/cross_entropy 0.06657\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/total_loss 0.06657\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step 1400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val/hateful_memes/accuracy 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val/hateful_memes/binary_f1 0.44134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/cross_entropy 1.7554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/hateful_memes/roc_auc 0.69422\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val/total_loss 1.7554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhateful_memes_lr3e5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/4b9cv470\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240723_075234-4b9cv470/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "Exception ignored in: <function WandbLogger.__del__ at 0x7f14d6898f70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf/utils/logger.py\", line 457, in __del__\n",
      "    self._wandb.finish()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 4262, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 390, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2100, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2108, in _finish\n",
      "    tel.feature.finish = True\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
      "    self._run._telemetry_callback(self._obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 798, in _telemetry_callback\n",
      "    self._telemetry_flush()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 809, in _telemetry_flush\n",
      "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 101, in _publish_telemetry\n",
      "    self._publish(rec)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_roberta/defaults.yaml \\\n",
    "model=visual_roberta \\\n",
    "dataset=hateful_memes \\\n",
    "training.log_interval=10 \\\n",
    "training.seed=2024 \\\n",
    "training.batch_size=80 \\\n",
    "training.evaluation_interval=100 \\\n",
    "training.experiment_name=hateful_memes_lr3e5 \\\n",
    "training.max_updates=2000 \\\n",
    "optimizer.params.lr=3e-5 \\\n",
    "training.fp16=True \\\n",
    "scheduler.params.num_warmup_steps=200 \\\n",
    "checkpoint.max_to_keep=1 \\\n",
    "training.wandb.enabled=True \\\n",
    "run_type=train_val\n",
    "# checkpoint.resume=True \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_roberta/defaults.yaml\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_roberta\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 10\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.seed to 2024\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 80\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 100\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.experiment_name to hateful_memes_lr5e5\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 2000\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5e-5\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.fp16 to True\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 200\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.wandb.enabled to True\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_roberta/defaults.yaml', 'model=visual_roberta', 'dataset=hateful_memes', 'training.log_interval=10', 'training.seed=2024', 'training.batch_size=80', 'training.evaluation_interval=100', 'training.experiment_name=hateful_memes_lr5e5', 'training.max_updates=2000', 'optimizer.params.lr=5e-5', 'training.fp16=True', 'scheduler.params.num_warmup_steps=200', 'checkpoint.max_to_keep=1', 'training.wandb.enabled=True', 'run_type=train_val'])\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf_cli.run: \u001b[0mTorch version: 1.11.0+cu102\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf_cli.run: \u001b[0mUsing seed 2024\n",
      "\u001b[32m2024-07-23T08:55:38 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/zeus/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/zeus/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/zeus/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at /home/zeus/.cache/huggingface/transformers/dfe8f1ad04cb25b61a647e3d13620f9bf0a0f51d277897b232a5735297134132.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\u001b[32m2024-07-23T08:55:39 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T08:55:39 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T08:55:39 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T08:55:39 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2024-07-23T08:55:39 | torch.distributed.nn.jit.instantiator: \u001b[0mCreated a temporary directory at /tmp/tmpmxqnngjf\n",
      "\u001b[32m2024-07-23T08:55:39 | torch.distributed.nn.jit.instantiator: \u001b[0mWriting /tmp/tmpmxqnngjf/_remote_module_non_sriptable.py\n",
      "HI BRO {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': ['cross_entropy'], 'model': 'visual_roberta'}\n",
      "SELFIE {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': ['cross_entropy'], 'model': 'visual_roberta'}\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "SELF ROBERT CONFIG RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "roberta-base\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/zeus/.cache/torch/mmf/distributed_-1/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VisualRobertaBase: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing VisualRobertaBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualRobertaBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualRobertaBase were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.projection.bias', 'roberta.embeddings.projection.weight', 'roberta.embeddings.token_type_embeddings_visual.weight', 'roberta.embeddings.position_ids', 'roberta.embeddings.position_embeddings_visual.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-07-23T08:55:45 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2024-07-23T08:55:45 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikrant17\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/wandb/run-20240723_085546-ebdvaph8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhateful_memes_lr5e5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/ebdvaph8\u001b[0m\n",
      "\u001b[32m2024-07-23T08:55:49 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2024-07-23T08:55:49 | mmf.trainers.mmf_trainer: \u001b[0mVisualRoberta(\n",
      "  (model): VisualRobertaForClassification(\n",
      "    (roberta): VisualRobertaBase(\n",
      "      (embeddings): RobertaVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(1, 768)\n",
      "        (position_embeddings_visual): Embedding(514, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2024-07-23T08:55:49 | mmf.utils.general: \u001b[0mTotal Parameters: 127208450. Trained Parameters: 127208450\n",
      "\u001b[32m2024-07-23T08:55:49 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2024-07-23T08:56:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10/2000, train/hateful_memes/cross_entropy: 0.7740, train/hateful_memes/cross_entropy/avg: 0.7740, train/total_loss: 0.7740, train/total_loss/avg: 0.7740, max mem: 13359.0, experiment: hateful_memes_lr5e5, epoch: 1, num_updates: 10, iterations: 10, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 386ms, time_since_start: 17s 710ms, eta: 47m 30s 358ms\n",
      "\u001b[32m2024-07-23T08:56:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20/2000, train/hateful_memes/cross_entropy: 0.6506, train/hateful_memes/cross_entropy/avg: 0.7123, train/total_loss: 0.6506, train/total_loss/avg: 0.7123, max mem: 13359.0, experiment: hateful_memes_lr5e5, epoch: 1, num_updates: 20, iterations: 20, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 726ms, time_since_start: 30s 436ms, eta: 44m 56s 179ms\n",
      "\u001b[32m2024-07-23T08:56:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 30/2000, train/hateful_memes/cross_entropy: 0.6831, train/hateful_memes/cross_entropy/avg: 0.7026, train/total_loss: 0.6831, train/total_loss/avg: 0.7026, max mem: 13359.0, experiment: hateful_memes_lr5e5, epoch: 1, num_updates: 30, iterations: 30, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 850ms, time_since_start: 43s 287ms, eta: 45m 08s 778ms\n",
      "\u001b[32m2024-07-23T08:56:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 40/2000, train/hateful_memes/cross_entropy: 0.6695, train/hateful_memes/cross_entropy/avg: 0.6943, train/total_loss: 0.6695, train/total_loss/avg: 0.6943, max mem: 13359.0, experiment: hateful_memes_lr5e5, epoch: 1, num_updates: 40, iterations: 40, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 786ms, time_since_start: 56s 074ms, eta: 44m 41s 680ms\n",
      "\u001b[32m2024-07-23T08:56:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/2000, train/hateful_memes/cross_entropy: 0.6695, train/hateful_memes/cross_entropy/avg: 0.6704, train/total_loss: 0.6695, train/total_loss/avg: 0.6704, max mem: 13359.0, experiment: hateful_memes_lr5e5, epoch: 1, num_updates: 50, iterations: 50, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 894ms, time_since_start: 01m 08s 969ms, eta: 44m 50s 518ms\n",
      "\u001b[32m2024-07-23T08:57:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 60/2000, train/hateful_memes/cross_entropy: 0.6506, train/hateful_memes/cross_entropy/avg: 0.6613, train/total_loss: 0.6506, train/total_loss/avg: 0.6613, max mem: 13359.0, experiment: hateful_memes_lr5e5, epoch: 1, num_updates: 60, iterations: 60, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 891ms, time_since_start: 01m 21s 860ms, eta: 44m 35s 968ms\n",
      "\u001b[32m2024-07-23T08:57:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 70/2000, train/hateful_memes/cross_entropy: 0.6695, train/hateful_memes/cross_entropy/avg: 0.6653, train/total_loss: 0.6695, train/total_loss/avg: 0.6653, max mem: 13359.0, experiment: hateful_memes_lr5e5, epoch: 1, num_updates: 70, iterations: 70, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 109ms, time_since_start: 01m 34s 969ms, eta: 45m 07s 226ms\n",
      "\u001b[32m2024-07-23T08:57:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 80/2000, train/hateful_memes/cross_entropy: 0.6506, train/hateful_memes/cross_entropy/avg: 0.6632, train/total_loss: 0.6506, train/total_loss/avg: 0.6632, max mem: 13359.0, experiment: hateful_memes_lr5e5, epoch: 1, num_updates: 80, iterations: 80, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 982ms, time_since_start: 01m 47s 952ms, eta: 44m 27s 214ms\n",
      "\u001b[32m2024-07-23T08:57:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 90/2000, train/hateful_memes/cross_entropy: 0.6695, train/hateful_memes/cross_entropy/avg: 0.6672, train/total_loss: 0.6695, train/total_loss/avg: 0.6672, max mem: 13359.0, experiment: hateful_memes_lr5e5, epoch: 1, num_updates: 90, iterations: 90, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 984ms, time_since_start: 02m 937ms, eta: 44m 13s 743ms\n",
      "\u001b[32m2024-07-23T08:57:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, train/hateful_memes/cross_entropy: 0.6506, train/hateful_memes/cross_entropy/avg: 0.6651, train/total_loss: 0.6506, train/total_loss/avg: 0.6651, max mem: 13359.0, experiment: hateful_memes_lr5e5, epoch: 1, num_updates: 100, iterations: 100, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 951ms, time_since_start: 02m 13s 889ms, eta: 43m 53s 131ms\n",
      "\u001b[32m2024-07-23T08:57:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T08:57:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T08:58:07 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T08:58:07 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T08:58:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T08:58:11 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T08:58:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T08:58:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T08:58:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, val/hateful_memes/cross_entropy: 0.7494, val/total_loss: 0.7494, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5333, num_updates: 100, epoch: 1, iterations: 100, max_updates: 2000, val_time: 35s 381ms, best_update: 100, best_iteration: 100, best_val/hateful_memes/roc_auc: 0.533312\n",
      "\u001b[32m2024-07-23T08:58:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 110/2000, train/hateful_memes/cross_entropy: 0.6506, train/hateful_memes/cross_entropy/avg: 0.6602, train/total_loss: 0.6506, train/total_loss/avg: 0.6602, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 2, num_updates: 110, iterations: 110, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 420ms, time_since_start: 03m 01s 699ms, eta: 41m 51s 751ms\n",
      "\u001b[32m2024-07-23T08:59:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 120/2000, train/hateful_memes/cross_entropy: 0.6506, train/hateful_memes/cross_entropy/avg: 0.6633, train/total_loss: 0.6506, train/total_loss/avg: 0.6633, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 2, num_updates: 120, iterations: 120, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 241ms, time_since_start: 03m 14s 940ms, eta: 44m 23s 622ms\n",
      "\u001b[32m2024-07-23T08:59:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 130/2000, train/hateful_memes/cross_entropy: 0.6506, train/hateful_memes/cross_entropy/avg: 0.6602, train/total_loss: 0.6506, train/total_loss/avg: 0.6602, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 2, num_updates: 130, iterations: 130, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 721ms, time_since_start: 03m 27s 661ms, eta: 42m 25s 453ms\n",
      "\u001b[32m2024-07-23T08:59:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 140/2000, train/hateful_memes/cross_entropy: 0.6485, train/hateful_memes/cross_entropy/avg: 0.6506, train/total_loss: 0.6485, train/total_loss/avg: 0.6506, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 2, num_updates: 140, iterations: 140, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 289ms, time_since_start: 03m 40s 951ms, eta: 44m 04s 835ms\n",
      "\u001b[32m2024-07-23T08:59:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/2000, train/hateful_memes/cross_entropy: 0.6485, train/hateful_memes/cross_entropy/avg: 0.6484, train/total_loss: 0.6485, train/total_loss/avg: 0.6484, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 2, num_updates: 150, iterations: 150, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 891ms, time_since_start: 03m 53s 842ms, eta: 42m 31s 910ms\n",
      "\u001b[32m2024-07-23T08:59:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 160/2000, train/hateful_memes/cross_entropy: 0.6464, train/hateful_memes/cross_entropy/avg: 0.6407, train/total_loss: 0.6464, train/total_loss/avg: 0.6407, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 2, num_updates: 160, iterations: 160, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 381ms, time_since_start: 04m 07s 224ms, eta: 43m 54s 509ms\n",
      "\u001b[32m2024-07-23T09:00:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 170/2000, train/hateful_memes/cross_entropy: 0.6464, train/hateful_memes/cross_entropy/avg: 0.6393, train/total_loss: 0.6464, train/total_loss/avg: 0.6393, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 2, num_updates: 170, iterations: 170, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 724ms, time_since_start: 04m 19s 948ms, eta: 41m 31s 612ms\n",
      "\u001b[32m2024-07-23T09:00:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 180/2000, train/hateful_memes/cross_entropy: 0.6223, train/hateful_memes/cross_entropy/avg: 0.6350, train/total_loss: 0.6223, train/total_loss/avg: 0.6350, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 2, num_updates: 180, iterations: 180, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 405ms, time_since_start: 04m 33s 354ms, eta: 43m 30s 655ms\n",
      "\u001b[32m2024-07-23T09:00:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 190/2000, train/hateful_memes/cross_entropy: 0.6226, train/hateful_memes/cross_entropy/avg: 0.6344, train/total_loss: 0.6226, train/total_loss/avg: 0.6344, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 2, num_updates: 190, iterations: 190, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 002ms, time_since_start: 04m 46s 356ms, eta: 41m 58s 126ms\n",
      "\u001b[32m2024-07-23T09:00:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, train/hateful_memes/cross_entropy: 0.6223, train/hateful_memes/cross_entropy/avg: 0.6324, train/total_loss: 0.6223, train/total_loss/avg: 0.6324, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 2, num_updates: 200, iterations: 200, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 506ms, time_since_start: 04m 59s 863ms, eta: 43m 21s 299ms\n",
      "\u001b[32m2024-07-23T09:00:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:00:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:00:53 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:00:53 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:00:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:00:57 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T09:01:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:01:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:01:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, val/hateful_memes/cross_entropy: 0.8780, val/total_loss: 0.8780, val/hateful_memes/accuracy: 0.5260, val/hateful_memes/binary_f1: 0.2074, val/hateful_memes/roc_auc: 0.5868, num_updates: 200, epoch: 2, iterations: 200, max_updates: 2000, val_time: 35s 351ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.586832\n",
      "\u001b[32m2024-07-23T09:01:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 210/2000, train/hateful_memes/cross_entropy: 0.6170, train/hateful_memes/cross_entropy/avg: 0.6288, train/total_loss: 0.6170, train/total_loss/avg: 0.6288, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 2, num_updates: 210, iterations: 210, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 153ms, time_since_start: 05m 48s 375ms, eta: 41m 59s 374ms\n",
      "\u001b[32m2024-07-23T09:01:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 220/2000, train/hateful_memes/cross_entropy: 0.6167, train/hateful_memes/cross_entropy/avg: 0.6281, train/total_loss: 0.6167, train/total_loss/avg: 0.6281, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 3, num_updates: 220, iterations: 220, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 361ms, time_since_start: 06m 737ms, eta: 39m 14s 347ms\n",
      "\u001b[32m2024-07-23T09:01:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 230/2000, train/hateful_memes/cross_entropy: 0.6167, train/hateful_memes/cross_entropy/avg: 0.6299, train/total_loss: 0.6167, train/total_loss/avg: 0.6299, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 3, num_updates: 230, iterations: 230, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 441ms, time_since_start: 06m 13s 179ms, eta: 39m 16s 363ms\n",
      "\u001b[32m2024-07-23T09:02:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 240/2000, train/hateful_memes/cross_entropy: 0.6158, train/hateful_memes/cross_entropy/avg: 0.6258, train/total_loss: 0.6158, train/total_loss/avg: 0.6258, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 3, num_updates: 240, iterations: 240, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 478ms, time_since_start: 06m 26s 657ms, eta: 42m 18s 275ms\n",
      "\u001b[32m2024-07-23T09:02:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/2000, train/hateful_memes/cross_entropy: 0.6158, train/hateful_memes/cross_entropy/avg: 0.6243, train/total_loss: 0.6158, train/total_loss/avg: 0.6243, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 3, num_updates: 250, iterations: 250, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 481ms, time_since_start: 06m 39s 139ms, eta: 38m 57s 247ms\n",
      "\u001b[32m2024-07-23T09:02:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 260/2000, train/hateful_memes/cross_entropy: 0.6167, train/hateful_memes/cross_entropy/avg: 0.6240, train/total_loss: 0.6167, train/total_loss/avg: 0.6240, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 3, num_updates: 260, iterations: 260, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 516ms, time_since_start: 06m 52s 655ms, eta: 41m 56s 480ms\n",
      "\u001b[32m2024-07-23T09:02:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 270/2000, train/hateful_memes/cross_entropy: 0.6143, train/hateful_memes/cross_entropy/avg: 0.6214, train/total_loss: 0.6143, train/total_loss/avg: 0.6214, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 3, num_updates: 270, iterations: 270, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 515ms, time_since_start: 07m 05s 171ms, eta: 38m 36s 838ms\n",
      "\u001b[32m2024-07-23T09:03:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 280/2000, train/hateful_memes/cross_entropy: 0.6112, train/hateful_memes/cross_entropy/avg: 0.6163, train/total_loss: 0.6112, train/total_loss/avg: 0.6163, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 3, num_updates: 280, iterations: 280, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 692ms, time_since_start: 07m 18s 864ms, eta: 41m 59s 955ms\n",
      "\u001b[32m2024-07-23T09:03:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 290/2000, train/hateful_memes/cross_entropy: 0.5951, train/hateful_memes/cross_entropy/avg: 0.6150, train/total_loss: 0.5951, train/total_loss/avg: 0.6150, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 3, num_updates: 290, iterations: 290, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 779ms, time_since_start: 07m 31s 643ms, eta: 38m 58s 280ms\n",
      "\u001b[32m2024-07-23T09:03:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, train/hateful_memes/cross_entropy: 0.5876, train/hateful_memes/cross_entropy/avg: 0.6096, train/total_loss: 0.5876, train/total_loss/avg: 0.6096, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 3, num_updates: 300, iterations: 300, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 972ms, time_since_start: 07m 45s 616ms, eta: 42m 21s 510ms\n",
      "\u001b[32m2024-07-23T09:03:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:03:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:03:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:03:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:03:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:03:43 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T09:03:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:04:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:04:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, val/hateful_memes/cross_entropy: 0.8556, val/total_loss: 0.8556, val/hateful_memes/accuracy: 0.5460, val/hateful_memes/binary_f1: 0.2508, val/hateful_memes/roc_auc: 0.6468, num_updates: 300, epoch: 3, iterations: 300, max_updates: 2000, val_time: 31s 044ms, best_update: 300, best_iteration: 300, best_val/hateful_memes/roc_auc: 0.646832\n",
      "\u001b[32m2024-07-23T09:04:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 310/2000, train/hateful_memes/cross_entropy: 0.5767, train/hateful_memes/cross_entropy/avg: 0.6073, train/total_loss: 0.5767, train/total_loss/avg: 0.6073, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 3, num_updates: 310, iterations: 310, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 774ms, time_since_start: 08m 29s 444ms, eta: 38m 29s 986ms\n",
      "\u001b[32m2024-07-23T09:04:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 320/2000, train/hateful_memes/cross_entropy: 0.5626, train/hateful_memes/cross_entropy/avg: 0.6045, train/total_loss: 0.5626, train/total_loss/avg: 0.6045, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 3, num_updates: 320, iterations: 320, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 626ms, time_since_start: 08m 42s 070ms, eta: 37m 49s 663ms\n",
      "\u001b[32m2024-07-23T09:04:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 330/2000, train/hateful_memes/cross_entropy: 0.5560, train/hateful_memes/cross_entropy/avg: 0.6016, train/total_loss: 0.5560, train/total_loss/avg: 0.6016, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 4, num_updates: 330, iterations: 330, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 322ms, time_since_start: 08m 54s 392ms, eta: 36m 41s 913ms\n",
      "\u001b[32m2024-07-23T09:04:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 340/2000, train/hateful_memes/cross_entropy: 0.5626, train/hateful_memes/cross_entropy/avg: 0.6030, train/total_loss: 0.5626, train/total_loss/avg: 0.6030, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 4, num_updates: 340, iterations: 340, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 262ms, time_since_start: 09m 07s 655ms, eta: 39m 15s 733ms\n",
      "\u001b[32m2024-07-23T09:05:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/2000, train/hateful_memes/cross_entropy: 0.5560, train/hateful_memes/cross_entropy/avg: 0.5999, train/total_loss: 0.5560, train/total_loss/avg: 0.5999, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 4, num_updates: 350, iterations: 350, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 585ms, time_since_start: 09m 20s 241ms, eta: 37m 01s 963ms\n",
      "\u001b[32m2024-07-23T09:05:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 360/2000, train/hateful_memes/cross_entropy: 0.5626, train/hateful_memes/cross_entropy/avg: 0.5992, train/total_loss: 0.5626, train/total_loss/avg: 0.5992, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 4, num_updates: 360, iterations: 360, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 325ms, time_since_start: 09m 33s 566ms, eta: 38m 58s 279ms\n",
      "\u001b[32m2024-07-23T09:05:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 370/2000, train/hateful_memes/cross_entropy: 0.5560, train/hateful_memes/cross_entropy/avg: 0.5952, train/total_loss: 0.5560, train/total_loss/avg: 0.5952, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 4, num_updates: 370, iterations: 370, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 775ms, time_since_start: 09m 46s 342ms, eta: 37m 08s 236ms\n",
      "\u001b[32m2024-07-23T09:05:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 380/2000, train/hateful_memes/cross_entropy: 0.5541, train/hateful_memes/cross_entropy/avg: 0.5918, train/total_loss: 0.5541, train/total_loss/avg: 0.5918, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 4, num_updates: 380, iterations: 380, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 445ms, time_since_start: 09m 59s 788ms, eta: 38m 50s 715ms\n",
      "\u001b[32m2024-07-23T09:05:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 390/2000, train/hateful_memes/cross_entropy: 0.5380, train/hateful_memes/cross_entropy/avg: 0.5896, train/total_loss: 0.5380, train/total_loss/avg: 0.5896, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 4, num_updates: 390, iterations: 390, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 856ms, time_since_start: 10m 12s 644ms, eta: 36m 54s 783ms\n",
      "\u001b[32m2024-07-23T09:06:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, train/hateful_memes/cross_entropy: 0.5325, train/hateful_memes/cross_entropy/avg: 0.5870, train/total_loss: 0.5325, train/total_loss/avg: 0.5870, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 4, num_updates: 400, iterations: 400, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 414ms, time_since_start: 10m 26s 059ms, eta: 38m 16s 602ms\n",
      "\u001b[32m2024-07-23T09:06:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:06:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:06:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:06:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:06:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:06:23 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T09:06:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:06:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:06:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, val/hateful_memes/cross_entropy: 0.7261, val/total_loss: 0.7261, val/hateful_memes/accuracy: 0.5900, val/hateful_memes/binary_f1: 0.4938, val/hateful_memes/roc_auc: 0.6563, num_updates: 400, epoch: 4, iterations: 400, max_updates: 2000, val_time: 31s 031ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.656320\n",
      "\u001b[32m2024-07-23T09:06:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 410/2000, train/hateful_memes/cross_entropy: 0.5190, train/hateful_memes/cross_entropy/avg: 0.5822, train/total_loss: 0.5190, train/total_loss/avg: 0.5822, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 4, num_updates: 410, iterations: 410, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 996ms, time_since_start: 11m 10s 094ms, eta: 36m 51s 075ms\n",
      "\u001b[32m2024-07-23T09:07:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 420/2000, train/hateful_memes/cross_entropy: 0.5073, train/hateful_memes/cross_entropy/avg: 0.5783, train/total_loss: 0.5073, train/total_loss/avg: 0.5783, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 4, num_updates: 420, iterations: 420, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 287ms, time_since_start: 11m 23s 381ms, eta: 37m 26s 335ms\n",
      "\u001b[32m2024-07-23T09:07:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 430/2000, train/hateful_memes/cross_entropy: 0.5062, train/hateful_memes/cross_entropy/avg: 0.5746, train/total_loss: 0.5062, train/total_loss/avg: 0.5746, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 5, num_updates: 430, iterations: 430, max_updates: 2000, lr: 0.00004, ups: 0.91, time: 11s 993ms, time_since_start: 11m 35s 374ms, eta: 33m 34s 761ms\n",
      "\u001b[32m2024-07-23T09:07:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 440/2000, train/hateful_memes/cross_entropy: 0.5062, train/hateful_memes/cross_entropy/avg: 0.5732, train/total_loss: 0.5062, train/total_loss/avg: 0.5732, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 5, num_updates: 440, iterations: 440, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 763ms, time_since_start: 11m 48s 138ms, eta: 35m 30s 469ms\n",
      "\u001b[32m2024-07-23T09:07:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/2000, train/hateful_memes/cross_entropy: 0.4968, train/hateful_memes/cross_entropy/avg: 0.5692, train/total_loss: 0.4968, train/total_loss/avg: 0.5692, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 5, num_updates: 450, iterations: 450, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 296ms, time_since_start: 12m 01s 434ms, eta: 36m 45s 184ms\n",
      "\u001b[32m2024-07-23T09:07:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 460/2000, train/hateful_memes/cross_entropy: 0.4861, train/hateful_memes/cross_entropy/avg: 0.5650, train/total_loss: 0.4861, train/total_loss/avg: 0.5650, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 5, num_updates: 460, iterations: 460, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 756ms, time_since_start: 12m 14s 190ms, eta: 35m 01s 959ms\n",
      "\u001b[32m2024-07-23T09:08:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 470/2000, train/hateful_memes/cross_entropy: 0.4780, train/hateful_memes/cross_entropy/avg: 0.5632, train/total_loss: 0.4780, train/total_loss/avg: 0.5632, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 5, num_updates: 470, iterations: 470, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 331ms, time_since_start: 12m 27s 522ms, eta: 36m 22s 555ms\n",
      "\u001b[32m2024-07-23T09:08:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 480/2000, train/hateful_memes/cross_entropy: 0.4759, train/hateful_memes/cross_entropy/avg: 0.5606, train/total_loss: 0.4759, train/total_loss/avg: 0.5606, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 5, num_updates: 480, iterations: 480, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 972ms, time_since_start: 12m 40s 494ms, eta: 35m 09s 813ms\n",
      "\u001b[32m2024-07-23T09:08:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 490/2000, train/hateful_memes/cross_entropy: 0.4674, train/hateful_memes/cross_entropy/avg: 0.5562, train/total_loss: 0.4674, train/total_loss/avg: 0.5562, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 5, num_updates: 490, iterations: 490, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 486ms, time_since_start: 12m 53s 981ms, eta: 36m 18s 986ms\n",
      "\u001b[32m2024-07-23T09:08:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, train/hateful_memes/cross_entropy: 0.4674, train/hateful_memes/cross_entropy/avg: 0.5538, train/total_loss: 0.4674, train/total_loss/avg: 0.5538, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 5, num_updates: 500, iterations: 500, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 942ms, time_since_start: 13m 06s 923ms, eta: 34m 37s 274ms\n",
      "\u001b[32m2024-07-23T09:08:52 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:08:52 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:09:00 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:09:00 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:09:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:09:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:09:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:09:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, val/hateful_memes/cross_entropy: 0.7709, val/total_loss: 0.7709, val/hateful_memes/accuracy: 0.5780, val/hateful_memes/binary_f1: 0.4790, val/hateful_memes/roc_auc: 0.6507, num_updates: 500, epoch: 5, iterations: 500, max_updates: 2000, val_time: 41s 826ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.656320\n",
      "\u001b[32m2024-07-23T09:09:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 510/2000, train/hateful_memes/cross_entropy: 0.4505, train/hateful_memes/cross_entropy/avg: 0.5517, train/total_loss: 0.4505, train/total_loss/avg: 0.5517, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 5, num_updates: 510, iterations: 510, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 442ms, time_since_start: 14m 02s 202ms, eta: 35m 43s 216ms\n",
      "\u001b[32m2024-07-23T09:10:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 520/2000, train/hateful_memes/cross_entropy: 0.4505, train/hateful_memes/cross_entropy/avg: 0.5501, train/total_loss: 0.4505, train/total_loss/avg: 0.5501, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 5, num_updates: 520, iterations: 520, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 204ms, time_since_start: 14m 15s 407ms, eta: 34m 51s 102ms\n",
      "\u001b[32m2024-07-23T09:10:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 530/2000, train/hateful_memes/cross_entropy: 0.4505, train/hateful_memes/cross_entropy/avg: 0.5493, train/total_loss: 0.4505, train/total_loss/avg: 0.5493, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 5, num_updates: 530, iterations: 530, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 282ms, time_since_start: 14m 28s 689ms, eta: 34m 49s 132ms\n",
      "\u001b[32m2024-07-23T09:10:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 540/2000, train/hateful_memes/cross_entropy: 0.4475, train/hateful_memes/cross_entropy/avg: 0.5458, train/total_loss: 0.4475, train/total_loss/avg: 0.5458, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 6, num_updates: 540, iterations: 540, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 538ms, time_since_start: 14m 41s 227ms, eta: 32m 38s 724ms\n",
      "\u001b[32m2024-07-23T09:10:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/2000, train/hateful_memes/cross_entropy: 0.4475, train/hateful_memes/cross_entropy/avg: 0.5453, train/total_loss: 0.4475, train/total_loss/avg: 0.5453, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 6, num_updates: 550, iterations: 550, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 439ms, time_since_start: 14m 53s 667ms, eta: 32m 09s 985ms\n",
      "\u001b[32m2024-07-23T09:10:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 560/2000, train/hateful_memes/cross_entropy: 0.4387, train/hateful_memes/cross_entropy/avg: 0.5419, train/total_loss: 0.4387, train/total_loss/avg: 0.5419, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 6, num_updates: 560, iterations: 560, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 333ms, time_since_start: 15m 07s 000ms, eta: 34m 14s 443ms\n",
      "\u001b[32m2024-07-23T09:11:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 570/2000, train/hateful_memes/cross_entropy: 0.4387, train/hateful_memes/cross_entropy/avg: 0.5403, train/total_loss: 0.4387, train/total_loss/avg: 0.5403, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 6, num_updates: 570, iterations: 570, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 419ms, time_since_start: 15m 19s 420ms, eta: 31m 40s 276ms\n",
      "\u001b[32m2024-07-23T09:11:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 580/2000, train/hateful_memes/cross_entropy: 0.4379, train/hateful_memes/cross_entropy/avg: 0.5365, train/total_loss: 0.4379, train/total_loss/avg: 0.5365, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 6, num_updates: 580, iterations: 580, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 694ms, time_since_start: 15m 33s 114ms, eta: 34m 40s 676ms\n",
      "\u001b[32m2024-07-23T09:11:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 590/2000, train/hateful_memes/cross_entropy: 0.4212, train/hateful_memes/cross_entropy/avg: 0.5339, train/total_loss: 0.4212, train/total_loss/avg: 0.5339, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 6, num_updates: 590, iterations: 590, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 619ms, time_since_start: 15m 45s 733ms, eta: 31m 43s 863ms\n",
      "\u001b[32m2024-07-23T09:11:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, train/hateful_memes/cross_entropy: 0.4212, train/hateful_memes/cross_entropy/avg: 0.5330, train/total_loss: 0.4212, train/total_loss/avg: 0.5330, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 6, num_updates: 600, iterations: 600, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 670ms, time_since_start: 15m 59s 404ms, eta: 34m 07s 854ms\n",
      "\u001b[32m2024-07-23T09:11:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:11:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:11:52 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:11:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:11:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:11:56 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T09:12:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:12:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:12:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, val/hateful_memes/cross_entropy: 1.0255, val/total_loss: 1.0255, val/hateful_memes/accuracy: 0.5680, val/hateful_memes/binary_f1: 0.3609, val/hateful_memes/roc_auc: 0.6696, num_updates: 600, epoch: 6, iterations: 600, max_updates: 2000, val_time: 31s 770ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.669616\n",
      "\u001b[32m2024-07-23T09:12:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 610/2000, train/hateful_memes/cross_entropy: 0.4212, train/hateful_memes/cross_entropy/avg: 0.5306, train/total_loss: 0.4212, train/total_loss/avg: 0.5306, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 6, num_updates: 610, iterations: 610, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 718ms, time_since_start: 16m 43s 902ms, eta: 31m 31s 660ms\n",
      "\u001b[32m2024-07-23T09:12:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 620/2000, train/hateful_memes/cross_entropy: 0.4379, train/hateful_memes/cross_entropy/avg: 0.5300, train/total_loss: 0.4379, train/total_loss/avg: 0.5300, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 6, num_updates: 620, iterations: 620, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 472ms, time_since_start: 16m 57s 374ms, eta: 33m 09s 354ms\n",
      "\u001b[32m2024-07-23T09:12:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 630/2000, train/hateful_memes/cross_entropy: 0.4379, train/hateful_memes/cross_entropy/avg: 0.5266, train/total_loss: 0.4379, train/total_loss/avg: 0.5266, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 6, num_updates: 630, iterations: 630, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 485ms, time_since_start: 17m 09s 860ms, eta: 30m 30s 300ms\n",
      "\u001b[32m2024-07-23T09:13:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 640/2000, train/hateful_memes/cross_entropy: 0.4379, train/hateful_memes/cross_entropy/avg: 0.5265, train/total_loss: 0.4379, train/total_loss/avg: 0.5265, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 6, num_updates: 640, iterations: 640, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 572ms, time_since_start: 17m 22s 433ms, eta: 30m 29s 598ms\n",
      "\u001b[32m2024-07-23T09:13:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/2000, train/hateful_memes/cross_entropy: 0.4379, train/hateful_memes/cross_entropy/avg: 0.5241, train/total_loss: 0.4379, train/total_loss/avg: 0.5241, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 7, num_updates: 650, iterations: 650, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 064ms, time_since_start: 17m 34s 498ms, eta: 29m 02s 744ms\n",
      "\u001b[32m2024-07-23T09:13:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 660/2000, train/hateful_memes/cross_entropy: 0.4379, train/hateful_memes/cross_entropy/avg: 0.5218, train/total_loss: 0.4379, train/total_loss/avg: 0.5218, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 7, num_updates: 660, iterations: 660, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 382ms, time_since_start: 17m 47s 880ms, eta: 31m 58s 824ms\n",
      "\u001b[32m2024-07-23T09:13:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 670/2000, train/hateful_memes/cross_entropy: 0.3867, train/hateful_memes/cross_entropy/avg: 0.5193, train/total_loss: 0.3867, train/total_loss/avg: 0.5193, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 7, num_updates: 670, iterations: 670, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 808ms, time_since_start: 18m 689ms, eta: 30m 22s 791ms\n",
      "\u001b[32m2024-07-23T09:13:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 680/2000, train/hateful_memes/cross_entropy: 0.3852, train/hateful_memes/cross_entropy/avg: 0.5168, train/total_loss: 0.3852, train/total_loss/avg: 0.5168, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 7, num_updates: 680, iterations: 680, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 464ms, time_since_start: 18m 14s 153ms, eta: 31m 41s 667ms\n",
      "\u001b[32m2024-07-23T09:14:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 690/2000, train/hateful_memes/cross_entropy: 0.3852, train/hateful_memes/cross_entropy/avg: 0.5138, train/total_loss: 0.3852, train/total_loss/avg: 0.5138, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 7, num_updates: 690, iterations: 690, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 858ms, time_since_start: 18m 27s 012ms, eta: 30m 02s 401ms\n",
      "\u001b[32m2024-07-23T09:14:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, train/hateful_memes/cross_entropy: 0.3710, train/hateful_memes/cross_entropy/avg: 0.5114, train/total_loss: 0.3710, train/total_loss/avg: 0.5114, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 7, num_updates: 700, iterations: 700, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 596ms, time_since_start: 18m 40s 608ms, eta: 31m 31s 301ms\n",
      "\u001b[32m2024-07-23T09:14:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:14:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:14:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:14:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:14:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:14:37 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T09:14:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:14:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:14:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, val/hateful_memes/cross_entropy: 0.9273, val/total_loss: 0.9273, val/hateful_memes/accuracy: 0.6260, val/hateful_memes/binary_f1: 0.5854, val/hateful_memes/roc_auc: 0.6877, num_updates: 700, epoch: 7, iterations: 700, max_updates: 2000, val_time: 26s 634ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.687728\n",
      "\u001b[32m2024-07-23T09:15:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 710/2000, train/hateful_memes/cross_entropy: 0.3710, train/hateful_memes/cross_entropy/avg: 0.5099, train/total_loss: 0.3710, train/total_loss/avg: 0.5099, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 7, num_updates: 710, iterations: 710, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 023ms, time_since_start: 19m 20s 274ms, eta: 29m 57s 577ms\n",
      "\u001b[32m2024-07-23T09:15:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 720/2000, train/hateful_memes/cross_entropy: 0.3687, train/hateful_memes/cross_entropy/avg: 0.5079, train/total_loss: 0.3687, train/total_loss/avg: 0.5079, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 7, num_updates: 720, iterations: 720, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 478ms, time_since_start: 19m 33s 753ms, eta: 30m 46s 084ms\n",
      "\u001b[32m2024-07-23T09:15:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 730/2000, train/hateful_memes/cross_entropy: 0.3664, train/hateful_memes/cross_entropy/avg: 0.5050, train/total_loss: 0.3664, train/total_loss/avg: 0.5050, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 7, num_updates: 730, iterations: 730, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 897ms, time_since_start: 19m 46s 650ms, eta: 29m 12s 619ms\n",
      "\u001b[32m2024-07-23T09:15:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 740/2000, train/hateful_memes/cross_entropy: 0.3664, train/hateful_memes/cross_entropy/avg: 0.5025, train/total_loss: 0.3664, train/total_loss/avg: 0.5025, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 7, num_updates: 740, iterations: 740, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 732ms, time_since_start: 20m 382ms, eta: 30m 51s 418ms\n",
      "\u001b[32m2024-07-23T09:15:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/2000, train/hateful_memes/cross_entropy: 0.3585, train/hateful_memes/cross_entropy/avg: 0.4998, train/total_loss: 0.3585, train/total_loss/avg: 0.4998, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 8, num_updates: 750, iterations: 750, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 659ms, time_since_start: 20m 13s 042ms, eta: 28m 13s 211ms\n",
      "\u001b[32m2024-07-23T09:16:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 760/2000, train/hateful_memes/cross_entropy: 0.3559, train/hateful_memes/cross_entropy/avg: 0.4974, train/total_loss: 0.3559, train/total_loss/avg: 0.4974, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 8, num_updates: 760, iterations: 760, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 840ms, time_since_start: 20m 25s 882ms, eta: 28m 23s 654ms\n",
      "\u001b[32m2024-07-23T09:16:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 770/2000, train/hateful_memes/cross_entropy: 0.3477, train/hateful_memes/cross_entropy/avg: 0.4954, train/total_loss: 0.3477, train/total_loss/avg: 0.4954, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 8, num_updates: 770, iterations: 770, max_updates: 2000, lr: 0.00003, ups: 0.71, time: 14s 061ms, time_since_start: 20m 39s 944ms, eta: 30m 50s 633ms\n",
      "\u001b[32m2024-07-23T09:16:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 780/2000, train/hateful_memes/cross_entropy: 0.3477, train/hateful_memes/cross_entropy/avg: 0.4919, train/total_loss: 0.3477, train/total_loss/avg: 0.4919, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 8, num_updates: 780, iterations: 780, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 809ms, time_since_start: 20m 52s 754ms, eta: 27m 52s 210ms\n",
      "\u001b[32m2024-07-23T09:16:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 790/2000, train/hateful_memes/cross_entropy: 0.3467, train/hateful_memes/cross_entropy/avg: 0.4893, train/total_loss: 0.3467, train/total_loss/avg: 0.4893, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 8, num_updates: 790, iterations: 790, max_updates: 2000, lr: 0.00003, ups: 0.71, time: 14s 007ms, time_since_start: 21m 06s 761ms, eta: 30m 13s 573ms\n",
      "\u001b[32m2024-07-23T09:17:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, train/hateful_memes/cross_entropy: 0.3467, train/hateful_memes/cross_entropy/avg: 0.4878, train/total_loss: 0.3467, train/total_loss/avg: 0.4878, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 8, num_updates: 800, iterations: 800, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 920ms, time_since_start: 21m 19s 682ms, eta: 27m 38s 949ms\n",
      "\u001b[32m2024-07-23T09:17:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:17:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:17:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:17:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:17:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:17:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:17:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:17:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, val/hateful_memes/cross_entropy: 0.7967, val/total_loss: 0.7967, val/hateful_memes/accuracy: 0.6100, val/hateful_memes/binary_f1: 0.5996, val/hateful_memes/roc_auc: 0.6746, num_updates: 800, epoch: 8, iterations: 800, max_updates: 2000, val_time: 23s 168ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.687728\n",
      "\u001b[32m2024-07-23T09:17:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 810/2000, train/hateful_memes/cross_entropy: 0.3428, train/hateful_memes/cross_entropy/avg: 0.4851, train/total_loss: 0.3428, train/total_loss/avg: 0.4851, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 8, num_updates: 810, iterations: 810, max_updates: 2000, lr: 0.00003, ups: 0.71, time: 14s 480ms, time_since_start: 21m 57s 338ms, eta: 30m 43s 837ms\n",
      "\u001b[32m2024-07-23T09:17:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 820/2000, train/hateful_memes/cross_entropy: 0.3229, train/hateful_memes/cross_entropy/avg: 0.4825, train/total_loss: 0.3229, train/total_loss/avg: 0.4825, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 8, num_updates: 820, iterations: 820, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 946ms, time_since_start: 22m 10s 285ms, eta: 27m 14s 654ms\n",
      "\u001b[32m2024-07-23T09:18:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 830/2000, train/hateful_memes/cross_entropy: 0.3428, train/hateful_memes/cross_entropy/avg: 0.4810, train/total_loss: 0.3428, train/total_loss/avg: 0.4810, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 8, num_updates: 830, iterations: 830, max_updates: 2000, lr: 0.00003, ups: 0.71, time: 14s 190ms, time_since_start: 22m 24s 476ms, eta: 29m 36s 560ms\n",
      "\u001b[32m2024-07-23T09:18:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 840/2000, train/hateful_memes/cross_entropy: 0.3229, train/hateful_memes/cross_entropy/avg: 0.4782, train/total_loss: 0.3229, train/total_loss/avg: 0.4782, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 8, num_updates: 840, iterations: 840, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 786ms, time_since_start: 22m 37s 262ms, eta: 26m 27s 020ms\n",
      "\u001b[32m2024-07-23T09:18:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/2000, train/hateful_memes/cross_entropy: 0.3225, train/hateful_memes/cross_entropy/avg: 0.4759, train/total_loss: 0.3225, train/total_loss/avg: 0.4759, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 8, num_updates: 850, iterations: 850, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 778ms, time_since_start: 22m 51s 040ms, eta: 28m 15s 405ms\n",
      "\u001b[32m2024-07-23T09:18:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 860/2000, train/hateful_memes/cross_entropy: 0.3108, train/hateful_memes/cross_entropy/avg: 0.4734, train/total_loss: 0.3108, train/total_loss/avg: 0.4734, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 9, num_updates: 860, iterations: 860, max_updates: 2000, lr: 0.00003, ups: 0.91, time: 11s 401ms, time_since_start: 23m 02s 442ms, eta: 23m 10s 715ms\n",
      "\u001b[32m2024-07-23T09:19:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 870/2000, train/hateful_memes/cross_entropy: 0.2953, train/hateful_memes/cross_entropy/avg: 0.4703, train/total_loss: 0.2953, train/total_loss/avg: 0.4703, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 9, num_updates: 870, iterations: 870, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 391ms, time_since_start: 23m 15s 833ms, eta: 26m 59s 141ms\n",
      "\u001b[32m2024-07-23T09:19:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 880/2000, train/hateful_memes/cross_entropy: 0.2953, train/hateful_memes/cross_entropy/avg: 0.4698, train/total_loss: 0.2953, train/total_loss/avg: 0.4698, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 9, num_updates: 880, iterations: 880, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 814ms, time_since_start: 23m 28s 648ms, eta: 25m 35s 743ms\n",
      "\u001b[32m2024-07-23T09:19:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 890/2000, train/hateful_memes/cross_entropy: 0.2953, train/hateful_memes/cross_entropy/avg: 0.4684, train/total_loss: 0.2953, train/total_loss/avg: 0.4684, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 9, num_updates: 890, iterations: 890, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 434ms, time_since_start: 23m 42s 082ms, eta: 26m 35s 586ms\n",
      "\u001b[32m2024-07-23T09:19:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, train/hateful_memes/cross_entropy: 0.2944, train/hateful_memes/cross_entropy/avg: 0.4657, train/total_loss: 0.2944, train/total_loss/avg: 0.4657, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 9, num_updates: 900, iterations: 900, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 025ms, time_since_start: 23m 55s 107ms, eta: 25m 33s 093ms\n",
      "\u001b[32m2024-07-23T09:19:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:19:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:19:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:19:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:19:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:19:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:20:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:20:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, val/hateful_memes/cross_entropy: 1.6894, val/total_loss: 1.6894, val/hateful_memes/accuracy: 0.5880, val/hateful_memes/binary_f1: 0.4246, val/hateful_memes/roc_auc: 0.6742, num_updates: 900, epoch: 9, iterations: 900, max_updates: 2000, val_time: 22s 679ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.687728\n",
      "\u001b[32m2024-07-23T09:20:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 910/2000, train/hateful_memes/cross_entropy: 0.2874, train/hateful_memes/cross_entropy/avg: 0.4627, train/total_loss: 0.2874, train/total_loss/avg: 0.4627, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 9, num_updates: 910, iterations: 910, max_updates: 2000, lr: 0.00003, ups: 0.71, time: 14s 050ms, time_since_start: 24m 31s 845ms, eta: 27m 18s 688ms\n",
      "\u001b[32m2024-07-23T09:20:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 920/2000, train/hateful_memes/cross_entropy: 0.2874, train/hateful_memes/cross_entropy/avg: 0.4622, train/total_loss: 0.2874, train/total_loss/avg: 0.4622, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 9, num_updates: 920, iterations: 920, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 189ms, time_since_start: 24m 45s 034ms, eta: 25m 24s 224ms\n",
      "\u001b[32m2024-07-23T09:20:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 930/2000, train/hateful_memes/cross_entropy: 0.2826, train/hateful_memes/cross_entropy/avg: 0.4601, train/total_loss: 0.2826, train/total_loss/avg: 0.4601, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 9, num_updates: 930, iterations: 930, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 515ms, time_since_start: 24m 58s 550ms, eta: 25m 47s 344ms\n",
      "\u001b[32m2024-07-23T09:20:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 940/2000, train/hateful_memes/cross_entropy: 0.2749, train/hateful_memes/cross_entropy/avg: 0.4577, train/total_loss: 0.2749, train/total_loss/avg: 0.4577, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 9, num_updates: 940, iterations: 940, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 979ms, time_since_start: 25m 11s 529ms, eta: 24m 32s 170ms\n",
      "\u001b[32m2024-07-23T09:21:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/2000, train/hateful_memes/cross_entropy: 0.2710, train/hateful_memes/cross_entropy/avg: 0.4553, train/total_loss: 0.2710, train/total_loss/avg: 0.4553, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 9, num_updates: 950, iterations: 950, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 623ms, time_since_start: 25m 25s 153ms, eta: 25m 30s 574ms\n",
      "\u001b[32m2024-07-23T09:21:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 960/2000, train/hateful_memes/cross_entropy: 0.2628, train/hateful_memes/cross_entropy/avg: 0.4521, train/total_loss: 0.2628, train/total_loss/avg: 0.4521, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 9, num_updates: 960, iterations: 960, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 719ms, time_since_start: 25m 37s 872ms, eta: 23m 35s 388ms\n",
      "\u001b[32m2024-07-23T09:21:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 970/2000, train/hateful_memes/cross_entropy: 0.2591, train/hateful_memes/cross_entropy/avg: 0.4489, train/total_loss: 0.2591, train/total_loss/avg: 0.4489, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 10, num_updates: 970, iterations: 970, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 535ms, time_since_start: 25m 50s 407ms, eta: 23m 01s 496ms\n",
      "\u001b[32m2024-07-23T09:21:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 980/2000, train/hateful_memes/cross_entropy: 0.2591, train/hateful_memes/cross_entropy/avg: 0.4462, train/total_loss: 0.2591, train/total_loss/avg: 0.4462, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 10, num_updates: 980, iterations: 980, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 657ms, time_since_start: 26m 04s 065ms, eta: 24m 50s 599ms\n",
      "\u001b[32m2024-07-23T09:22:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 990/2000, train/hateful_memes/cross_entropy: 0.2456, train/hateful_memes/cross_entropy/avg: 0.4436, train/total_loss: 0.2456, train/total_loss/avg: 0.4436, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 10, num_updates: 990, iterations: 990, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 576ms, time_since_start: 26m 16s 641ms, eta: 22m 39s 096ms\n",
      "\u001b[32m2024-07-23T09:22:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-23T09:22:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:22:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:22:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:22:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, train/hateful_memes/cross_entropy: 0.2385, train/hateful_memes/cross_entropy/avg: 0.4415, train/total_loss: 0.2385, train/total_loss/avg: 0.4415, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 10, num_updates: 1000, iterations: 1000, max_updates: 2000, lr: 0.00003, ups: 0.37, time: 27s 444ms, time_since_start: 26m 44s 085ms, eta: 48m 56s 539ms\n",
      "\u001b[32m2024-07-23T09:22:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:22:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:22:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:22:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:22:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:22:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:22:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:22:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, val/hateful_memes/cross_entropy: 1.4206, val/total_loss: 1.4206, val/hateful_memes/accuracy: 0.6240, val/hateful_memes/binary_f1: 0.5524, val/hateful_memes/roc_auc: 0.6686, num_updates: 1000, epoch: 10, iterations: 1000, max_updates: 2000, val_time: 25s 195ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.687728\n",
      "\u001b[32m2024-07-23T09:23:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1010/2000, train/hateful_memes/cross_entropy: 0.2385, train/hateful_memes/cross_entropy/avg: 0.4396, train/total_loss: 0.2385, train/total_loss/avg: 0.4396, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 10, num_updates: 1010, iterations: 1010, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 726ms, time_since_start: 27m 22s 009ms, eta: 22m 28s 170ms\n",
      "\u001b[32m2024-07-23T09:23:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1020/2000, train/hateful_memes/cross_entropy: 0.2385, train/hateful_memes/cross_entropy/avg: 0.4382, train/total_loss: 0.2385, train/total_loss/avg: 0.4382, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 10, num_updates: 1020, iterations: 1020, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 363ms, time_since_start: 27m 35s 373ms, eta: 23m 21s 334ms\n",
      "\u001b[32m2024-07-23T09:23:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1030/2000, train/hateful_memes/cross_entropy: 0.2348, train/hateful_memes/cross_entropy/avg: 0.4361, train/total_loss: 0.2348, train/total_loss/avg: 0.4361, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 10, num_updates: 1030, iterations: 1030, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 428ms, time_since_start: 27m 47s 801ms, eta: 21m 29s 960ms\n",
      "\u001b[32m2024-07-23T09:23:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1040/2000, train/hateful_memes/cross_entropy: 0.2315, train/hateful_memes/cross_entropy/avg: 0.4340, train/total_loss: 0.2315, train/total_loss/avg: 0.4340, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 10, num_updates: 1040, iterations: 1040, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 552ms, time_since_start: 28m 01s 354ms, eta: 23m 12s 106ms\n",
      "\u001b[32m2024-07-23T09:23:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/2000, train/hateful_memes/cross_entropy: 0.2272, train/hateful_memes/cross_entropy/avg: 0.4315, train/total_loss: 0.2272, train/total_loss/avg: 0.4315, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 10, num_updates: 1050, iterations: 1050, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 448ms, time_since_start: 28m 13s 803ms, eta: 21m 05s 429ms\n",
      "\u001b[32m2024-07-23T09:24:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1060/2000, train/hateful_memes/cross_entropy: 0.2272, train/hateful_memes/cross_entropy/avg: 0.4297, train/total_loss: 0.2272, train/total_loss/avg: 0.4297, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 10, num_updates: 1060, iterations: 1060, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 401ms, time_since_start: 28m 27s 204ms, eta: 22m 27s 891ms\n",
      "\u001b[32m2024-07-23T09:24:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1070/2000, train/hateful_memes/cross_entropy: 0.2272, train/hateful_memes/cross_entropy/avg: 0.4291, train/total_loss: 0.2272, train/total_loss/avg: 0.4291, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 10, num_updates: 1070, iterations: 1070, max_updates: 2000, lr: 0.00003, ups: 1.00, time: 10s 795ms, time_since_start: 28m 38s 000ms, eta: 17m 54s 295ms\n",
      "\u001b[32m2024-07-23T09:24:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1080/2000, train/hateful_memes/cross_entropy: 0.2272, train/hateful_memes/cross_entropy/avg: 0.4272, train/total_loss: 0.2272, train/total_loss/avg: 0.4272, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 11, num_updates: 1080, iterations: 1080, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 645ms, time_since_start: 28m 51s 645ms, eta: 22m 23s 262ms\n",
      "\u001b[32m2024-07-23T09:24:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1090/2000, train/hateful_memes/cross_entropy: 0.2260, train/hateful_memes/cross_entropy/avg: 0.4254, train/total_loss: 0.2260, train/total_loss/avg: 0.4254, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 11, num_updates: 1090, iterations: 1090, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 888ms, time_since_start: 29m 04s 534ms, eta: 20m 54s 959ms\n",
      "\u001b[32m2024-07-23T09:25:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, train/hateful_memes/cross_entropy: 0.2248, train/hateful_memes/cross_entropy/avg: 0.4224, train/total_loss: 0.2248, train/total_loss/avg: 0.4224, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 11, num_updates: 1100, iterations: 1100, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 642ms, time_since_start: 29m 18s 177ms, eta: 21m 53s 780ms\n",
      "\u001b[32m2024-07-23T09:25:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:25:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:25:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:25:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:25:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:25:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:25:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:25:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, val/hateful_memes/cross_entropy: 1.7199, val/total_loss: 1.7199, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.5052, val/hateful_memes/roc_auc: 0.6806, num_updates: 1100, epoch: 11, iterations: 1100, max_updates: 2000, val_time: 22s 355ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.687728\n",
      "\u001b[32m2024-07-23T09:25:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1110/2000, train/hateful_memes/cross_entropy: 0.2248, train/hateful_memes/cross_entropy/avg: 0.4201, train/total_loss: 0.2248, train/total_loss/avg: 0.4201, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 11, num_updates: 1110, iterations: 1110, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 173ms, time_since_start: 29m 53s 713ms, eta: 20m 54s 498ms\n",
      "\u001b[32m2024-07-23T09:25:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1120/2000, train/hateful_memes/cross_entropy: 0.2177, train/hateful_memes/cross_entropy/avg: 0.4182, train/total_loss: 0.2177, train/total_loss/avg: 0.4182, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 11, num_updates: 1120, iterations: 1120, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 459ms, time_since_start: 30m 07s 172ms, eta: 21m 07s 344ms\n",
      "\u001b[32m2024-07-23T09:26:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1130/2000, train/hateful_memes/cross_entropy: 0.2104, train/hateful_memes/cross_entropy/avg: 0.4159, train/total_loss: 0.2104, train/total_loss/avg: 0.4159, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 11, num_updates: 1130, iterations: 1130, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 003ms, time_since_start: 30m 20s 176ms, eta: 20m 10s 506ms\n",
      "\u001b[32m2024-07-23T09:26:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1140/2000, train/hateful_memes/cross_entropy: 0.2104, train/hateful_memes/cross_entropy/avg: 0.4150, train/total_loss: 0.2104, train/total_loss/avg: 0.4150, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 11, num_updates: 1140, iterations: 1140, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 608ms, time_since_start: 30m 33s 784ms, eta: 20m 52s 243ms\n",
      "\u001b[32m2024-07-23T09:26:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/2000, train/hateful_memes/cross_entropy: 0.1880, train/hateful_memes/cross_entropy/avg: 0.4128, train/total_loss: 0.1880, train/total_loss/avg: 0.4128, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 11, num_updates: 1150, iterations: 1150, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 980ms, time_since_start: 30m 46s 764ms, eta: 19m 40s 532ms\n",
      "\u001b[32m2024-07-23T09:26:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1160/2000, train/hateful_memes/cross_entropy: 0.2090, train/hateful_memes/cross_entropy/avg: 0.4110, train/total_loss: 0.2090, train/total_loss/avg: 0.4110, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 11, num_updates: 1160, iterations: 1160, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 525ms, time_since_start: 31m 290ms, eta: 20m 15s 662ms\n",
      "\u001b[32m2024-07-23T09:26:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1170/2000, train/hateful_memes/cross_entropy: 0.2090, train/hateful_memes/cross_entropy/avg: 0.4088, train/total_loss: 0.2090, train/total_loss/avg: 0.4088, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 11, num_updates: 1170, iterations: 1170, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 919ms, time_since_start: 31m 13s 210ms, eta: 19m 07s 420ms\n",
      "\u001b[32m2024-07-23T09:27:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1180/2000, train/hateful_memes/cross_entropy: 0.2090, train/hateful_memes/cross_entropy/avg: 0.4063, train/total_loss: 0.2090, train/total_loss/avg: 0.4063, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 12, num_updates: 1180, iterations: 1180, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 385ms, time_since_start: 31m 25s 595ms, eta: 18m 06s 737ms\n",
      "\u001b[32m2024-07-23T09:27:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1190/2000, train/hateful_memes/cross_entropy: 0.2090, train/hateful_memes/cross_entropy/avg: 0.4038, train/total_loss: 0.2090, train/total_loss/avg: 0.4038, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 12, num_updates: 1190, iterations: 1190, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 769ms, time_since_start: 31m 38s 365ms, eta: 18m 26s 720ms\n",
      "\u001b[32m2024-07-23T09:27:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, train/hateful_memes/cross_entropy: 0.1815, train/hateful_memes/cross_entropy/avg: 0.4018, train/total_loss: 0.1815, train/total_loss/avg: 0.4018, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 12, num_updates: 1200, iterations: 1200, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 390ms, time_since_start: 31m 51s 755ms, eta: 19m 06s 203ms\n",
      "\u001b[32m2024-07-23T09:27:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:27:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:27:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:27:45 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:27:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:27:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:27:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:27:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, val/hateful_memes/cross_entropy: 2.4361, val/total_loss: 2.4361, val/hateful_memes/accuracy: 0.5880, val/hateful_memes/binary_f1: 0.4402, val/hateful_memes/roc_auc: 0.6787, num_updates: 1200, epoch: 12, iterations: 1200, max_updates: 2000, val_time: 21s 373ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.687728\n",
      "\u001b[32m2024-07-23T09:28:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1210/2000, train/hateful_memes/cross_entropy: 0.1714, train/hateful_memes/cross_entropy/avg: 0.3991, train/total_loss: 0.1714, train/total_loss/avg: 0.3991, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 12, num_updates: 1210, iterations: 1210, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 140ms, time_since_start: 32m 26s 276ms, eta: 18m 30s 747ms\n",
      "\u001b[32m2024-07-23T09:28:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1220/2000, train/hateful_memes/cross_entropy: 0.1658, train/hateful_memes/cross_entropy/avg: 0.3970, train/total_loss: 0.1658, train/total_loss/avg: 0.3970, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 12, num_updates: 1220, iterations: 1220, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 728ms, time_since_start: 32m 40s 004ms, eta: 19m 05s 744ms\n",
      "\u001b[32m2024-07-23T09:28:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1230/2000, train/hateful_memes/cross_entropy: 0.1637, train/hateful_memes/cross_entropy/avg: 0.3943, train/total_loss: 0.1637, train/total_loss/avg: 0.3943, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 12, num_updates: 1230, iterations: 1230, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 925ms, time_since_start: 32m 52s 930ms, eta: 17m 44s 969ms\n",
      "\u001b[32m2024-07-23T09:28:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1240/2000, train/hateful_memes/cross_entropy: 0.1585, train/hateful_memes/cross_entropy/avg: 0.3917, train/total_loss: 0.1585, train/total_loss/avg: 0.3917, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 12, num_updates: 1240, iterations: 1240, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 360ms, time_since_start: 33m 06s 291ms, eta: 18m 06s 499ms\n",
      "\u001b[32m2024-07-23T09:29:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/2000, train/hateful_memes/cross_entropy: 0.1585, train/hateful_memes/cross_entropy/avg: 0.3902, train/total_loss: 0.1585, train/total_loss/avg: 0.3902, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 12, num_updates: 1250, iterations: 1250, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 814ms, time_since_start: 33m 19s 105ms, eta: 17m 08s 351ms\n",
      "\u001b[32m2024-07-23T09:29:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1260/2000, train/hateful_memes/cross_entropy: 0.1564, train/hateful_memes/cross_entropy/avg: 0.3883, train/total_loss: 0.1564, train/total_loss/avg: 0.3883, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 12, num_updates: 1260, iterations: 1260, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 336ms, time_since_start: 33m 32s 442ms, eta: 17m 35s 985ms\n",
      "\u001b[32m2024-07-23T09:29:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1270/2000, train/hateful_memes/cross_entropy: 0.1550, train/hateful_memes/cross_entropy/avg: 0.3863, train/total_loss: 0.1550, train/total_loss/avg: 0.3863, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 12, num_updates: 1270, iterations: 1270, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 793ms, time_since_start: 33m 45s 235ms, eta: 16m 39s 281ms\n",
      "\u001b[32m2024-07-23T09:29:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1280/2000, train/hateful_memes/cross_entropy: 0.1534, train/hateful_memes/cross_entropy/avg: 0.3841, train/total_loss: 0.1534, train/total_loss/avg: 0.3841, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 12, num_updates: 1280, iterations: 1280, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 184ms, time_since_start: 33m 58s 419ms, eta: 16m 55s 710ms\n",
      "\u001b[32m2024-07-23T09:29:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1290/2000, train/hateful_memes/cross_entropy: 0.1408, train/hateful_memes/cross_entropy/avg: 0.3818, train/total_loss: 0.1408, train/total_loss/avg: 0.3818, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 13, num_updates: 1290, iterations: 1290, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 565ms, time_since_start: 34m 10s 985ms, eta: 15m 54s 577ms\n",
      "\u001b[32m2024-07-23T09:30:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, train/hateful_memes/cross_entropy: 0.1408, train/hateful_memes/cross_entropy/avg: 0.3792, train/total_loss: 0.1408, train/total_loss/avg: 0.3792, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 13, num_updates: 1300, iterations: 1300, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 666ms, time_since_start: 34m 23s 652ms, eta: 15m 48s 747ms\n",
      "\u001b[32m2024-07-23T09:30:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:30:09 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:30:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:30:16 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:30:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:30:20 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T09:30:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:30:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:30:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, val/hateful_memes/cross_entropy: 2.2143, val/total_loss: 2.2143, val/hateful_memes/accuracy: 0.5900, val/hateful_memes/binary_f1: 0.4474, val/hateful_memes/roc_auc: 0.6882, num_updates: 1300, epoch: 13, iterations: 1300, max_updates: 2000, val_time: 31s 192ms, best_update: 1300, best_iteration: 1300, best_val/hateful_memes/roc_auc: 0.688156\n",
      "\u001b[32m2024-07-23T09:30:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1310/2000, train/hateful_memes/cross_entropy: 0.1325, train/hateful_memes/cross_entropy/avg: 0.3769, train/total_loss: 0.1325, train/total_loss/avg: 0.3769, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 13, num_updates: 1310, iterations: 1310, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 691ms, time_since_start: 35m 08s 544ms, eta: 16m 50s 878ms\n",
      "\u001b[32m2024-07-23T09:31:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1320/2000, train/hateful_memes/cross_entropy: 0.1156, train/hateful_memes/cross_entropy/avg: 0.3744, train/total_loss: 0.1156, train/total_loss/avg: 0.3744, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 13, num_updates: 1320, iterations: 1320, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 997ms, time_since_start: 35m 21s 541ms, eta: 15m 45s 689ms\n",
      "\u001b[32m2024-07-23T09:31:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1330/2000, train/hateful_memes/cross_entropy: 0.1122, train/hateful_memes/cross_entropy/avg: 0.3721, train/total_loss: 0.1122, train/total_loss/avg: 0.3721, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 13, num_updates: 1330, iterations: 1330, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 973ms, time_since_start: 35m 35s 515ms, eta: 16m 41s 774ms\n",
      "\u001b[32m2024-07-23T09:31:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1340/2000, train/hateful_memes/cross_entropy: 0.1113, train/hateful_memes/cross_entropy/avg: 0.3697, train/total_loss: 0.1113, train/total_loss/avg: 0.3697, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 13, num_updates: 1340, iterations: 1340, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 676ms, time_since_start: 35m 48s 191ms, eta: 14m 55s 196ms\n",
      "\u001b[32m2024-07-23T09:31:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/2000, train/hateful_memes/cross_entropy: 0.0902, train/hateful_memes/cross_entropy/avg: 0.3676, train/total_loss: 0.0902, train/total_loss/avg: 0.3676, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 13, num_updates: 1350, iterations: 1350, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 816ms, time_since_start: 36m 02s 008ms, eta: 16m 949ms\n",
      "\u001b[32m2024-07-23T09:32:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1360/2000, train/hateful_memes/cross_entropy: 0.0856, train/hateful_memes/cross_entropy/avg: 0.3653, train/total_loss: 0.0856, train/total_loss/avg: 0.3653, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 13, num_updates: 1360, iterations: 1360, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 659ms, time_since_start: 36m 14s 667ms, eta: 14m 26s 928ms\n",
      "\u001b[32m2024-07-23T09:32:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1370/2000, train/hateful_memes/cross_entropy: 0.0856, train/hateful_memes/cross_entropy/avg: 0.3634, train/total_loss: 0.0856, train/total_loss/avg: 0.3634, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 13, num_updates: 1370, iterations: 1370, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 869ms, time_since_start: 36m 28s 537ms, eta: 15m 34s 970ms\n",
      "\u001b[32m2024-07-23T09:32:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1380/2000, train/hateful_memes/cross_entropy: 0.0856, train/hateful_memes/cross_entropy/avg: 0.3615, train/total_loss: 0.0856, train/total_loss/avg: 0.3615, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 13, num_updates: 1380, iterations: 1380, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 846ms, time_since_start: 36m 41s 384ms, eta: 14m 12s 260ms\n",
      "\u001b[32m2024-07-23T09:32:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1390/2000, train/hateful_memes/cross_entropy: 0.0819, train/hateful_memes/cross_entropy/avg: 0.3593, train/total_loss: 0.0819, train/total_loss/avg: 0.3593, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 13, num_updates: 1390, iterations: 1390, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 108ms, time_since_start: 36m 54s 493ms, eta: 14m 15s 585ms\n",
      "\u001b[32m2024-07-23T09:32:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, train/hateful_memes/cross_entropy: 0.0728, train/hateful_memes/cross_entropy/avg: 0.3572, train/total_loss: 0.0728, train/total_loss/avg: 0.3572, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 14, num_updates: 1400, iterations: 1400, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 627ms, time_since_start: 37m 07s 120ms, eta: 13m 30s 683ms\n",
      "\u001b[32m2024-07-23T09:32:52 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:32:52 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:33:01 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:33:01 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:33:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:33:05 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T09:33:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:33:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:33:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, val/hateful_memes/cross_entropy: 2.1562, val/total_loss: 2.1562, val/hateful_memes/accuracy: 0.6060, val/hateful_memes/binary_f1: 0.4632, val/hateful_memes/roc_auc: 0.7008, num_updates: 1400, epoch: 14, iterations: 1400, max_updates: 2000, val_time: 34s 739ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.700832\n",
      "\u001b[32m2024-07-23T09:33:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1410/2000, train/hateful_memes/cross_entropy: 0.0720, train/hateful_memes/cross_entropy/avg: 0.3551, train/total_loss: 0.0720, train/total_loss/avg: 0.3551, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 14, num_updates: 1410, iterations: 1410, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 812ms, time_since_start: 37m 55s 679ms, eta: 14m 31s 984ms\n",
      "\u001b[32m2024-07-23T09:33:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1420/2000, train/hateful_memes/cross_entropy: 0.0720, train/hateful_memes/cross_entropy/avg: 0.3532, train/total_loss: 0.0720, train/total_loss/avg: 0.3532, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 14, num_updates: 1420, iterations: 1420, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 210ms, time_since_start: 38m 08s 890ms, eta: 13m 39s 829ms\n",
      "\u001b[32m2024-07-23T09:34:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1430/2000, train/hateful_memes/cross_entropy: 0.0720, train/hateful_memes/cross_entropy/avg: 0.3510, train/total_loss: 0.0720, train/total_loss/avg: 0.3510, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 14, num_updates: 1430, iterations: 1430, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 085ms, time_since_start: 38m 21s 976ms, eta: 13m 18s 107ms\n",
      "\u001b[32m2024-07-23T09:34:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1440/2000, train/hateful_memes/cross_entropy: 0.0720, train/hateful_memes/cross_entropy/avg: 0.3492, train/total_loss: 0.0720, train/total_loss/avg: 0.3492, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 14, num_updates: 1440, iterations: 1440, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 083ms, time_since_start: 38m 35s 059ms, eta: 13m 03s 959ms\n",
      "\u001b[32m2024-07-23T09:34:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/2000, train/hateful_memes/cross_entropy: 0.0720, train/hateful_memes/cross_entropy/avg: 0.3474, train/total_loss: 0.0720, train/total_loss/avg: 0.3474, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 14, num_updates: 1450, iterations: 1450, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 163ms, time_since_start: 38m 48s 223ms, eta: 12m 54s 700ms\n",
      "\u001b[32m2024-07-23T09:34:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1460/2000, train/hateful_memes/cross_entropy: 0.0720, train/hateful_memes/cross_entropy/avg: 0.3457, train/total_loss: 0.0720, train/total_loss/avg: 0.3457, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 14, num_updates: 1460, iterations: 1460, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 157ms, time_since_start: 39m 01s 380ms, eta: 12m 40s 214ms\n",
      "\u001b[32m2024-07-23T09:35:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1470/2000, train/hateful_memes/cross_entropy: 0.0680, train/hateful_memes/cross_entropy/avg: 0.3438, train/total_loss: 0.0680, train/total_loss/avg: 0.3438, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 14, num_updates: 1470, iterations: 1470, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 196ms, time_since_start: 39m 14s 577ms, eta: 12m 28s 368ms\n",
      "\u001b[32m2024-07-23T09:35:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1480/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3416, train/total_loss: 0.0640, train/total_loss/avg: 0.3416, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 14, num_updates: 1480, iterations: 1480, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 240ms, time_since_start: 39m 27s 817ms, eta: 12m 16s 702ms\n",
      "\u001b[32m2024-07-23T09:35:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1490/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3404, train/total_loss: 0.0640, train/total_loss/avg: 0.3404, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 14, num_updates: 1490, iterations: 1490, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 244ms, time_since_start: 39m 41s 062ms, eta: 12m 02s 754ms\n",
      "\u001b[32m2024-07-23T09:35:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3383, train/total_loss: 0.0640, train/total_loss/avg: 0.3383, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 15, num_updates: 1500, iterations: 1500, max_updates: 2000, lr: 0.00001, ups: 0.91, time: 11s 839ms, time_since_start: 39m 52s 901ms, eta: 10m 33s 402ms\n",
      "\u001b[32m2024-07-23T09:35:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:35:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:35:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:35:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:35:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:36:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:36:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:36:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, val/hateful_memes/cross_entropy: 2.0785, val/total_loss: 2.0785, val/hateful_memes/accuracy: 0.6220, val/hateful_memes/binary_f1: 0.5356, val/hateful_memes/roc_auc: 0.6968, num_updates: 1500, epoch: 15, iterations: 1500, max_updates: 2000, val_time: 42s 304ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.700832\n",
      "\u001b[32m2024-07-23T09:36:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1510/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3366, train/total_loss: 0.0640, train/total_loss/avg: 0.3366, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 15, num_updates: 1510, iterations: 1510, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 161ms, time_since_start: 40m 48s 374ms, eta: 11m 30s 058ms\n",
      "\u001b[32m2024-07-23T09:36:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1520/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3346, train/total_loss: 0.0640, train/total_loss/avg: 0.3346, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 15, num_updates: 1520, iterations: 1520, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 264ms, time_since_start: 41m 01s 639ms, eta: 11m 21s 262ms\n",
      "\u001b[32m2024-07-23T09:36:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1530/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3332, train/total_loss: 0.0640, train/total_loss/avg: 0.3332, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 15, num_updates: 1530, iterations: 1530, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 607ms, time_since_start: 41m 14s 246ms, eta: 10m 34s 030ms\n",
      "\u001b[32m2024-07-23T09:37:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1540/2000, train/hateful_memes/cross_entropy: 0.0687, train/hateful_memes/cross_entropy/avg: 0.3315, train/total_loss: 0.0687, train/total_loss/avg: 0.3315, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 15, num_updates: 1540, iterations: 1540, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 159ms, time_since_start: 41m 27s 405ms, eta: 10m 47s 702ms\n",
      "\u001b[32m2024-07-23T09:37:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3297, train/total_loss: 0.0640, train/total_loss/avg: 0.3297, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 15, num_updates: 1550, iterations: 1550, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 660ms, time_since_start: 41m 40s 066ms, eta: 10m 09s 611ms\n",
      "\u001b[32m2024-07-23T09:37:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1560/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3279, train/total_loss: 0.0640, train/total_loss/avg: 0.3279, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 15, num_updates: 1560, iterations: 1560, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 183ms, time_since_start: 41m 53s 250ms, eta: 10m 20s 702ms\n",
      "\u001b[32m2024-07-23T09:37:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1570/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3265, train/total_loss: 0.0640, train/total_loss/avg: 0.3265, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 15, num_updates: 1570, iterations: 1570, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 633ms, time_since_start: 42m 05s 883ms, eta: 09m 41s 251ms\n",
      "\u001b[32m2024-07-23T09:38:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1580/2000, train/hateful_memes/cross_entropy: 0.0600, train/hateful_memes/cross_entropy/avg: 0.3246, train/total_loss: 0.0600, train/total_loss/avg: 0.3246, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 15, num_updates: 1580, iterations: 1580, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 326ms, time_since_start: 42m 19s 210ms, eta: 09m 58s 885ms\n",
      "\u001b[32m2024-07-23T09:38:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1590/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3230, train/total_loss: 0.0640, train/total_loss/avg: 0.3230, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 15, num_updates: 1590, iterations: 1590, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 774ms, time_since_start: 42m 31s 984ms, eta: 09m 20s 433ms\n",
      "\u001b[32m2024-07-23T09:38:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, train/hateful_memes/cross_entropy: 0.0661, train/hateful_memes/cross_entropy/avg: 0.3216, train/total_loss: 0.0661, train/total_loss/avg: 0.3216, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 15, num_updates: 1600, iterations: 1600, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 264ms, time_since_start: 42m 45s 249ms, eta: 09m 27s 737ms\n",
      "\u001b[32m2024-07-23T09:38:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:38:30 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:38:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:38:38 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:38:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:38:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:38:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:38:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, val/hateful_memes/cross_entropy: 2.0533, val/total_loss: 2.0533, val/hateful_memes/accuracy: 0.6180, val/hateful_memes/binary_f1: 0.5115, val/hateful_memes/roc_auc: 0.6855, num_updates: 1600, epoch: 15, iterations: 1600, max_updates: 2000, val_time: 22s 456ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.700832\n",
      "\u001b[32m2024-07-23T09:39:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1610/2000, train/hateful_memes/cross_entropy: 0.0661, train/hateful_memes/cross_entropy/avg: 0.3200, train/total_loss: 0.0661, train/total_loss/avg: 0.3200, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 16, num_updates: 1610, iterations: 1610, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 925ms, time_since_start: 43m 20s 639ms, eta: 08m 59s 376ms\n",
      "\u001b[32m2024-07-23T09:39:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1620/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3181, train/total_loss: 0.0640, train/total_loss/avg: 0.3181, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 16, num_updates: 1620, iterations: 1620, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 572ms, time_since_start: 43m 33s 212ms, eta: 08m 31s 204ms\n",
      "\u001b[32m2024-07-23T09:39:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1630/2000, train/hateful_memes/cross_entropy: 0.0640, train/hateful_memes/cross_entropy/avg: 0.3165, train/total_loss: 0.0640, train/total_loss/avg: 0.3165, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 16, num_updates: 1630, iterations: 1630, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 559ms, time_since_start: 43m 46s 771ms, eta: 08m 56s 825ms\n",
      "\u001b[32m2024-07-23T09:39:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1640/2000, train/hateful_memes/cross_entropy: 0.0569, train/hateful_memes/cross_entropy/avg: 0.3148, train/total_loss: 0.0569, train/total_loss/avg: 0.3148, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 16, num_updates: 1640, iterations: 1640, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 491ms, time_since_start: 43m 59s 263ms, eta: 08m 01s 172ms\n",
      "\u001b[32m2024-07-23T09:39:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/2000, train/hateful_memes/cross_entropy: 0.0569, train/hateful_memes/cross_entropy/avg: 0.3135, train/total_loss: 0.0569, train/total_loss/avg: 0.3135, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 16, num_updates: 1650, iterations: 1650, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 727ms, time_since_start: 44m 12s 990ms, eta: 08m 34s 081ms\n",
      "\u001b[32m2024-07-23T09:40:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1660/2000, train/hateful_memes/cross_entropy: 0.0569, train/hateful_memes/cross_entropy/avg: 0.3122, train/total_loss: 0.0569, train/total_loss/avg: 0.3122, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 16, num_updates: 1660, iterations: 1660, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 728ms, time_since_start: 44m 25s 718ms, eta: 07m 43s 056ms\n",
      "\u001b[32m2024-07-23T09:40:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1670/2000, train/hateful_memes/cross_entropy: 0.0569, train/hateful_memes/cross_entropy/avg: 0.3107, train/total_loss: 0.0569, train/total_loss/avg: 0.3107, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 16, num_updates: 1670, iterations: 1670, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 847ms, time_since_start: 44m 39s 566ms, eta: 08m 08s 973ms\n",
      "\u001b[32m2024-07-23T09:40:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1680/2000, train/hateful_memes/cross_entropy: 0.0635, train/hateful_memes/cross_entropy/avg: 0.3095, train/total_loss: 0.0635, train/total_loss/avg: 0.3095, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 16, num_updates: 1680, iterations: 1680, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 521ms, time_since_start: 44m 52s 088ms, eta: 07m 08s 727ms\n",
      "\u001b[32m2024-07-23T09:40:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1690/2000, train/hateful_memes/cross_entropy: 0.0635, train/hateful_memes/cross_entropy/avg: 0.3082, train/total_loss: 0.0635, train/total_loss/avg: 0.3082, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 16, num_updates: 1690, iterations: 1690, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 695ms, time_since_start: 45m 05s 783ms, eta: 07m 34s 277ms\n",
      "\u001b[32m2024-07-23T09:41:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, train/hateful_memes/cross_entropy: 0.0661, train/hateful_memes/cross_entropy/avg: 0.3068, train/total_loss: 0.0661, train/total_loss/avg: 0.3068, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 16, num_updates: 1700, iterations: 1700, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 567ms, time_since_start: 45m 18s 350ms, eta: 06m 43s 401ms\n",
      "\u001b[32m2024-07-23T09:41:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:41:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:41:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:41:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:41:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:41:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:41:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:41:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, val/hateful_memes/cross_entropy: 2.5455, val/total_loss: 2.5455, val/hateful_memes/accuracy: 0.6040, val/hateful_memes/binary_f1: 0.4706, val/hateful_memes/roc_auc: 0.6817, num_updates: 1700, epoch: 16, iterations: 1700, max_updates: 2000, val_time: 22s 198ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.700832\n",
      "\u001b[32m2024-07-23T09:41:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1710/2000, train/hateful_memes/cross_entropy: 0.0661, train/hateful_memes/cross_entropy/avg: 0.3054, train/total_loss: 0.0661, train/total_loss/avg: 0.3054, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 16, num_updates: 1710, iterations: 1710, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 114ms, time_since_start: 45m 53s 671ms, eta: 06m 46s 946ms\n",
      "\u001b[32m2024-07-23T09:41:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1720/2000, train/hateful_memes/cross_entropy: 0.0661, train/hateful_memes/cross_entropy/avg: 0.3039, train/total_loss: 0.0661, train/total_loss/avg: 0.3039, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 17, num_updates: 1720, iterations: 1720, max_updates: 2000, lr: 0.00001, ups: 0.91, time: 11s 625ms, time_since_start: 46m 05s 296ms, eta: 05m 48s 291ms\n",
      "\u001b[32m2024-07-23T09:42:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1730/2000, train/hateful_memes/cross_entropy: 0.0635, train/hateful_memes/cross_entropy/avg: 0.3024, train/total_loss: 0.0635, train/total_loss/avg: 0.3024, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 17, num_updates: 1730, iterations: 1730, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 422ms, time_since_start: 46m 18s 718ms, eta: 06m 27s 775ms\n",
      "\u001b[32m2024-07-23T09:42:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1740/2000, train/hateful_memes/cross_entropy: 0.0569, train/hateful_memes/cross_entropy/avg: 0.3010, train/total_loss: 0.0569, train/total_loss/avg: 0.3010, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 17, num_updates: 1740, iterations: 1740, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 497ms, time_since_start: 46m 31s 216ms, eta: 05m 47s 687ms\n",
      "\u001b[32m2024-07-23T09:42:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/2000, train/hateful_memes/cross_entropy: 0.0635, train/hateful_memes/cross_entropy/avg: 0.2998, train/total_loss: 0.0635, train/total_loss/avg: 0.2998, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 17, num_updates: 1750, iterations: 1750, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 713ms, time_since_start: 46m 44s 929ms, eta: 06m 06s 832ms\n",
      "\u001b[32m2024-07-23T09:42:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1760/2000, train/hateful_memes/cross_entropy: 0.0635, train/hateful_memes/cross_entropy/avg: 0.2982, train/total_loss: 0.0635, train/total_loss/avg: 0.2982, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 17, num_updates: 1760, iterations: 1760, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 589ms, time_since_start: 46m 57s 519ms, eta: 05m 23s 287ms\n",
      "\u001b[32m2024-07-23T09:42:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1770/2000, train/hateful_memes/cross_entropy: 0.0635, train/hateful_memes/cross_entropy/avg: 0.2969, train/total_loss: 0.0635, train/total_loss/avg: 0.2969, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 17, num_updates: 1770, iterations: 1770, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 533ms, time_since_start: 47m 11s 052ms, eta: 05m 33s 057ms\n",
      "\u001b[32m2024-07-23T09:43:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1780/2000, train/hateful_memes/cross_entropy: 0.0635, train/hateful_memes/cross_entropy/avg: 0.2953, train/total_loss: 0.0635, train/total_loss/avg: 0.2953, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 17, num_updates: 1780, iterations: 1780, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 466ms, time_since_start: 47m 23s 519ms, eta: 04m 53s 465ms\n",
      "\u001b[32m2024-07-23T09:43:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1790/2000, train/hateful_memes/cross_entropy: 0.0600, train/hateful_memes/cross_entropy/avg: 0.2940, train/total_loss: 0.0600, train/total_loss/avg: 0.2940, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 17, num_updates: 1790, iterations: 1790, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 519ms, time_since_start: 47m 37s 038ms, eta: 05m 03s 786ms\n",
      "\u001b[32m2024-07-23T09:43:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, train/hateful_memes/cross_entropy: 0.0569, train/hateful_memes/cross_entropy/avg: 0.2924, train/total_loss: 0.0569, train/total_loss/avg: 0.2924, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 17, num_updates: 1800, iterations: 1800, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 453ms, time_since_start: 47m 49s 492ms, eta: 04m 26s 509ms\n",
      "\u001b[32m2024-07-23T09:43:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:43:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:43:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:43:43 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:43:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:43:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:43:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:43:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, val/hateful_memes/cross_entropy: 2.6043, val/total_loss: 2.6043, val/hateful_memes/accuracy: 0.6000, val/hateful_memes/binary_f1: 0.4845, val/hateful_memes/roc_auc: 0.6825, num_updates: 1800, epoch: 17, iterations: 1800, max_updates: 2000, val_time: 22s 904ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.700832\n",
      "\u001b[32m2024-07-23T09:44:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1810/2000, train/hateful_memes/cross_entropy: 0.0569, train/hateful_memes/cross_entropy/avg: 0.2909, train/total_loss: 0.0569, train/total_loss/avg: 0.2909, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 17, num_updates: 1810, iterations: 1810, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 907ms, time_since_start: 48m 26s 312ms, eta: 04m 42s 743ms\n",
      "\u001b[32m2024-07-23T09:44:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1820/2000, train/hateful_memes/cross_entropy: 0.0569, train/hateful_memes/cross_entropy/avg: 0.2895, train/total_loss: 0.0569, train/total_loss/avg: 0.2895, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 18, num_updates: 1820, iterations: 1820, max_updates: 2000, lr: 0.00001, ups: 0.91, time: 11s 351ms, time_since_start: 48m 37s 663ms, eta: 03m 38s 623ms\n",
      "\u001b[32m2024-07-23T09:44:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1830/2000, train/hateful_memes/cross_entropy: 0.0547, train/hateful_memes/cross_entropy/avg: 0.2882, train/total_loss: 0.0547, train/total_loss/avg: 0.2882, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 18, num_updates: 1830, iterations: 1830, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 304ms, time_since_start: 48m 50s 968ms, eta: 04m 02s 014ms\n",
      "\u001b[32m2024-07-23T09:44:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1840/2000, train/hateful_memes/cross_entropy: 0.0547, train/hateful_memes/cross_entropy/avg: 0.2868, train/total_loss: 0.0547, train/total_loss/avg: 0.2868, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 18, num_updates: 1840, iterations: 1840, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 818ms, time_since_start: 49m 03s 786ms, eta: 03m 39s 452ms\n",
      "\u001b[32m2024-07-23T09:45:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/2000, train/hateful_memes/cross_entropy: 0.0542, train/hateful_memes/cross_entropy/avg: 0.2853, train/total_loss: 0.0542, train/total_loss/avg: 0.2853, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 18, num_updates: 1850, iterations: 1850, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 272ms, time_since_start: 49m 17s 059ms, eta: 03m 33s 029ms\n",
      "\u001b[32m2024-07-23T09:45:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1860/2000, train/hateful_memes/cross_entropy: 0.0515, train/hateful_memes/cross_entropy/avg: 0.2840, train/total_loss: 0.0515, train/total_loss/avg: 0.2840, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 18, num_updates: 1860, iterations: 1860, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 662ms, time_since_start: 49m 29s 722ms, eta: 03m 09s 686ms\n",
      "\u001b[32m2024-07-23T09:45:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1870/2000, train/hateful_memes/cross_entropy: 0.0515, train/hateful_memes/cross_entropy/avg: 0.2828, train/total_loss: 0.0515, train/total_loss/avg: 0.2828, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 18, num_updates: 1870, iterations: 1870, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 309ms, time_since_start: 49m 43s 031ms, eta: 03m 05s 128ms\n",
      "\u001b[32m2024-07-23T09:45:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1880/2000, train/hateful_memes/cross_entropy: 0.0515, train/hateful_memes/cross_entropy/avg: 0.2817, train/total_loss: 0.0515, train/total_loss/avg: 0.2817, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 18, num_updates: 1880, iterations: 1880, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 684ms, time_since_start: 49m 55s 716ms, eta: 02m 42s 875ms\n",
      "\u001b[32m2024-07-23T09:45:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1890/2000, train/hateful_memes/cross_entropy: 0.0515, train/hateful_memes/cross_entropy/avg: 0.2806, train/total_loss: 0.0515, train/total_loss/avg: 0.2806, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 18, num_updates: 1890, iterations: 1890, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 382ms, time_since_start: 50m 09s 098ms, eta: 02m 37s 508ms\n",
      "\u001b[32m2024-07-23T09:46:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, train/hateful_memes/cross_entropy: 0.0401, train/hateful_memes/cross_entropy/avg: 0.2792, train/total_loss: 0.0401, train/total_loss/avg: 0.2792, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 18, num_updates: 1900, iterations: 1900, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 888ms, time_since_start: 50m 21s 986ms, eta: 02m 17s 904ms\n",
      "\u001b[32m2024-07-23T09:46:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:46:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:46:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:46:16 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:46:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:46:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:46:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:46:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, val/hateful_memes/cross_entropy: 2.8605, val/total_loss: 2.8605, val/hateful_memes/accuracy: 0.6100, val/hateful_memes/binary_f1: 0.4909, val/hateful_memes/roc_auc: 0.6767, num_updates: 1900, epoch: 18, iterations: 1900, max_updates: 2000, val_time: 23s 239ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.700832\n",
      "\u001b[32m2024-07-23T09:46:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1910/2000, train/hateful_memes/cross_entropy: 0.0361, train/hateful_memes/cross_entropy/avg: 0.2779, train/total_loss: 0.0361, train/total_loss/avg: 0.2779, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 18, num_updates: 1910, iterations: 1910, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 554ms, time_since_start: 50m 58s 788ms, eta: 02m 10s 528ms\n",
      "\u001b[32m2024-07-23T09:46:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1920/2000, train/hateful_memes/cross_entropy: 0.0401, train/hateful_memes/cross_entropy/avg: 0.2766, train/total_loss: 0.0401, train/total_loss/avg: 0.2766, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 18, num_updates: 1920, iterations: 1920, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 834ms, time_since_start: 51m 11s 623ms, eta: 01m 49s 865ms\n",
      "\u001b[32m2024-07-23T09:47:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1930/2000, train/hateful_memes/cross_entropy: 0.0324, train/hateful_memes/cross_entropy/avg: 0.2754, train/total_loss: 0.0324, train/total_loss/avg: 0.2754, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 19, num_updates: 1930, iterations: 1930, max_updates: 2000, lr: 0., ups: 0.91, time: 11s 800ms, time_since_start: 51m 23s 424ms, eta: 01m 28s 388ms\n",
      "\u001b[32m2024-07-23T09:47:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1940/2000, train/hateful_memes/cross_entropy: 0.0318, train/hateful_memes/cross_entropy/avg: 0.2740, train/total_loss: 0.0318, train/total_loss/avg: 0.2740, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 19, num_updates: 1940, iterations: 1940, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 862ms, time_since_start: 51m 36s 286ms, eta: 01m 22s 574ms\n",
      "\u001b[32m2024-07-23T09:47:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/2000, train/hateful_memes/cross_entropy: 0.0318, train/hateful_memes/cross_entropy/avg: 0.2728, train/total_loss: 0.0318, train/total_loss/avg: 0.2728, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 19, num_updates: 1950, iterations: 1950, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 015ms, time_since_start: 51m 49s 301ms, eta: 01m 09s 631ms\n",
      "\u001b[32m2024-07-23T09:47:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1960/2000, train/hateful_memes/cross_entropy: 0.0318, train/hateful_memes/cross_entropy/avg: 0.2715, train/total_loss: 0.0318, train/total_loss/avg: 0.2715, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 19, num_updates: 1960, iterations: 1960, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 904ms, time_since_start: 52m 02s 205ms, eta: 55s 231ms\n",
      "\u001b[32m2024-07-23T09:48:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1970/2000, train/hateful_memes/cross_entropy: 0.0301, train/hateful_memes/cross_entropy/avg: 0.2702, train/total_loss: 0.0301, train/total_loss/avg: 0.2702, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 19, num_updates: 1970, iterations: 1970, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 056ms, time_since_start: 52m 15s 261ms, eta: 41s 910ms\n",
      "\u001b[32m2024-07-23T09:48:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1980/2000, train/hateful_memes/cross_entropy: 0.0301, train/hateful_memes/cross_entropy/avg: 0.2689, train/total_loss: 0.0301, train/total_loss/avg: 0.2689, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 19, num_updates: 1980, iterations: 1980, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 880ms, time_since_start: 52m 28s 142ms, eta: 27s 564ms\n",
      "\u001b[32m2024-07-23T09:48:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1990/2000, train/hateful_memes/cross_entropy: 0.0265, train/hateful_memes/cross_entropy/avg: 0.2676, train/total_loss: 0.0265, train/total_loss/avg: 0.2676, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 19, num_updates: 1990, iterations: 1990, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 924ms, time_since_start: 52m 41s 066ms, eta: 13s 829ms\n",
      "\u001b[32m2024-07-23T09:48:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-23T09:48:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:48:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:48:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:48:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, train/hateful_memes/cross_entropy: 0.0265, train/hateful_memes/cross_entropy/avg: 0.2664, train/total_loss: 0.0265, train/total_loss/avg: 0.2664, max mem: 13427.0, experiment: hateful_memes_lr5e5, epoch: 19, num_updates: 2000, iterations: 2000, max_updates: 2000, lr: 0., ups: 0.53, time: 19s 552ms, time_since_start: 53m 619ms, eta: 0ms\n",
      "\u001b[32m2024-07-23T09:48:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T09:48:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T09:48:54 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:48:54 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:48:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T09:49:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T09:49:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T09:49:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, val/hateful_memes/cross_entropy: 2.8393, val/total_loss: 2.8393, val/hateful_memes/accuracy: 0.6140, val/hateful_memes/binary_f1: 0.4881, val/hateful_memes/roc_auc: 0.6798, num_updates: 2000, epoch: 19, iterations: 2000, max_updates: 2000, val_time: 26s 953ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.700832\n",
      "\u001b[32m2024-07-23T09:49:13 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2024-07-23T09:49:13 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2024-07-23T09:49:13 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2024-07-23T09:49:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2024-07-23T09:49:21 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1400\n",
      "\u001b[32m2024-07-23T09:49:21 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1400\n",
      "\u001b[32m2024-07-23T09:49:21 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 14\n",
      "\u001b[32m2024-07-23T09:49:23 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2024-07-23T09:49:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.12s/it]\n",
      "\u001b[32m2024-07-23T09:49:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T09:49:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T09:49:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, val/hateful_memes/cross_entropy: 2.1562, val/total_loss: 2.1562, val/hateful_memes/accuracy: 0.6060, val/hateful_memes/binary_f1: 0.4632, val/hateful_memes/roc_auc: 0.7008\n",
      "\u001b[32m2024-07-23T09:49:31 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 53m 46s 181ms\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.002 MB of 0.140 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/cross_entropy █▇█▆▇▇▆▇▅▅▆▅▆▄▅▃▄▅▅▃▄▃▃▂▂▂▁▂▂▁▂▁▁▂▁▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/learning_rate ▂▄▅▇████▇▇▇▇▇▇▇▅▅▅▅▅▅▅▄▄▄▄▄▄▄▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/total_loss █▇█▆▇▇▆▇▅▅▆▅▆▄▅▃▄▅▅▃▄▃▃▂▂▂▁▂▂▁▂▁▁▂▁▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val/hateful_memes/accuracy ▁▂▄▆▅▅█▇▆█▇▆▆▇██▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val/hateful_memes/binary_f1 ▁▃▄▇▇▅██▆▇▇▆▆▆▇▇▆▇▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/cross_entropy ▁▂▁▁▁▂▁▁▄▂▄█▄▄▅▃▅▄▇▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/hateful_memes/roc_auc ▁▃▆▆▆▇▇▇▇▇▇▇▇██▇▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val/total_loss ▁▂▁▁▁▂▁▁▄▂▄█▄▄▅▃▅▄▇▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/cross_entropy 0.01982\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/total_loss 0.01982\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step 1400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val/hateful_memes/accuracy 0.606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val/hateful_memes/binary_f1 0.46322\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/cross_entropy 2.26438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/hateful_memes/roc_auc 0.70083\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val/total_loss 2.26438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhateful_memes_lr5e5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/ebdvaph8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240723_085546-ebdvaph8/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "Exception ignored in: <function WandbLogger.__del__ at 0x7f7d56b21f70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf/utils/logger.py\", line 457, in __del__\n",
      "    self._wandb.finish()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 4262, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 390, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2100, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2108, in _finish\n",
      "    tel.feature.finish = True\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
      "    self._run._telemetry_callback(self._obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 798, in _telemetry_callback\n",
      "    self._telemetry_flush()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 809, in _telemetry_flush\n",
      "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 101, in _publish_telemetry\n",
      "    self._publish(rec)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_roberta/defaults.yaml \\\n",
    "model=visual_roberta \\\n",
    "dataset=hateful_memes \\\n",
    "training.log_interval=10 \\\n",
    "training.seed=2024 \\\n",
    "training.batch_size=80 \\\n",
    "training.evaluation_interval=100 \\\n",
    "training.experiment_name=hateful_memes_lr5e5 \\\n",
    "training.max_updates=2000 \\\n",
    "optimizer.params.lr=5e-5 \\\n",
    "training.fp16=True \\\n",
    "scheduler.params.num_warmup_steps=200 \\\n",
    "checkpoint.max_to_keep=1 \\\n",
    "training.wandb.enabled=True \\\n",
    "run_type=train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_roberta/defaults.yaml\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_roberta\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 10\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option training.seed to 2024\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 80\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 100\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option training.experiment_name to hateful_memes_lr1e5\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 2000\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 1e-5\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option training.fp16 to True\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 200\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option training.wandb.enabled to True\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_roberta/defaults.yaml', 'model=visual_roberta', 'dataset=hateful_memes', 'training.log_interval=10', 'training.seed=2024', 'training.batch_size=80', 'training.evaluation_interval=100', 'training.experiment_name=hateful_memes_lr1e5', 'training.max_updates=2000', 'optimizer.params.lr=1e-5', 'training.fp16=True', 'scheduler.params.num_warmup_steps=200', 'checkpoint.max_to_keep=1', 'training.wandb.enabled=True', 'run_type=train_val'])\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf_cli.run: \u001b[0mTorch version: 1.11.0+cu102\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf_cli.run: \u001b[0mUsing seed 2024\n",
      "\u001b[32m2024-07-23T13:38:30 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/zeus/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/zeus/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/zeus/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at /home/zeus/.cache/huggingface/transformers/dfe8f1ad04cb25b61a647e3d13620f9bf0a0f51d277897b232a5735297134132.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\u001b[32m2024-07-23T13:38:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T13:38:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T13:38:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T13:38:32 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2024-07-23T13:38:32 | torch.distributed.nn.jit.instantiator: \u001b[0mCreated a temporary directory at /tmp/tmp8wudrlvy\n",
      "\u001b[32m2024-07-23T13:38:32 | torch.distributed.nn.jit.instantiator: \u001b[0mWriting /tmp/tmp8wudrlvy/_remote_module_non_sriptable.py\n",
      "HI BRO {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': ['cross_entropy'], 'model': 'visual_roberta'}\n",
      "SELFIE {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': ['cross_entropy'], 'model': 'visual_roberta'}\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "SELF ROBERT CONFIG RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "roberta-base\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/zeus/.cache/torch/mmf/distributed_-1/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VisualRobertaBase: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing VisualRobertaBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualRobertaBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualRobertaBase were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'roberta.embeddings.position_embeddings_visual.weight', 'roberta.embeddings.projection.bias', 'roberta.embeddings.token_type_embeddings_visual.weight', 'roberta.embeddings.projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-07-23T13:38:41 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2024-07-23T13:38:41 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikrant17\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/wandb/run-20240723_133842-1dcj8yho\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhateful_memes_lr1e5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/1dcj8yho\u001b[0m\n",
      "\u001b[32m2024-07-23T13:38:47 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2024-07-23T13:38:47 | mmf.trainers.mmf_trainer: \u001b[0mVisualRoberta(\n",
      "  (model): VisualRobertaForClassification(\n",
      "    (roberta): VisualRobertaBase(\n",
      "      (embeddings): RobertaVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(1, 768)\n",
      "        (position_embeddings_visual): Embedding(514, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2024-07-23T13:38:47 | mmf.utils.general: \u001b[0mTotal Parameters: 127208450. Trained Parameters: 127208450\n",
      "\u001b[32m2024-07-23T13:38:47 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2024-07-23T13:39:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10/2000, train/hateful_memes/cross_entropy: 0.7432, train/hateful_memes/cross_entropy/avg: 0.7432, train/total_loss: 0.7432, train/total_loss/avg: 0.7432, max mem: 13359.0, experiment: hateful_memes_lr1e5, epoch: 1, num_updates: 10, iterations: 10, max_updates: 2000, lr: 0., ups: 0.18, time: 56s 395ms, time_since_start: 01m 01s 966ms, eta: 03h 20m 08s 297ms\n",
      "\u001b[32m2024-07-23T13:40:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20/2000, train/hateful_memes/cross_entropy: 0.6596, train/hateful_memes/cross_entropy/avg: 0.7014, train/total_loss: 0.6596, train/total_loss/avg: 0.7014, max mem: 13359.0, experiment: hateful_memes_lr1e5, epoch: 1, num_updates: 20, iterations: 20, max_updates: 2000, lr: 0., ups: 0.33, time: 30s 387ms, time_since_start: 01m 32s 354ms, eta: 01h 47m 17s 994ms\n",
      "\u001b[32m2024-07-23T13:40:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 30/2000, train/hateful_memes/cross_entropy: 0.6925, train/hateful_memes/cross_entropy/avg: 0.6984, train/total_loss: 0.6925, train/total_loss/avg: 0.6984, max mem: 13359.0, experiment: hateful_memes_lr1e5, epoch: 1, num_updates: 30, iterations: 30, max_updates: 2000, lr: 0., ups: 0.29, time: 34s 411ms, time_since_start: 02m 06s 765ms, eta: 02h 53s 575ms\n",
      "\u001b[32m2024-07-23T13:41:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 40/2000, train/hateful_memes/cross_entropy: 0.6713, train/hateful_memes/cross_entropy/avg: 0.6916, train/total_loss: 0.6713, train/total_loss/avg: 0.6916, max mem: 13359.0, experiment: hateful_memes_lr1e5, epoch: 1, num_updates: 40, iterations: 40, max_updates: 2000, lr: 0., ups: 0.40, time: 25s 718ms, time_since_start: 02m 32s 484ms, eta: 01h 29m 53s 766ms\n",
      "\u001b[32m2024-07-23T13:41:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/2000, train/hateful_memes/cross_entropy: 0.6713, train/hateful_memes/cross_entropy/avg: 0.6725, train/total_loss: 0.6713, train/total_loss/avg: 0.6725, max mem: 13359.0, experiment: hateful_memes_lr1e5, epoch: 1, num_updates: 50, iterations: 50, max_updates: 2000, lr: 0., ups: 0.48, time: 21s 240ms, time_since_start: 02m 53s 725ms, eta: 01h 13m 51s 847ms\n",
      "\u001b[32m2024-07-23T13:41:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 60/2000, train/hateful_memes/cross_entropy: 0.6596, train/hateful_memes/cross_entropy/avg: 0.6647, train/total_loss: 0.6596, train/total_loss/avg: 0.6647, max mem: 13359.0, experiment: hateful_memes_lr1e5, epoch: 1, num_updates: 60, iterations: 60, max_updates: 2000, lr: 0., ups: 0.56, time: 18s 752ms, time_since_start: 03m 12s 477ms, eta: 01h 04m 52s 673ms\n",
      "\u001b[32m2024-07-23T13:42:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 70/2000, train/hateful_memes/cross_entropy: 0.6713, train/hateful_memes/cross_entropy/avg: 0.6693, train/total_loss: 0.6713, train/total_loss/avg: 0.6693, max mem: 13359.0, experiment: hateful_memes_lr1e5, epoch: 1, num_updates: 70, iterations: 70, max_updates: 2000, lr: 0., ups: 0.45, time: 22s 525ms, time_since_start: 03m 35s 003ms, eta: 01h 17m 31s 841ms\n",
      "\u001b[32m2024-07-23T13:42:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 80/2000, train/hateful_memes/cross_entropy: 0.6596, train/hateful_memes/cross_entropy/avg: 0.6660, train/total_loss: 0.6596, train/total_loss/avg: 0.6660, max mem: 13359.0, experiment: hateful_memes_lr1e5, epoch: 1, num_updates: 80, iterations: 80, max_updates: 2000, lr: 0., ups: 0.56, time: 18s 912ms, time_since_start: 03m 53s 915ms, eta: 01h 04m 45s 294ms\n",
      "\u001b[32m2024-07-23T13:42:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 90/2000, train/hateful_memes/cross_entropy: 0.6713, train/hateful_memes/cross_entropy/avg: 0.6685, train/total_loss: 0.6713, train/total_loss/avg: 0.6685, max mem: 13359.0, experiment: hateful_memes_lr1e5, epoch: 1, num_updates: 90, iterations: 90, max_updates: 2000, lr: 0., ups: 0.48, time: 21s 113ms, time_since_start: 04m 15s 029ms, eta: 01h 11m 55s 016ms\n",
      "\u001b[32m2024-07-23T13:43:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, train/hateful_memes/cross_entropy: 0.6596, train/hateful_memes/cross_entropy/avg: 0.6671, train/total_loss: 0.6596, train/total_loss/avg: 0.6671, max mem: 13359.0, experiment: hateful_memes_lr1e5, epoch: 1, num_updates: 100, iterations: 100, max_updates: 2000, lr: 0.00001, ups: 0.56, time: 18s 566ms, time_since_start: 04m 33s 596ms, eta: 01h 02m 54s 725ms\n",
      "\u001b[32m2024-07-23T13:43:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:43:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:43:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:43:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:43:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:43:38 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T13:43:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:43:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:43:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, val/hateful_memes/cross_entropy: 0.7240, val/total_loss: 0.7240, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5379, num_updates: 100, epoch: 1, iterations: 100, max_updates: 2000, val_time: 39s 059ms, best_update: 100, best_iteration: 100, best_val/hateful_memes/roc_auc: 0.537880\n",
      "\u001b[32m2024-07-23T13:44:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 110/2000, train/hateful_memes/cross_entropy: 0.6596, train/hateful_memes/cross_entropy/avg: 0.6626, train/total_loss: 0.6596, train/total_loss/avg: 0.6626, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 2, num_updates: 110, iterations: 110, max_updates: 2000, lr: 0.00001, ups: 0.53, time: 19s 411ms, time_since_start: 05m 32s 076ms, eta: 01h 05m 25s 679ms\n",
      "\u001b[32m2024-07-23T13:44:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 120/2000, train/hateful_memes/cross_entropy: 0.6539, train/hateful_memes/cross_entropy/avg: 0.6617, train/total_loss: 0.6539, train/total_loss/avg: 0.6617, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 2, num_updates: 120, iterations: 120, max_updates: 2000, lr: 0.00001, ups: 0.42, time: 24s 974ms, time_since_start: 05m 57s 050ms, eta: 01h 23m 43s 841ms\n",
      "\u001b[32m2024-07-23T13:44:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 130/2000, train/hateful_memes/cross_entropy: 0.6539, train/hateful_memes/cross_entropy/avg: 0.6584, train/total_loss: 0.6539, train/total_loss/avg: 0.6584, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 2, num_updates: 130, iterations: 130, max_updates: 2000, lr: 0.00001, ups: 0.50, time: 20s 841ms, time_since_start: 06m 17s 891ms, eta: 01h 09m 30s 112ms\n",
      "\u001b[32m2024-07-23T13:45:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 140/2000, train/hateful_memes/cross_entropy: 0.6513, train/hateful_memes/cross_entropy/avg: 0.6494, train/total_loss: 0.6513, train/total_loss/avg: 0.6494, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 2, num_updates: 140, iterations: 140, max_updates: 2000, lr: 0.00001, ups: 0.43, time: 23s 022ms, time_since_start: 06m 40s 913ms, eta: 01h 16m 21s 850ms\n",
      "\u001b[32m2024-07-23T13:45:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/2000, train/hateful_memes/cross_entropy: 0.6539, train/hateful_memes/cross_entropy/avg: 0.6506, train/total_loss: 0.6539, train/total_loss/avg: 0.6506, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 2, num_updates: 150, iterations: 150, max_updates: 2000, lr: 0.00001, ups: 0.43, time: 23s 518ms, time_since_start: 07m 04s 432ms, eta: 01h 17m 35s 484ms\n",
      "\u001b[32m2024-07-23T13:46:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 160/2000, train/hateful_memes/cross_entropy: 0.6513, train/hateful_memes/cross_entropy/avg: 0.6478, train/total_loss: 0.6513, train/total_loss/avg: 0.6478, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 2, num_updates: 160, iterations: 160, max_updates: 2000, lr: 0.00001, ups: 0.48, time: 21s 675ms, time_since_start: 07m 26s 108ms, eta: 01h 11m 07s 550ms\n",
      "\u001b[32m2024-07-23T13:46:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 170/2000, train/hateful_memes/cross_entropy: 0.6513, train/hateful_memes/cross_entropy/avg: 0.6452, train/total_loss: 0.6513, train/total_loss/avg: 0.6452, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 2, num_updates: 170, iterations: 170, max_updates: 2000, lr: 0.00001, ups: 0.43, time: 23s 698ms, time_since_start: 07m 49s 807ms, eta: 01h 17m 20s 453ms\n",
      "\u001b[32m2024-07-23T13:46:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 180/2000, train/hateful_memes/cross_entropy: 0.6432, train/hateful_memes/cross_entropy/avg: 0.6431, train/total_loss: 0.6432, train/total_loss/avg: 0.6431, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 2, num_updates: 180, iterations: 180, max_updates: 2000, lr: 0.00001, ups: 0.43, time: 23s 956ms, time_since_start: 08m 13s 763ms, eta: 01h 17m 45s 195ms\n",
      "\u001b[32m2024-07-23T13:47:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 190/2000, train/hateful_memes/cross_entropy: 0.6432, train/hateful_memes/cross_entropy/avg: 0.6418, train/total_loss: 0.6432, train/total_loss/avg: 0.6418, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 2, num_updates: 190, iterations: 190, max_updates: 2000, lr: 0.00001, ups: 0.42, time: 24s 561ms, time_since_start: 08m 38s 325ms, eta: 01h 19m 16s 909ms\n",
      "\u001b[32m2024-07-23T13:47:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, train/hateful_memes/cross_entropy: 0.6370, train/hateful_memes/cross_entropy/avg: 0.6415, train/total_loss: 0.6370, train/total_loss/avg: 0.6415, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 2, num_updates: 200, iterations: 200, max_updates: 2000, lr: 0.00001, ups: 0.36, time: 28s 202ms, time_since_start: 09m 06s 528ms, eta: 01h 30m 31s 889ms\n",
      "\u001b[32m2024-07-23T13:47:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:47:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:47:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:47:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:47:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:48:02 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T13:48:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:48:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:48:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, val/hateful_memes/cross_entropy: 0.7552, val/total_loss: 0.7552, val/hateful_memes/accuracy: 0.5220, val/hateful_memes/binary_f1: 0.1555, val/hateful_memes/roc_auc: 0.5756, num_updates: 200, epoch: 2, iterations: 200, max_updates: 2000, val_time: 33s 832ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.575632\n",
      "\u001b[32m2024-07-23T13:48:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 210/2000, train/hateful_memes/cross_entropy: 0.6258, train/hateful_memes/cross_entropy/avg: 0.6400, train/total_loss: 0.6258, train/total_loss/avg: 0.6400, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 2, num_updates: 210, iterations: 210, max_updates: 2000, lr: 0.00001, ups: 0.62, time: 16s 199ms, time_since_start: 09m 56s 567ms, eta: 51m 42s 633ms\n",
      "\u001b[32m2024-07-23T13:48:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 220/2000, train/hateful_memes/cross_entropy: 0.6192, train/hateful_memes/cross_entropy/avg: 0.6375, train/total_loss: 0.6192, train/total_loss/avg: 0.6375, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 3, num_updates: 220, iterations: 220, max_updates: 2000, lr: 0.00001, ups: 0.50, time: 20s 772ms, time_since_start: 10m 17s 340ms, eta: 01h 05m 56s 375ms\n",
      "\u001b[32m2024-07-23T13:49:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 230/2000, train/hateful_memes/cross_entropy: 0.6192, train/hateful_memes/cross_entropy/avg: 0.6416, train/total_loss: 0.6192, train/total_loss/avg: 0.6416, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 3, num_updates: 230, iterations: 230, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 711ms, time_since_start: 10m 30s 052ms, eta: 40m 07s 417ms\n",
      "\u001b[32m2024-07-23T13:49:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 240/2000, train/hateful_memes/cross_entropy: 0.6186, train/hateful_memes/cross_entropy/avg: 0.6390, train/total_loss: 0.6186, train/total_loss/avg: 0.6390, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 3, num_updates: 240, iterations: 240, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 700ms, time_since_start: 10m 42s 752ms, eta: 39m 51s 766ms\n",
      "\u001b[32m2024-07-23T13:49:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/2000, train/hateful_memes/cross_entropy: 0.6192, train/hateful_memes/cross_entropy/avg: 0.6408, train/total_loss: 0.6192, train/total_loss/avg: 0.6408, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 3, num_updates: 250, iterations: 250, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 675ms, time_since_start: 10m 55s 428ms, eta: 39m 33s 530ms\n",
      "\u001b[32m2024-07-23T13:49:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 260/2000, train/hateful_memes/cross_entropy: 0.6186, train/hateful_memes/cross_entropy/avg: 0.6387, train/total_loss: 0.6186, train/total_loss/avg: 0.6387, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 3, num_updates: 260, iterations: 260, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 646ms, time_since_start: 11m 08s 074ms, eta: 39m 14s 487ms\n",
      "\u001b[32m2024-07-23T13:50:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 270/2000, train/hateful_memes/cross_entropy: 0.6186, train/hateful_memes/cross_entropy/avg: 0.6363, train/total_loss: 0.6186, train/total_loss/avg: 0.6363, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 3, num_updates: 270, iterations: 270, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 651ms, time_since_start: 11m 20s 726ms, eta: 39m 01s 937ms\n",
      "\u001b[32m2024-07-23T13:50:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 280/2000, train/hateful_memes/cross_entropy: 0.6098, train/hateful_memes/cross_entropy/avg: 0.6327, train/total_loss: 0.6098, train/total_loss/avg: 0.6327, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 3, num_updates: 280, iterations: 280, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 597ms, time_since_start: 11m 33s 323ms, eta: 38m 38s 381ms\n",
      "\u001b[32m2024-07-23T13:50:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 290/2000, train/hateful_memes/cross_entropy: 0.6074, train/hateful_memes/cross_entropy/avg: 0.6304, train/total_loss: 0.6074, train/total_loss/avg: 0.6304, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 3, num_updates: 290, iterations: 290, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 674ms, time_since_start: 11m 45s 997ms, eta: 38m 39s 001ms\n",
      "\u001b[32m2024-07-23T13:50:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, train/hateful_memes/cross_entropy: 0.6059, train/hateful_memes/cross_entropy/avg: 0.6268, train/total_loss: 0.6059, train/total_loss/avg: 0.6268, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 3, num_updates: 300, iterations: 300, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 829ms, time_since_start: 11m 58s 826ms, eta: 38m 53s 610ms\n",
      "\u001b[32m2024-07-23T13:50:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:50:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:50:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:50:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:50:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:50:51 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T13:51:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:51:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:51:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, val/hateful_memes/cross_entropy: 0.7612, val/total_loss: 0.7612, val/hateful_memes/accuracy: 0.5460, val/hateful_memes/binary_f1: 0.2972, val/hateful_memes/roc_auc: 0.6352, num_updates: 300, epoch: 3, iterations: 300, max_updates: 2000, val_time: 31s 109ms, best_update: 300, best_iteration: 300, best_val/hateful_memes/roc_auc: 0.635216\n",
      "\u001b[32m2024-07-23T13:51:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 310/2000, train/hateful_memes/cross_entropy: 0.6034, train/hateful_memes/cross_entropy/avg: 0.6252, train/total_loss: 0.6034, train/total_loss/avg: 0.6252, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 3, num_updates: 310, iterations: 310, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 391ms, time_since_start: 12m 43s 336ms, eta: 40m 21s 662ms\n",
      "\u001b[32m2024-07-23T13:51:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 320/2000, train/hateful_memes/cross_entropy: 0.5869, train/hateful_memes/cross_entropy/avg: 0.6206, train/total_loss: 0.5869, train/total_loss/avg: 0.6206, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 3, num_updates: 320, iterations: 320, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 168ms, time_since_start: 12m 55s 505ms, eta: 36m 27s 481ms\n",
      "\u001b[32m2024-07-23T13:51:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 330/2000, train/hateful_memes/cross_entropy: 0.5840, train/hateful_memes/cross_entropy/avg: 0.6163, train/total_loss: 0.5840, train/total_loss/avg: 0.6163, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 4, num_updates: 330, iterations: 330, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 277ms, time_since_start: 13m 08s 782ms, eta: 39m 32s 565ms\n",
      "\u001b[32m2024-07-23T13:52:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 340/2000, train/hateful_memes/cross_entropy: 0.5840, train/hateful_memes/cross_entropy/avg: 0.6152, train/total_loss: 0.5840, train/total_loss/avg: 0.6152, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 4, num_updates: 340, iterations: 340, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 813ms, time_since_start: 13m 21s 596ms, eta: 37m 56s 011ms\n",
      "\u001b[32m2024-07-23T13:52:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/2000, train/hateful_memes/cross_entropy: 0.5810, train/hateful_memes/cross_entropy/avg: 0.6121, train/total_loss: 0.5810, train/total_loss/avg: 0.6121, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 4, num_updates: 350, iterations: 350, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 854ms, time_since_start: 13m 35s 451ms, eta: 40m 46s 094ms\n",
      "\u001b[32m2024-07-23T13:52:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 360/2000, train/hateful_memes/cross_entropy: 0.5788, train/hateful_memes/cross_entropy/avg: 0.6095, train/total_loss: 0.5788, train/total_loss/avg: 0.6095, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 4, num_updates: 360, iterations: 360, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 727ms, time_since_start: 13m 48s 178ms, eta: 37m 13s 348ms\n",
      "\u001b[32m2024-07-23T13:52:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 370/2000, train/hateful_memes/cross_entropy: 0.5783, train/hateful_memes/cross_entropy/avg: 0.6069, train/total_loss: 0.5783, train/total_loss/avg: 0.6069, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 4, num_updates: 370, iterations: 370, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 628ms, time_since_start: 14m 01s 807ms, eta: 39m 36s 998ms\n",
      "\u001b[32m2024-07-23T13:52:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 380/2000, train/hateful_memes/cross_entropy: 0.5740, train/hateful_memes/cross_entropy/avg: 0.6051, train/total_loss: 0.5740, train/total_loss/avg: 0.6051, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 4, num_updates: 380, iterations: 380, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 743ms, time_since_start: 14m 14s 550ms, eta: 36m 48s 900ms\n",
      "\u001b[32m2024-07-23T13:53:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 390/2000, train/hateful_memes/cross_entropy: 0.5664, train/hateful_memes/cross_entropy/avg: 0.6024, train/total_loss: 0.5664, train/total_loss/avg: 0.6024, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 4, num_updates: 390, iterations: 390, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 112ms, time_since_start: 14m 28s 662ms, eta: 40m 31s 096ms\n",
      "\u001b[32m2024-07-23T13:53:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, train/hateful_memes/cross_entropy: 0.5529, train/hateful_memes/cross_entropy/avg: 0.6011, train/total_loss: 0.5529, train/total_loss/avg: 0.6011, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 4, num_updates: 400, iterations: 400, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 048ms, time_since_start: 14m 41s 711ms, eta: 37m 13s 848ms\n",
      "\u001b[32m2024-07-23T13:53:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:53:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:53:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:53:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:53:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:53:35 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T13:53:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:53:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:53:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, val/hateful_memes/cross_entropy: 0.8304, val/total_loss: 0.8304, val/hateful_memes/accuracy: 0.5500, val/hateful_memes/binary_f1: 0.3284, val/hateful_memes/roc_auc: 0.6560, num_updates: 400, epoch: 4, iterations: 400, max_updates: 2000, val_time: 32s 323ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.656048\n",
      "\u001b[32m2024-07-23T13:54:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 410/2000, train/hateful_memes/cross_entropy: 0.5387, train/hateful_memes/cross_entropy/avg: 0.5964, train/total_loss: 0.5387, train/total_loss/avg: 0.5964, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 4, num_updates: 410, iterations: 410, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 071ms, time_since_start: 15m 28s 114ms, eta: 39m 54s 072ms\n",
      "\u001b[32m2024-07-23T13:54:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 420/2000, train/hateful_memes/cross_entropy: 0.5353, train/hateful_memes/cross_entropy/avg: 0.5932, train/total_loss: 0.5353, train/total_loss/avg: 0.5932, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 4, num_updates: 420, iterations: 420, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 561ms, time_since_start: 15m 40s 676ms, eta: 35m 23s 649ms\n",
      "\u001b[32m2024-07-23T13:54:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 430/2000, train/hateful_memes/cross_entropy: 0.5219, train/hateful_memes/cross_entropy/avg: 0.5892, train/total_loss: 0.5219, train/total_loss/avg: 0.5892, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 5, num_updates: 430, iterations: 430, max_updates: 2000, lr: 0.00001, ups: 0.91, time: 11s 536ms, time_since_start: 15m 52s 213ms, eta: 32m 18s 049ms\n",
      "\u001b[32m2024-07-23T13:54:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 440/2000, train/hateful_memes/cross_entropy: 0.5175, train/hateful_memes/cross_entropy/avg: 0.5867, train/total_loss: 0.5175, train/total_loss/avg: 0.5867, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 5, num_updates: 440, iterations: 440, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 747ms, time_since_start: 16m 04s 960ms, eta: 35m 27s 812ms\n",
      "\u001b[32m2024-07-23T13:54:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/2000, train/hateful_memes/cross_entropy: 0.5130, train/hateful_memes/cross_entropy/avg: 0.5842, train/total_loss: 0.5130, train/total_loss/avg: 0.5842, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 5, num_updates: 450, iterations: 450, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 262ms, time_since_start: 16m 18s 223ms, eta: 36m 39s 587ms\n",
      "\u001b[32m2024-07-23T13:55:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 460/2000, train/hateful_memes/cross_entropy: 0.5063, train/hateful_memes/cross_entropy/avg: 0.5813, train/total_loss: 0.5063, train/total_loss/avg: 0.5813, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 5, num_updates: 460, iterations: 460, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 835ms, time_since_start: 16m 31s 058ms, eta: 35m 15s 059ms\n",
      "\u001b[32m2024-07-23T13:55:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 470/2000, train/hateful_memes/cross_entropy: 0.4991, train/hateful_memes/cross_entropy/avg: 0.5793, train/total_loss: 0.4991, train/total_loss/avg: 0.5793, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 5, num_updates: 470, iterations: 470, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 412ms, time_since_start: 16m 44s 471ms, eta: 36m 35s 747ms\n",
      "\u001b[32m2024-07-23T13:55:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 480/2000, train/hateful_memes/cross_entropy: 0.4893, train/hateful_memes/cross_entropy/avg: 0.5772, train/total_loss: 0.4893, train/total_loss/avg: 0.5772, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 5, num_updates: 480, iterations: 480, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 917ms, time_since_start: 16m 57s 389ms, eta: 35m 967ms\n",
      "\u001b[32m2024-07-23T13:55:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 490/2000, train/hateful_memes/cross_entropy: 0.4893, train/hateful_memes/cross_entropy/avg: 0.5756, train/total_loss: 0.4893, train/total_loss/avg: 0.5756, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 5, num_updates: 490, iterations: 490, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 419ms, time_since_start: 17m 10s 808ms, eta: 36m 08s 161ms\n",
      "\u001b[32m2024-07-23T13:56:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, train/hateful_memes/cross_entropy: 0.4795, train/hateful_memes/cross_entropy/avg: 0.5719, train/total_loss: 0.4795, train/total_loss/avg: 0.5719, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 5, num_updates: 500, iterations: 500, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 858ms, time_since_start: 17m 23s 667ms, eta: 34m 23s 862ms\n",
      "\u001b[32m2024-07-23T13:56:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:56:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:56:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:56:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:56:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:56:17 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T13:56:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:56:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:56:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, val/hateful_memes/cross_entropy: 0.7774, val/total_loss: 0.7774, val/hateful_memes/accuracy: 0.5740, val/hateful_memes/binary_f1: 0.4259, val/hateful_memes/roc_auc: 0.6665, num_updates: 500, epoch: 5, iterations: 500, max_updates: 2000, val_time: 32s 928ms, best_update: 500, best_iteration: 500, best_val/hateful_memes/roc_auc: 0.666528\n",
      "\u001b[32m2024-07-23T13:56:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 510/2000, train/hateful_memes/cross_entropy: 0.4795, train/hateful_memes/cross_entropy/avg: 0.5719, train/total_loss: 0.4795, train/total_loss/avg: 0.5719, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 5, num_updates: 510, iterations: 510, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 729ms, time_since_start: 18m 10s 333ms, eta: 36m 28s 893ms\n",
      "\u001b[32m2024-07-23T13:57:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 520/2000, train/hateful_memes/cross_entropy: 0.4893, train/hateful_memes/cross_entropy/avg: 0.5711, train/total_loss: 0.4893, train/total_loss/avg: 0.5711, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 5, num_updates: 520, iterations: 520, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 047ms, time_since_start: 18m 23s 381ms, eta: 34m 26s 276ms\n",
      "\u001b[32m2024-07-23T13:57:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 530/2000, train/hateful_memes/cross_entropy: 0.4991, train/hateful_memes/cross_entropy/avg: 0.5722, train/total_loss: 0.4991, train/total_loss/avg: 0.5722, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 5, num_updates: 530, iterations: 530, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 210ms, time_since_start: 18m 36s 591ms, eta: 34m 37s 868ms\n",
      "\u001b[32m2024-07-23T13:57:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 540/2000, train/hateful_memes/cross_entropy: 0.4928, train/hateful_memes/cross_entropy/avg: 0.5707, train/total_loss: 0.4928, train/total_loss/avg: 0.5707, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 6, num_updates: 540, iterations: 540, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 551ms, time_since_start: 18m 49s 143ms, eta: 32m 40s 827ms\n",
      "\u001b[32m2024-07-23T13:57:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/2000, train/hateful_memes/cross_entropy: 0.4893, train/hateful_memes/cross_entropy/avg: 0.5683, train/total_loss: 0.4893, train/total_loss/avg: 0.5683, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 6, num_updates: 550, iterations: 550, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 447ms, time_since_start: 19m 01s 590ms, eta: 32m 11s 193ms\n",
      "\u001b[32m2024-07-23T13:57:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 560/2000, train/hateful_memes/cross_entropy: 0.4795, train/hateful_memes/cross_entropy/avg: 0.5655, train/total_loss: 0.4795, train/total_loss/avg: 0.5655, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 6, num_updates: 560, iterations: 560, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 224ms, time_since_start: 19m 14s 815ms, eta: 33m 57s 699ms\n",
      "\u001b[32m2024-07-23T13:58:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 570/2000, train/hateful_memes/cross_entropy: 0.4795, train/hateful_memes/cross_entropy/avg: 0.5644, train/total_loss: 0.4795, train/total_loss/avg: 0.5644, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 6, num_updates: 570, iterations: 570, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 387ms, time_since_start: 19m 27s 203ms, eta: 31m 35s 359ms\n",
      "\u001b[32m2024-07-23T13:58:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 580/2000, train/hateful_memes/cross_entropy: 0.4763, train/hateful_memes/cross_entropy/avg: 0.5616, train/total_loss: 0.4763, train/total_loss/avg: 0.5616, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 6, num_updates: 580, iterations: 580, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 230ms, time_since_start: 19m 40s 433ms, eta: 33m 30s 298ms\n",
      "\u001b[32m2024-07-23T13:58:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 590/2000, train/hateful_memes/cross_entropy: 0.4756, train/hateful_memes/cross_entropy/avg: 0.5589, train/total_loss: 0.4756, train/total_loss/avg: 0.5589, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 6, num_updates: 590, iterations: 590, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 471ms, time_since_start: 19m 52s 905ms, eta: 31m 21s 609ms\n",
      "\u001b[32m2024-07-23T13:58:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, train/hateful_memes/cross_entropy: 0.4756, train/hateful_memes/cross_entropy/avg: 0.5591, train/total_loss: 0.4756, train/total_loss/avg: 0.5591, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 6, num_updates: 600, iterations: 600, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 251ms, time_since_start: 20m 06s 156ms, eta: 33m 05s 031ms\n",
      "\u001b[32m2024-07-23T13:58:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:58:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:58:56 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:58:56 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:58:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:59:00 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T13:59:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:59:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:59:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, val/hateful_memes/cross_entropy: 1.0263, val/total_loss: 1.0263, val/hateful_memes/accuracy: 0.5480, val/hateful_memes/binary_f1: 0.2893, val/hateful_memes/roc_auc: 0.6720, num_updates: 600, epoch: 6, iterations: 600, max_updates: 2000, val_time: 34s 113ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.672032\n",
      "\u001b[32m2024-07-23T13:59:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 610/2000, train/hateful_memes/cross_entropy: 0.4756, train/hateful_memes/cross_entropy/avg: 0.5563, train/total_loss: 0.4756, train/total_loss/avg: 0.5563, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 6, num_updates: 610, iterations: 610, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 925ms, time_since_start: 20m 53s 203ms, eta: 32m 02s 414ms\n",
      "\u001b[32m2024-07-23T13:59:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 620/2000, train/hateful_memes/cross_entropy: 0.4763, train/hateful_memes/cross_entropy/avg: 0.5550, train/total_loss: 0.4763, train/total_loss/avg: 0.5550, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 6, num_updates: 620, iterations: 620, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 509ms, time_since_start: 21m 06s 712ms, eta: 33m 14s 782ms\n",
      "\u001b[32m2024-07-23T14:00:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 630/2000, train/hateful_memes/cross_entropy: 0.4763, train/hateful_memes/cross_entropy/avg: 0.5529, train/total_loss: 0.4763, train/total_loss/avg: 0.5529, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 6, num_updates: 630, iterations: 630, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 333ms, time_since_start: 21m 19s 045ms, eta: 30m 07s 941ms\n",
      "\u001b[32m2024-07-23T14:00:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 640/2000, train/hateful_memes/cross_entropy: 0.4763, train/hateful_memes/cross_entropy/avg: 0.5530, train/total_loss: 0.4763, train/total_loss/avg: 0.5530, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 6, num_updates: 640, iterations: 640, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 292ms, time_since_start: 21m 31s 338ms, eta: 29m 48s 759ms\n",
      "\u001b[32m2024-07-23T14:00:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/2000, train/hateful_memes/cross_entropy: 0.4763, train/hateful_memes/cross_entropy/avg: 0.5507, train/total_loss: 0.4763, train/total_loss/avg: 0.5507, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 7, num_updates: 650, iterations: 650, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 117ms, time_since_start: 21m 43s 455ms, eta: 29m 10s 376ms\n",
      "\u001b[32m2024-07-23T14:00:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 660/2000, train/hateful_memes/cross_entropy: 0.4763, train/hateful_memes/cross_entropy/avg: 0.5487, train/total_loss: 0.4763, train/total_loss/avg: 0.5487, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 7, num_updates: 660, iterations: 660, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 213ms, time_since_start: 21m 56s 669ms, eta: 31m 34s 544ms\n",
      "\u001b[32m2024-07-23T14:00:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 670/2000, train/hateful_memes/cross_entropy: 0.4596, train/hateful_memes/cross_entropy/avg: 0.5474, train/total_loss: 0.4596, train/total_loss/avg: 0.5474, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 7, num_updates: 670, iterations: 670, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 656ms, time_since_start: 22m 09s 325ms, eta: 30m 01s 148ms\n",
      "\u001b[32m2024-07-23T14:01:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 680/2000, train/hateful_memes/cross_entropy: 0.4407, train/hateful_memes/cross_entropy/avg: 0.5456, train/total_loss: 0.4407, train/total_loss/avg: 0.5456, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 7, num_updates: 680, iterations: 680, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 307ms, time_since_start: 22m 22s 633ms, eta: 31m 19s 562ms\n",
      "\u001b[32m2024-07-23T14:01:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 690/2000, train/hateful_memes/cross_entropy: 0.4285, train/hateful_memes/cross_entropy/avg: 0.5433, train/total_loss: 0.4285, train/total_loss/avg: 0.5433, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 7, num_updates: 690, iterations: 690, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 747ms, time_since_start: 22m 35s 381ms, eta: 29m 46s 873ms\n",
      "\u001b[32m2024-07-23T14:01:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, train/hateful_memes/cross_entropy: 0.4285, train/hateful_memes/cross_entropy/avg: 0.5413, train/total_loss: 0.4285, train/total_loss/avg: 0.5413, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 7, num_updates: 700, iterations: 700, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 317ms, time_since_start: 22m 48s 698ms, eta: 30m 52s 496ms\n",
      "\u001b[32m2024-07-23T14:01:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:01:30 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:01:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:01:38 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:01:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:01:41 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T14:01:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:02:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:02:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, val/hateful_memes/cross_entropy: 0.8355, val/total_loss: 0.8355, val/hateful_memes/accuracy: 0.6100, val/hateful_memes/binary_f1: 0.5013, val/hateful_memes/roc_auc: 0.6892, num_updates: 700, epoch: 7, iterations: 700, max_updates: 2000, val_time: 32s 408ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.689200\n",
      "\u001b[32m2024-07-23T14:02:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 710/2000, train/hateful_memes/cross_entropy: 0.4285, train/hateful_memes/cross_entropy/avg: 0.5426, train/total_loss: 0.4285, train/total_loss/avg: 0.5426, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 7, num_updates: 710, iterations: 710, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 253ms, time_since_start: 23m 34s 368ms, eta: 30m 29s 394ms\n",
      "\u001b[32m2024-07-23T14:02:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 720/2000, train/hateful_memes/cross_entropy: 0.4211, train/hateful_memes/cross_entropy/avg: 0.5405, train/total_loss: 0.4211, train/total_loss/avg: 0.5405, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 7, num_updates: 720, iterations: 720, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 027ms, time_since_start: 23m 48s 395ms, eta: 32m 01s 169ms\n",
      "\u001b[32m2024-07-23T14:02:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 730/2000, train/hateful_memes/cross_entropy: 0.4190, train/hateful_memes/cross_entropy/avg: 0.5384, train/total_loss: 0.4190, train/total_loss/avg: 0.5384, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 7, num_updates: 730, iterations: 730, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 007ms, time_since_start: 24m 01s 402ms, eta: 29m 27s 532ms\n",
      "\u001b[32m2024-07-23T14:02:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 740/2000, train/hateful_memes/cross_entropy: 0.4106, train/hateful_memes/cross_entropy/avg: 0.5360, train/total_loss: 0.4106, train/total_loss/avg: 0.5360, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 7, num_updates: 740, iterations: 740, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 826ms, time_since_start: 24m 15s 228ms, eta: 31m 04s 041ms\n",
      "\u001b[32m2024-07-23T14:03:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/2000, train/hateful_memes/cross_entropy: 0.4068, train/hateful_memes/cross_entropy/avg: 0.5332, train/total_loss: 0.4068, train/total_loss/avg: 0.5332, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 8, num_updates: 750, iterations: 750, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 064ms, time_since_start: 24m 27s 292ms, eta: 26m 53s 585ms\n",
      "\u001b[32m2024-07-23T14:03:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 760/2000, train/hateful_memes/cross_entropy: 0.4064, train/hateful_memes/cross_entropy/avg: 0.5310, train/total_loss: 0.4064, train/total_loss/avg: 0.5310, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 8, num_updates: 760, iterations: 760, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 182ms, time_since_start: 24m 40s 475ms, eta: 29m 09s 005ms\n",
      "\u001b[32m2024-07-23T14:03:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 770/2000, train/hateful_memes/cross_entropy: 0.4064, train/hateful_memes/cross_entropy/avg: 0.5301, train/total_loss: 0.4064, train/total_loss/avg: 0.5301, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 8, num_updates: 770, iterations: 770, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 620ms, time_since_start: 24m 54s 095ms, eta: 29m 52s 579ms\n",
      "\u001b[32m2024-07-23T14:03:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 780/2000, train/hateful_memes/cross_entropy: 0.4025, train/hateful_memes/cross_entropy/avg: 0.5284, train/total_loss: 0.4025, train/total_loss/avg: 0.5284, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 8, num_updates: 780, iterations: 780, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 003ms, time_since_start: 25m 07s 098ms, eta: 28m 17s 472ms\n",
      "\u001b[32m2024-07-23T14:04:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 790/2000, train/hateful_memes/cross_entropy: 0.3993, train/hateful_memes/cross_entropy/avg: 0.5257, train/total_loss: 0.3993, train/total_loss/avg: 0.5257, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 8, num_updates: 790, iterations: 790, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 452ms, time_since_start: 25m 20s 551ms, eta: 29m 01s 669ms\n",
      "\u001b[32m2024-07-23T14:04:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, train/hateful_memes/cross_entropy: 0.3993, train/hateful_memes/cross_entropy/avg: 0.5255, train/total_loss: 0.3993, train/total_loss/avg: 0.5255, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 8, num_updates: 800, iterations: 800, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 955ms, time_since_start: 25m 33s 506ms, eta: 27m 43s 504ms\n",
      "\u001b[32m2024-07-23T14:04:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:04:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:04:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:04:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:04:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:04:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:04:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:04:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, val/hateful_memes/cross_entropy: 0.8221, val/total_loss: 0.8221, val/hateful_memes/accuracy: 0.6140, val/hateful_memes/binary_f1: 0.5480, val/hateful_memes/roc_auc: 0.6791, num_updates: 800, epoch: 8, iterations: 800, max_updates: 2000, val_time: 21s 802ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.689200\n",
      "\u001b[32m2024-07-23T14:04:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 810/2000, train/hateful_memes/cross_entropy: 0.4064, train/hateful_memes/cross_entropy/avg: 0.5241, train/total_loss: 0.4064, train/total_loss/avg: 0.5241, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 8, num_updates: 810, iterations: 810, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 936ms, time_since_start: 26m 09s 254ms, eta: 29m 34s 585ms\n",
      "\u001b[32m2024-07-23T14:05:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 820/2000, train/hateful_memes/cross_entropy: 0.3993, train/hateful_memes/cross_entropy/avg: 0.5217, train/total_loss: 0.3993, train/total_loss/avg: 0.5217, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 8, num_updates: 820, iterations: 820, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 898ms, time_since_start: 26m 22s 152ms, eta: 27m 08s 605ms\n",
      "\u001b[32m2024-07-23T14:05:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 830/2000, train/hateful_memes/cross_entropy: 0.3993, train/hateful_memes/cross_entropy/avg: 0.5211, train/total_loss: 0.3993, train/total_loss/avg: 0.5211, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 8, num_updates: 830, iterations: 830, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 392ms, time_since_start: 26m 35s 545ms, eta: 27m 56s 607ms\n",
      "\u001b[32m2024-07-23T14:05:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 840/2000, train/hateful_memes/cross_entropy: 0.3934, train/hateful_memes/cross_entropy/avg: 0.5196, train/total_loss: 0.3934, train/total_loss/avg: 0.5196, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 8, num_updates: 840, iterations: 840, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 001ms, time_since_start: 26m 48s 547ms, eta: 26m 53s 802ms\n",
      "\u001b[32m2024-07-23T14:05:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/2000, train/hateful_memes/cross_entropy: 0.3904, train/hateful_memes/cross_entropy/avg: 0.5174, train/total_loss: 0.3904, train/total_loss/avg: 0.5174, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 8, num_updates: 850, iterations: 850, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 388ms, time_since_start: 27m 01s 935ms, eta: 27m 27s 427ms\n",
      "\u001b[32m2024-07-23T14:05:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 860/2000, train/hateful_memes/cross_entropy: 0.3904, train/hateful_memes/cross_entropy/avg: 0.5161, train/total_loss: 0.3904, train/total_loss/avg: 0.5161, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 9, num_updates: 860, iterations: 860, max_updates: 2000, lr: 0.00001, ups: 0.91, time: 11s 370ms, time_since_start: 27m 13s 306ms, eta: 23m 06s 994ms\n",
      "\u001b[32m2024-07-23T14:06:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 870/2000, train/hateful_memes/cross_entropy: 0.3879, train/hateful_memes/cross_entropy/avg: 0.5144, train/total_loss: 0.3879, train/total_loss/avg: 0.5144, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 9, num_updates: 870, iterations: 870, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 879ms, time_since_start: 27m 26s 186ms, eta: 25m 57s 306ms\n",
      "\u001b[32m2024-07-23T14:06:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 880/2000, train/hateful_memes/cross_entropy: 0.3879, train/hateful_memes/cross_entropy/avg: 0.5131, train/total_loss: 0.3879, train/total_loss/avg: 0.5131, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 9, num_updates: 880, iterations: 880, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 965ms, time_since_start: 27m 39s 152ms, eta: 25m 53s 819ms\n",
      "\u001b[32m2024-07-23T14:06:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 890/2000, train/hateful_memes/cross_entropy: 0.3879, train/hateful_memes/cross_entropy/avg: 0.5116, train/total_loss: 0.3879, train/total_loss/avg: 0.5116, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 9, num_updates: 890, iterations: 890, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 775ms, time_since_start: 27m 51s 927ms, eta: 25m 17s 335ms\n",
      "\u001b[32m2024-07-23T14:06:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, train/hateful_memes/cross_entropy: 0.3859, train/hateful_memes/cross_entropy/avg: 0.5100, train/total_loss: 0.3859, train/total_loss/avg: 0.5100, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 9, num_updates: 900, iterations: 900, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 956ms, time_since_start: 28m 04s 883ms, eta: 25m 24s 944ms\n",
      "\u001b[32m2024-07-23T14:06:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:06:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:06:55 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:06:55 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:06:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:06:58 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T14:07:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:07:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:07:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, val/hateful_memes/cross_entropy: 1.0143, val/total_loss: 1.0143, val/hateful_memes/accuracy: 0.5760, val/hateful_memes/binary_f1: 0.3765, val/hateful_memes/roc_auc: 0.6958, num_updates: 900, epoch: 9, iterations: 900, max_updates: 2000, val_time: 33s 439ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:07:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 910/2000, train/hateful_memes/cross_entropy: 0.3830, train/hateful_memes/cross_entropy/avg: 0.5071, train/total_loss: 0.3830, train/total_loss/avg: 0.5071, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 9, num_updates: 910, iterations: 910, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 551ms, time_since_start: 28m 51s 882ms, eta: 26m 20s 561ms\n",
      "\u001b[32m2024-07-23T14:07:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 920/2000, train/hateful_memes/cross_entropy: 0.3679, train/hateful_memes/cross_entropy/avg: 0.5052, train/total_loss: 0.3679, train/total_loss/avg: 0.5052, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 9, num_updates: 920, iterations: 920, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 797ms, time_since_start: 29m 04s 680ms, eta: 24m 38s 929ms\n",
      "\u001b[32m2024-07-23T14:07:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 930/2000, train/hateful_memes/cross_entropy: 0.3679, train/hateful_memes/cross_entropy/avg: 0.5044, train/total_loss: 0.3679, train/total_loss/avg: 0.5044, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 9, num_updates: 930, iterations: 930, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 548ms, time_since_start: 29m 17s 228ms, eta: 23m 56s 651ms\n",
      "\u001b[32m2024-07-23T14:08:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 940/2000, train/hateful_memes/cross_entropy: 0.3731, train/hateful_memes/cross_entropy/avg: 0.5030, train/total_loss: 0.3731, train/total_loss/avg: 0.5030, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 9, num_updates: 940, iterations: 940, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 717ms, time_since_start: 29m 29s 945ms, eta: 24m 02s 421ms\n",
      "\u001b[32m2024-07-23T14:08:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/2000, train/hateful_memes/cross_entropy: 0.3731, train/hateful_memes/cross_entropy/avg: 0.5009, train/total_loss: 0.3731, train/total_loss/avg: 0.5009, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 9, num_updates: 950, iterations: 950, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 861ms, time_since_start: 29m 42s 807ms, eta: 24m 04s 957ms\n",
      "\u001b[32m2024-07-23T14:08:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 960/2000, train/hateful_memes/cross_entropy: 0.3731, train/hateful_memes/cross_entropy/avg: 0.4992, train/total_loss: 0.3731, train/total_loss/avg: 0.4992, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 9, num_updates: 960, iterations: 960, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 568ms, time_since_start: 29m 55s 376ms, eta: 23m 18s 678ms\n",
      "\u001b[32m2024-07-23T14:08:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 970/2000, train/hateful_memes/cross_entropy: 0.3679, train/hateful_memes/cross_entropy/avg: 0.4966, train/total_loss: 0.3679, train/total_loss/avg: 0.4966, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 10, num_updates: 970, iterations: 970, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 359ms, time_since_start: 30m 07s 736ms, eta: 22m 42s 182ms\n",
      "\u001b[32m2024-07-23T14:09:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 980/2000, train/hateful_memes/cross_entropy: 0.3679, train/hateful_memes/cross_entropy/avg: 0.4943, train/total_loss: 0.3679, train/total_loss/avg: 0.4943, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 10, num_updates: 980, iterations: 980, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 590ms, time_since_start: 30m 21s 326ms, eta: 24m 43s 240ms\n",
      "\u001b[32m2024-07-23T14:09:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 990/2000, train/hateful_memes/cross_entropy: 0.3679, train/hateful_memes/cross_entropy/avg: 0.4919, train/total_loss: 0.3679, train/total_loss/avg: 0.4919, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 10, num_updates: 990, iterations: 990, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 736ms, time_since_start: 30m 34s 063ms, eta: 22m 56s 482ms\n",
      "\u001b[32m2024-07-23T14:09:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-23T14:09:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:09:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:09:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:09:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, train/hateful_memes/cross_entropy: 0.3349, train/hateful_memes/cross_entropy/avg: 0.4898, train/total_loss: 0.3349, train/total_loss/avg: 0.4898, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 10, num_updates: 1000, iterations: 1000, max_updates: 2000, lr: 0.00001, ups: 0.36, time: 28s 617ms, time_since_start: 31m 02s 680ms, eta: 51m 02s 076ms\n",
      "\u001b[32m2024-07-23T14:09:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:09:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:09:52 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:09:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:09:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:10:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:10:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:10:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, val/hateful_memes/cross_entropy: 0.9142, val/total_loss: 0.9142, val/hateful_memes/accuracy: 0.6280, val/hateful_memes/binary_f1: 0.5634, val/hateful_memes/roc_auc: 0.6921, num_updates: 1000, epoch: 10, iterations: 1000, max_updates: 2000, val_time: 27s 076ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:10:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1010/2000, train/hateful_memes/cross_entropy: 0.3349, train/hateful_memes/cross_entropy/avg: 0.4886, train/total_loss: 0.3349, train/total_loss/avg: 0.4886, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 10, num_updates: 1010, iterations: 1010, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 935ms, time_since_start: 31m 42s 694ms, eta: 22m 50s 290ms\n",
      "\u001b[32m2024-07-23T14:10:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1020/2000, train/hateful_memes/cross_entropy: 0.3349, train/hateful_memes/cross_entropy/avg: 0.4869, train/total_loss: 0.3349, train/total_loss/avg: 0.4869, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 10, num_updates: 1020, iterations: 1020, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 969ms, time_since_start: 31m 56s 663ms, eta: 24m 24s 814ms\n",
      "\u001b[32m2024-07-23T14:10:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1030/2000, train/hateful_memes/cross_entropy: 0.3347, train/hateful_memes/cross_entropy/avg: 0.4845, train/total_loss: 0.3347, train/total_loss/avg: 0.4845, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 10, num_updates: 1030, iterations: 1030, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 733ms, time_since_start: 32m 09s 397ms, eta: 22m 01s 628ms\n",
      "\u001b[32m2024-07-23T14:11:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1040/2000, train/hateful_memes/cross_entropy: 0.3347, train/hateful_memes/cross_entropy/avg: 0.4838, train/total_loss: 0.3347, train/total_loss/avg: 0.4838, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 10, num_updates: 1040, iterations: 1040, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 037ms, time_since_start: 32m 23s 434ms, eta: 24m 01s 905ms\n",
      "\u001b[32m2024-07-23T14:11:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/2000, train/hateful_memes/cross_entropy: 0.3318, train/hateful_memes/cross_entropy/avg: 0.4818, train/total_loss: 0.3318, train/total_loss/avg: 0.4818, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 10, num_updates: 1050, iterations: 1050, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 992ms, time_since_start: 32m 36s 427ms, eta: 22m 714ms\n",
      "\u001b[32m2024-07-23T14:11:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1060/2000, train/hateful_memes/cross_entropy: 0.3136, train/hateful_memes/cross_entropy/avg: 0.4801, train/total_loss: 0.3136, train/total_loss/avg: 0.4801, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 10, num_updates: 1060, iterations: 1060, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 820ms, time_since_start: 32m 50s 248ms, eta: 23m 10s 062ms\n",
      "\u001b[32m2024-07-23T14:11:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1070/2000, train/hateful_memes/cross_entropy: 0.3066, train/hateful_memes/cross_entropy/avg: 0.4796, train/total_loss: 0.3066, train/total_loss/avg: 0.4796, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 10, num_updates: 1070, iterations: 1070, max_updates: 2000, lr: 0.00001, ups: 0.91, time: 11s 083ms, time_since_start: 33m 01s 331ms, eta: 18m 22s 896ms\n",
      "\u001b[32m2024-07-23T14:11:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1080/2000, train/hateful_memes/cross_entropy: 0.2952, train/hateful_memes/cross_entropy/avg: 0.4775, train/total_loss: 0.2952, train/total_loss/avg: 0.4775, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 11, num_updates: 1080, iterations: 1080, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 670ms, time_since_start: 33m 15s 001ms, eta: 22m 25s 736ms\n",
      "\u001b[32m2024-07-23T14:12:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1090/2000, train/hateful_memes/cross_entropy: 0.2844, train/hateful_memes/cross_entropy/avg: 0.4754, train/total_loss: 0.2844, train/total_loss/avg: 0.4754, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 11, num_updates: 1090, iterations: 1090, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 683ms, time_since_start: 33m 27s 685ms, eta: 20m 34s 991ms\n",
      "\u001b[32m2024-07-23T14:12:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, train/hateful_memes/cross_entropy: 0.2800, train/hateful_memes/cross_entropy/avg: 0.4729, train/total_loss: 0.2800, train/total_loss/avg: 0.4729, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 11, num_updates: 1100, iterations: 1100, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 703ms, time_since_start: 33m 41s 389ms, eta: 21m 59s 659ms\n",
      "\u001b[32m2024-07-23T14:12:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:12:22 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:12:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:12:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:12:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:12:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:12:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:12:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, val/hateful_memes/cross_entropy: 1.1610, val/total_loss: 1.1610, val/hateful_memes/accuracy: 0.5880, val/hateful_memes/binary_f1: 0.4246, val/hateful_memes/roc_auc: 0.6896, num_updates: 1100, epoch: 11, iterations: 1100, max_updates: 2000, val_time: 21s 902ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:12:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1110/2000, train/hateful_memes/cross_entropy: 0.2844, train/hateful_memes/cross_entropy/avg: 0.4714, train/total_loss: 0.2844, train/total_loss/avg: 0.4714, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 11, num_updates: 1110, iterations: 1110, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 832ms, time_since_start: 34m 16s 132ms, eta: 20m 22s 088ms\n",
      "\u001b[32m2024-07-23T14:13:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1120/2000, train/hateful_memes/cross_entropy: 0.2844, train/hateful_memes/cross_entropy/avg: 0.4706, train/total_loss: 0.2844, train/total_loss/avg: 0.4706, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 11, num_updates: 1120, iterations: 1120, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 961ms, time_since_start: 34m 30s 093ms, eta: 21m 54s 586ms\n",
      "\u001b[32m2024-07-23T14:13:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1130/2000, train/hateful_memes/cross_entropy: 0.2800, train/hateful_memes/cross_entropy/avg: 0.4686, train/total_loss: 0.2800, train/total_loss/avg: 0.4686, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 11, num_updates: 1130, iterations: 1130, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 604ms, time_since_start: 34m 42s 698ms, eta: 19m 33s 380ms\n",
      "\u001b[32m2024-07-23T14:13:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1140/2000, train/hateful_memes/cross_entropy: 0.2800, train/hateful_memes/cross_entropy/avg: 0.4676, train/total_loss: 0.2800, train/total_loss/avg: 0.4676, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 11, num_updates: 1140, iterations: 1140, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 522ms, time_since_start: 34m 56s 220ms, eta: 20m 44s 304ms\n",
      "\u001b[32m2024-07-23T14:13:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/2000, train/hateful_memes/cross_entropy: 0.2800, train/hateful_memes/cross_entropy/avg: 0.4669, train/total_loss: 0.2800, train/total_loss/avg: 0.4669, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 11, num_updates: 1150, iterations: 1150, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 607ms, time_since_start: 35m 08s 828ms, eta: 19m 06s 623ms\n",
      "\u001b[32m2024-07-23T14:14:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1160/2000, train/hateful_memes/cross_entropy: 0.2800, train/hateful_memes/cross_entropy/avg: 0.4655, train/total_loss: 0.2800, train/total_loss/avg: 0.4655, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 11, num_updates: 1160, iterations: 1160, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 597ms, time_since_start: 35m 22s 425ms, eta: 20m 22s 116ms\n",
      "\u001b[32m2024-07-23T14:14:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1170/2000, train/hateful_memes/cross_entropy: 0.2800, train/hateful_memes/cross_entropy/avg: 0.4635, train/total_loss: 0.2800, train/total_loss/avg: 0.4635, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 11, num_updates: 1170, iterations: 1170, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 646ms, time_since_start: 35m 35s 071ms, eta: 18m 43s 133ms\n",
      "\u001b[32m2024-07-23T14:14:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1180/2000, train/hateful_memes/cross_entropy: 0.2800, train/hateful_memes/cross_entropy/avg: 0.4613, train/total_loss: 0.2800, train/total_loss/avg: 0.4613, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 12, num_updates: 1180, iterations: 1180, max_updates: 2000, lr: 0., ups: 0.91, time: 11s 885ms, time_since_start: 35m 46s 957ms, eta: 17m 22s 852ms\n",
      "\u001b[32m2024-07-23T14:14:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1190/2000, train/hateful_memes/cross_entropy: 0.2800, train/hateful_memes/cross_entropy/avg: 0.4588, train/total_loss: 0.2800, train/total_loss/avg: 0.4588, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 12, num_updates: 1190, iterations: 1190, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 990ms, time_since_start: 35m 59s 948ms, eta: 18m 45s 914ms\n",
      "\u001b[32m2024-07-23T14:14:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, train/hateful_memes/cross_entropy: 0.2755, train/hateful_memes/cross_entropy/avg: 0.4568, train/total_loss: 0.2755, train/total_loss/avg: 0.4568, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 12, num_updates: 1200, iterations: 1200, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 549ms, time_since_start: 36m 12s 498ms, eta: 17m 54s 261ms\n",
      "\u001b[32m2024-07-23T14:14:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:14:54 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:15:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:15:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:15:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:15:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:15:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:15:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, val/hateful_memes/cross_entropy: 1.4645, val/total_loss: 1.4645, val/hateful_memes/accuracy: 0.5600, val/hateful_memes/binary_f1: 0.3491, val/hateful_memes/roc_auc: 0.6732, num_updates: 1200, epoch: 12, iterations: 1200, max_updates: 2000, val_time: 42s 709ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:15:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1210/2000, train/hateful_memes/cross_entropy: 0.2508, train/hateful_memes/cross_entropy/avg: 0.4544, train/total_loss: 0.2508, train/total_loss/avg: 0.4544, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 12, num_updates: 1210, iterations: 1210, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 501ms, time_since_start: 37m 08s 717ms, eta: 19m 01s 318ms\n",
      "\u001b[32m2024-07-23T14:16:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1220/2000, train/hateful_memes/cross_entropy: 0.2500, train/hateful_memes/cross_entropy/avg: 0.4519, train/total_loss: 0.2500, train/total_loss/avg: 0.4519, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 12, num_updates: 1220, iterations: 1220, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 571ms, time_since_start: 37m 21s 289ms, eta: 17m 29s 260ms\n",
      "\u001b[32m2024-07-23T14:16:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1230/2000, train/hateful_memes/cross_entropy: 0.2500, train/hateful_memes/cross_entropy/avg: 0.4499, train/total_loss: 0.2500, train/total_loss/avg: 0.4499, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 12, num_updates: 1230, iterations: 1230, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 049ms, time_since_start: 37m 34s 338ms, eta: 17m 55s 115ms\n",
      "\u001b[32m2024-07-23T14:16:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1240/2000, train/hateful_memes/cross_entropy: 0.2416, train/hateful_memes/cross_entropy/avg: 0.4480, train/total_loss: 0.2416, train/total_loss/avg: 0.4480, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 12, num_updates: 1240, iterations: 1240, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 548ms, time_since_start: 37m 46s 886ms, eta: 17m 408ms\n",
      "\u001b[32m2024-07-23T14:16:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/2000, train/hateful_memes/cross_entropy: 0.2391, train/hateful_memes/cross_entropy/avg: 0.4462, train/total_loss: 0.2391, train/total_loss/avg: 0.4462, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 12, num_updates: 1250, iterations: 1250, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 982ms, time_since_start: 37m 59s 868ms, eta: 17m 21s 812ms\n",
      "\u001b[32m2024-07-23T14:16:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1260/2000, train/hateful_memes/cross_entropy: 0.2391, train/hateful_memes/cross_entropy/avg: 0.4454, train/total_loss: 0.2391, train/total_loss/avg: 0.4454, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 12, num_updates: 1260, iterations: 1260, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 558ms, time_since_start: 38m 12s 427ms, eta: 16m 34s 412ms\n",
      "\u001b[32m2024-07-23T14:17:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1270/2000, train/hateful_memes/cross_entropy: 0.2391, train/hateful_memes/cross_entropy/avg: 0.4439, train/total_loss: 0.2391, train/total_loss/avg: 0.4439, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 12, num_updates: 1270, iterations: 1270, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 051ms, time_since_start: 38m 25s 479ms, eta: 16m 59s 481ms\n",
      "\u001b[32m2024-07-23T14:17:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1280/2000, train/hateful_memes/cross_entropy: 0.2300, train/hateful_memes/cross_entropy/avg: 0.4421, train/total_loss: 0.2300, train/total_loss/avg: 0.4421, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 12, num_updates: 1280, iterations: 1280, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 478ms, time_since_start: 38m 37s 957ms, eta: 16m 01s 313ms\n",
      "\u001b[32m2024-07-23T14:17:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1290/2000, train/hateful_memes/cross_entropy: 0.2240, train/hateful_memes/cross_entropy/avg: 0.4400, train/total_loss: 0.2240, train/total_loss/avg: 0.4400, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 13, num_updates: 1290, iterations: 1290, max_updates: 2000, lr: 0., ups: 0.91, time: 11s 956ms, time_since_start: 38m 49s 914ms, eta: 15m 08s 364ms\n",
      "\u001b[32m2024-07-23T14:17:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, train/hateful_memes/cross_entropy: 0.2240, train/hateful_memes/cross_entropy/avg: 0.4377, train/total_loss: 0.2240, train/total_loss/avg: 0.4377, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 13, num_updates: 1300, iterations: 1300, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 857ms, time_since_start: 39m 02s 771ms, eta: 16m 03s 012ms\n",
      "\u001b[32m2024-07-23T14:17:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:17:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:17:52 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:17:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:17:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:17:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:18:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:18:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, val/hateful_memes/cross_entropy: 1.3181, val/total_loss: 1.3181, val/hateful_memes/accuracy: 0.5660, val/hateful_memes/binary_f1: 0.3636, val/hateful_memes/roc_auc: 0.6895, num_updates: 1300, epoch: 13, iterations: 1300, max_updates: 2000, val_time: 22s 518ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:18:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1310/2000, train/hateful_memes/cross_entropy: 0.2156, train/hateful_memes/cross_entropy/avg: 0.4359, train/total_loss: 0.2156, train/total_loss/avg: 0.4359, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 13, num_updates: 1310, iterations: 1310, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 955ms, time_since_start: 39m 39s 253ms, eta: 17m 10s 329ms\n",
      "\u001b[32m2024-07-23T14:18:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1320/2000, train/hateful_memes/cross_entropy: 0.2100, train/hateful_memes/cross_entropy/avg: 0.4341, train/total_loss: 0.2100, train/total_loss/avg: 0.4341, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 13, num_updates: 1320, iterations: 1320, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 836ms, time_since_start: 39m 52s 089ms, eta: 15m 33s 986ms\n",
      "\u001b[32m2024-07-23T14:18:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1330/2000, train/hateful_memes/cross_entropy: 0.2070, train/hateful_memes/cross_entropy/avg: 0.4321, train/total_loss: 0.2070, train/total_loss/avg: 0.4321, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 13, num_updates: 1330, iterations: 1330, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 289ms, time_since_start: 40m 05s 379ms, eta: 15m 52s 726ms\n",
      "\u001b[32m2024-07-23T14:18:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1340/2000, train/hateful_memes/cross_entropy: 0.2070, train/hateful_memes/cross_entropy/avg: 0.4305, train/total_loss: 0.2070, train/total_loss/avg: 0.4305, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 13, num_updates: 1340, iterations: 1340, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 770ms, time_since_start: 40m 18s 149ms, eta: 15m 01s 844ms\n",
      "\u001b[32m2024-07-23T14:19:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/2000, train/hateful_memes/cross_entropy: 0.2063, train/hateful_memes/cross_entropy/avg: 0.4289, train/total_loss: 0.2063, train/total_loss/avg: 0.4289, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 13, num_updates: 1350, iterations: 1350, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 170ms, time_since_start: 40m 31s 319ms, eta: 15m 15s 978ms\n",
      "\u001b[32m2024-07-23T14:19:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1360/2000, train/hateful_memes/cross_entropy: 0.2060, train/hateful_memes/cross_entropy/avg: 0.4270, train/total_loss: 0.2060, train/total_loss/avg: 0.4270, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 13, num_updates: 1360, iterations: 1360, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 857ms, time_since_start: 40m 44s 176ms, eta: 14m 40s 460ms\n",
      "\u001b[32m2024-07-23T14:19:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1370/2000, train/hateful_memes/cross_entropy: 0.2053, train/hateful_memes/cross_entropy/avg: 0.4251, train/total_loss: 0.2053, train/total_loss/avg: 0.4251, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 13, num_updates: 1370, iterations: 1370, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 183ms, time_since_start: 40m 57s 360ms, eta: 14m 48s 732ms\n",
      "\u001b[32m2024-07-23T14:19:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1380/2000, train/hateful_memes/cross_entropy: 0.2053, train/hateful_memes/cross_entropy/avg: 0.4240, train/total_loss: 0.2053, train/total_loss/avg: 0.4240, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 13, num_updates: 1380, iterations: 1380, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 945ms, time_since_start: 41m 10s 306ms, eta: 14m 18s 794ms\n",
      "\u001b[32m2024-07-23T14:20:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1390/2000, train/hateful_memes/cross_entropy: 0.2060, train/hateful_memes/cross_entropy/avg: 0.4228, train/total_loss: 0.2060, train/total_loss/avg: 0.4228, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 13, num_updates: 1390, iterations: 1390, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 448ms, time_since_start: 41m 22s 754ms, eta: 13m 32s 504ms\n",
      "\u001b[32m2024-07-23T14:20:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, train/hateful_memes/cross_entropy: 0.2053, train/hateful_memes/cross_entropy/avg: 0.4211, train/total_loss: 0.2053, train/total_loss/avg: 0.4211, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 14, num_updates: 1400, iterations: 1400, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 977ms, time_since_start: 41m 35s 732ms, eta: 13m 53s 161ms\n",
      "\u001b[32m2024-07-23T14:20:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:20:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:20:25 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:20:25 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:20:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:20:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:20:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:20:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, val/hateful_memes/cross_entropy: 1.5271, val/total_loss: 1.5271, val/hateful_memes/accuracy: 0.5600, val/hateful_memes/binary_f1: 0.3529, val/hateful_memes/roc_auc: 0.6838, num_updates: 1400, epoch: 14, iterations: 1400, max_updates: 2000, val_time: 15s 073ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:20:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1410/2000, train/hateful_memes/cross_entropy: 0.2053, train/hateful_memes/cross_entropy/avg: 0.4195, train/total_loss: 0.2053, train/total_loss/avg: 0.4195, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 14, num_updates: 1410, iterations: 1410, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 883ms, time_since_start: 42m 04s 695ms, eta: 14m 36s 439ms\n",
      "\u001b[32m2024-07-23T14:21:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1420/2000, train/hateful_memes/cross_entropy: 0.2053, train/hateful_memes/cross_entropy/avg: 0.4179, train/total_loss: 0.2053, train/total_loss/avg: 0.4179, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 14, num_updates: 1420, iterations: 1420, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 953ms, time_since_start: 42m 18s 649ms, eta: 14m 25s 937ms\n",
      "\u001b[32m2024-07-23T14:21:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1430/2000, train/hateful_memes/cross_entropy: 0.1987, train/hateful_memes/cross_entropy/avg: 0.4164, train/total_loss: 0.1987, train/total_loss/avg: 0.4164, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 14, num_updates: 1430, iterations: 1430, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 237ms, time_since_start: 42m 31s 886ms, eta: 13m 27s 358ms\n",
      "\u001b[32m2024-07-23T14:21:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1440/2000, train/hateful_memes/cross_entropy: 0.1978, train/hateful_memes/cross_entropy/avg: 0.4148, train/total_loss: 0.1978, train/total_loss/avg: 0.4148, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 14, num_updates: 1440, iterations: 1440, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 843ms, time_since_start: 42m 45s 730ms, eta: 13m 49s 529ms\n",
      "\u001b[32m2024-07-23T14:21:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/2000, train/hateful_memes/cross_entropy: 0.1978, train/hateful_memes/cross_entropy/avg: 0.4137, train/total_loss: 0.1978, train/total_loss/avg: 0.4137, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 14, num_updates: 1450, iterations: 1450, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 207ms, time_since_start: 42m 58s 938ms, eta: 12m 57s 270ms\n",
      "\u001b[32m2024-07-23T14:21:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1460/2000, train/hateful_memes/cross_entropy: 0.1978, train/hateful_memes/cross_entropy/avg: 0.4126, train/total_loss: 0.1978, train/total_loss/avg: 0.4126, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 14, num_updates: 1460, iterations: 1460, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 892ms, time_since_start: 43m 12s 830ms, eta: 13m 22s 686ms\n",
      "\u001b[32m2024-07-23T14:22:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1470/2000, train/hateful_memes/cross_entropy: 0.1978, train/hateful_memes/cross_entropy/avg: 0.4119, train/total_loss: 0.1978, train/total_loss/avg: 0.4119, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 14, num_updates: 1470, iterations: 1470, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 230ms, time_since_start: 43m 26s 060ms, eta: 12m 30s 293ms\n",
      "\u001b[32m2024-07-23T14:22:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1480/2000, train/hateful_memes/cross_entropy: 0.1978, train/hateful_memes/cross_entropy/avg: 0.4106, train/total_loss: 0.1978, train/total_loss/avg: 0.4106, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 14, num_updates: 1480, iterations: 1480, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 874ms, time_since_start: 43m 39s 935ms, eta: 12m 51s 964ms\n",
      "\u001b[32m2024-07-23T14:22:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1490/2000, train/hateful_memes/cross_entropy: 0.1978, train/hateful_memes/cross_entropy/avg: 0.4087, train/total_loss: 0.1978, train/total_loss/avg: 0.4087, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 14, num_updates: 1490, iterations: 1490, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 347ms, time_since_start: 43m 53s 282ms, eta: 12m 08s 387ms\n",
      "\u001b[32m2024-07-23T14:22:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, train/hateful_memes/cross_entropy: 0.1978, train/hateful_memes/cross_entropy/avg: 0.4070, train/total_loss: 0.1978, train/total_loss/avg: 0.4070, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 15, num_updates: 1500, iterations: 1500, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 469ms, time_since_start: 44m 05s 752ms, eta: 11m 07s 145ms\n",
      "\u001b[32m2024-07-23T14:22:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:22:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:22:55 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:22:55 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:22:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:22:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:23:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:23:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, val/hateful_memes/cross_entropy: 1.3654, val/total_loss: 1.3654, val/hateful_memes/accuracy: 0.5760, val/hateful_memes/binary_f1: 0.4078, val/hateful_memes/roc_auc: 0.6813, num_updates: 1500, epoch: 15, iterations: 1500, max_updates: 2000, val_time: 14s 992ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:23:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1510/2000, train/hateful_memes/cross_entropy: 0.1978, train/hateful_memes/cross_entropy/avg: 0.4060, train/total_loss: 0.1978, train/total_loss/avg: 0.4060, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 15, num_updates: 1510, iterations: 1510, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 327ms, time_since_start: 44m 34s 079ms, eta: 11m 38s 737ms\n",
      "\u001b[32m2024-07-23T14:23:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1520/2000, train/hateful_memes/cross_entropy: 0.1978, train/hateful_memes/cross_entropy/avg: 0.4041, train/total_loss: 0.1978, train/total_loss/avg: 0.4041, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 15, num_updates: 1520, iterations: 1520, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 998ms, time_since_start: 44m 48s 077ms, eta: 11m 58s 938ms\n",
      "\u001b[32m2024-07-23T14:23:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1530/2000, train/hateful_memes/cross_entropy: 0.1978, train/hateful_memes/cross_entropy/avg: 0.4025, train/total_loss: 0.1978, train/total_loss/avg: 0.4025, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 15, num_updates: 1530, iterations: 1530, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 838ms, time_since_start: 45m 916ms, eta: 10m 45s 673ms\n",
      "\u001b[32m2024-07-23T14:23:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1540/2000, train/hateful_memes/cross_entropy: 0.1965, train/hateful_memes/cross_entropy/avg: 0.4008, train/total_loss: 0.1965, train/total_loss/avg: 0.4008, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 15, num_updates: 1540, iterations: 1540, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 904ms, time_since_start: 45m 14s 821ms, eta: 11m 24s 370ms\n",
      "\u001b[32m2024-07-23T14:24:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/2000, train/hateful_memes/cross_entropy: 0.1958, train/hateful_memes/cross_entropy/avg: 0.3991, train/total_loss: 0.1958, train/total_loss/avg: 0.3991, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 15, num_updates: 1550, iterations: 1550, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 799ms, time_since_start: 45m 27s 621ms, eta: 10m 16s 320ms\n",
      "\u001b[32m2024-07-23T14:24:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1560/2000, train/hateful_memes/cross_entropy: 0.1958, train/hateful_memes/cross_entropy/avg: 0.3972, train/total_loss: 0.1958, train/total_loss/avg: 0.3972, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 15, num_updates: 1560, iterations: 1560, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 792ms, time_since_start: 45m 41s 413ms, eta: 10m 49s 343ms\n",
      "\u001b[32m2024-07-23T14:24:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1570/2000, train/hateful_memes/cross_entropy: 0.1965, train/hateful_memes/cross_entropy/avg: 0.3962, train/total_loss: 0.1965, train/total_loss/avg: 0.3962, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 15, num_updates: 1570, iterations: 1570, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 766ms, time_since_start: 45m 54s 179ms, eta: 09m 47s 378ms\n",
      "\u001b[32m2024-07-23T14:24:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1580/2000, train/hateful_memes/cross_entropy: 0.1958, train/hateful_memes/cross_entropy/avg: 0.3946, train/total_loss: 0.1958, train/total_loss/avg: 0.3946, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 15, num_updates: 1580, iterations: 1580, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 813ms, time_since_start: 46m 07s 993ms, eta: 10m 20s 799ms\n",
      "\u001b[32m2024-07-23T14:25:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1590/2000, train/hateful_memes/cross_entropy: 0.1958, train/hateful_memes/cross_entropy/avg: 0.3940, train/total_loss: 0.1958, train/total_loss/avg: 0.3940, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 15, num_updates: 1590, iterations: 1590, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 814ms, time_since_start: 46m 20s 808ms, eta: 09m 22s 183ms\n",
      "\u001b[32m2024-07-23T14:25:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, train/hateful_memes/cross_entropy: 0.1958, train/hateful_memes/cross_entropy/avg: 0.3922, train/total_loss: 0.1958, train/total_loss/avg: 0.3922, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 15, num_updates: 1600, iterations: 1600, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 720ms, time_since_start: 46m 34s 529ms, eta: 09m 47s 246ms\n",
      "\u001b[32m2024-07-23T14:25:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:25:16 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:25:24 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:25:24 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:25:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:25:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:25:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:25:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, val/hateful_memes/cross_entropy: 1.4788, val/total_loss: 1.4788, val/hateful_memes/accuracy: 0.5860, val/hateful_memes/binary_f1: 0.4329, val/hateful_memes/roc_auc: 0.6896, num_updates: 1600, epoch: 15, iterations: 1600, max_updates: 2000, val_time: 22s 970ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:25:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1610/2000, train/hateful_memes/cross_entropy: 0.1958, train/hateful_memes/cross_entropy/avg: 0.3910, train/total_loss: 0.1958, train/total_loss/avg: 0.3910, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 16, num_updates: 1610, iterations: 1610, max_updates: 2000, lr: 0., ups: 0.91, time: 11s 783ms, time_since_start: 47m 09s 290ms, eta: 08m 11s 723ms\n",
      "\u001b[32m2024-07-23T14:26:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1620/2000, train/hateful_memes/cross_entropy: 0.1958, train/hateful_memes/cross_entropy/avg: 0.3899, train/total_loss: 0.1958, train/total_loss/avg: 0.3899, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 16, num_updates: 1620, iterations: 1620, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 116ms, time_since_start: 47m 22s 406ms, eta: 08m 53s 301ms\n",
      "\u001b[32m2024-07-23T14:26:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1630/2000, train/hateful_memes/cross_entropy: 0.1978, train/hateful_memes/cross_entropy/avg: 0.3887, train/total_loss: 0.1978, train/total_loss/avg: 0.3887, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 16, num_updates: 1630, iterations: 1630, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 045ms, time_since_start: 47m 35s 451ms, eta: 08m 36s 472ms\n",
      "\u001b[32m2024-07-23T14:26:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1640/2000, train/hateful_memes/cross_entropy: 0.1650, train/hateful_memes/cross_entropy/avg: 0.3869, train/total_loss: 0.1650, train/total_loss/avg: 0.3869, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 16, num_updates: 1640, iterations: 1640, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 163ms, time_since_start: 47m 48s 615ms, eta: 08m 27s 054ms\n",
      "\u001b[32m2024-07-23T14:26:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/2000, train/hateful_memes/cross_entropy: 0.1650, train/hateful_memes/cross_entropy/avg: 0.3856, train/total_loss: 0.1650, train/total_loss/avg: 0.3856, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 16, num_updates: 1650, iterations: 1650, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 982ms, time_since_start: 48m 01s 597ms, eta: 08m 06s 187ms\n",
      "\u001b[32m2024-07-23T14:26:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1660/2000, train/hateful_memes/cross_entropy: 0.1650, train/hateful_memes/cross_entropy/avg: 0.3847, train/total_loss: 0.1650, train/total_loss/avg: 0.3847, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 16, num_updates: 1660, iterations: 1660, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 149ms, time_since_start: 48m 14s 747ms, eta: 07m 58s 380ms\n",
      "\u001b[32m2024-07-23T14:27:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1670/2000, train/hateful_memes/cross_entropy: 0.1650, train/hateful_memes/cross_entropy/avg: 0.3836, train/total_loss: 0.1650, train/total_loss/avg: 0.3836, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 16, num_updates: 1670, iterations: 1670, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 040ms, time_since_start: 48m 27s 788ms, eta: 07m 40s 474ms\n",
      "\u001b[32m2024-07-23T14:27:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1680/2000, train/hateful_memes/cross_entropy: 0.1650, train/hateful_memes/cross_entropy/avg: 0.3827, train/total_loss: 0.1650, train/total_loss/avg: 0.3827, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 16, num_updates: 1680, iterations: 1680, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 130ms, time_since_start: 48m 40s 918ms, eta: 07m 29s 587ms\n",
      "\u001b[32m2024-07-23T14:27:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1690/2000, train/hateful_memes/cross_entropy: 0.1650, train/hateful_memes/cross_entropy/avg: 0.3813, train/total_loss: 0.1650, train/total_loss/avg: 0.3813, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 16, num_updates: 1690, iterations: 1690, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 139ms, time_since_start: 48m 54s 058ms, eta: 07m 15s 853ms\n",
      "\u001b[32m2024-07-23T14:27:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, train/hateful_memes/cross_entropy: 0.1742, train/hateful_memes/cross_entropy/avg: 0.3805, train/total_loss: 0.1742, train/total_loss/avg: 0.3805, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 16, num_updates: 1700, iterations: 1700, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 155ms, time_since_start: 49m 07s 214ms, eta: 07m 02s 299ms\n",
      "\u001b[32m2024-07-23T14:27:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:27:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:27:56 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:27:56 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:27:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:28:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:28:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:28:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, val/hateful_memes/cross_entropy: 1.6199, val/total_loss: 1.6199, val/hateful_memes/accuracy: 0.5760, val/hateful_memes/binary_f1: 0.4144, val/hateful_memes/roc_auc: 0.6800, num_updates: 1700, epoch: 16, iterations: 1700, max_updates: 2000, val_time: 21s 204ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:28:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1710/2000, train/hateful_memes/cross_entropy: 0.1651, train/hateful_memes/cross_entropy/avg: 0.3793, train/total_loss: 0.1651, train/total_loss/avg: 0.3793, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 16, num_updates: 1710, iterations: 1710, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 747ms, time_since_start: 49m 41s 174ms, eta: 06m 35s 561ms\n",
      "\u001b[32m2024-07-23T14:28:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1720/2000, train/hateful_memes/cross_entropy: 0.1651, train/hateful_memes/cross_entropy/avg: 0.3780, train/total_loss: 0.1651, train/total_loss/avg: 0.3780, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 17, num_updates: 1720, iterations: 1720, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 419ms, time_since_start: 49m 53s 593ms, eta: 06m 12s 093ms\n",
      "\u001b[32m2024-07-23T14:28:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1730/2000, train/hateful_memes/cross_entropy: 0.1651, train/hateful_memes/cross_entropy/avg: 0.3766, train/total_loss: 0.1651, train/total_loss/avg: 0.3766, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 17, num_updates: 1730, iterations: 1730, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 309ms, time_since_start: 50m 06s 902ms, eta: 06m 24s 501ms\n",
      "\u001b[32m2024-07-23T14:29:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1740/2000, train/hateful_memes/cross_entropy: 0.1742, train/hateful_memes/cross_entropy/avg: 0.3756, train/total_loss: 0.1742, train/total_loss/avg: 0.3756, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 17, num_updates: 1740, iterations: 1740, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 729ms, time_since_start: 50m 19s 632ms, eta: 05m 54s 148ms\n",
      "\u001b[32m2024-07-23T14:29:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/2000, train/hateful_memes/cross_entropy: 0.1742, train/hateful_memes/cross_entropy/avg: 0.3743, train/total_loss: 0.1742, train/total_loss/avg: 0.3743, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 17, num_updates: 1750, iterations: 1750, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 222ms, time_since_start: 50m 32s 855ms, eta: 05m 53s 701ms\n",
      "\u001b[32m2024-07-23T14:29:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1760/2000, train/hateful_memes/cross_entropy: 0.1742, train/hateful_memes/cross_entropy/avg: 0.3730, train/total_loss: 0.1742, train/total_loss/avg: 0.3730, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 17, num_updates: 1760, iterations: 1760, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 799ms, time_since_start: 50m 45s 654ms, eta: 05m 28s 687ms\n",
      "\u001b[32m2024-07-23T14:29:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1770/2000, train/hateful_memes/cross_entropy: 0.1742, train/hateful_memes/cross_entropy/avg: 0.3719, train/total_loss: 0.1742, train/total_loss/avg: 0.3719, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 17, num_updates: 1770, iterations: 1770, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 268ms, time_since_start: 50m 58s 923ms, eta: 05m 26s 548ms\n",
      "\u001b[32m2024-07-23T14:29:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1780/2000, train/hateful_memes/cross_entropy: 0.1742, train/hateful_memes/cross_entropy/avg: 0.3704, train/total_loss: 0.1742, train/total_loss/avg: 0.3704, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 17, num_updates: 1780, iterations: 1780, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 779ms, time_since_start: 51m 11s 703ms, eta: 05m 830ms\n",
      "\u001b[32m2024-07-23T14:30:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1790/2000, train/hateful_memes/cross_entropy: 0.1651, train/hateful_memes/cross_entropy/avg: 0.3692, train/total_loss: 0.1651, train/total_loss/avg: 0.3692, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 17, num_updates: 1790, iterations: 1790, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 283ms, time_since_start: 51m 24s 986ms, eta: 04m 58s 479ms\n",
      "\u001b[32m2024-07-23T14:30:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, train/hateful_memes/cross_entropy: 0.1742, train/hateful_memes/cross_entropy/avg: 0.3683, train/total_loss: 0.1742, train/total_loss/avg: 0.3683, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 17, num_updates: 1800, iterations: 1800, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 838ms, time_since_start: 51m 37s 824ms, eta: 04m 34s 738ms\n",
      "\u001b[32m2024-07-23T14:30:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:30:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:30:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:30:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:30:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:30:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:30:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:30:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, val/hateful_memes/cross_entropy: 1.7228, val/total_loss: 1.7228, val/hateful_memes/accuracy: 0.5880, val/hateful_memes/binary_f1: 0.4341, val/hateful_memes/roc_auc: 0.6785, num_updates: 1800, epoch: 17, iterations: 1800, max_updates: 2000, val_time: 22s 777ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:30:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1810/2000, train/hateful_memes/cross_entropy: 0.1742, train/hateful_memes/cross_entropy/avg: 0.3672, train/total_loss: 0.1742, train/total_loss/avg: 0.3672, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 17, num_updates: 1810, iterations: 1810, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 840ms, time_since_start: 52m 14s 450ms, eta: 04m 41s 375ms\n",
      "\u001b[32m2024-07-23T14:31:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1820/2000, train/hateful_memes/cross_entropy: 0.1742, train/hateful_memes/cross_entropy/avg: 0.3663, train/total_loss: 0.1742, train/total_loss/avg: 0.3663, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 18, num_updates: 1820, iterations: 1820, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 014ms, time_since_start: 52m 26s 465ms, eta: 03m 51s 402ms\n",
      "\u001b[32m2024-07-23T14:31:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1830/2000, train/hateful_memes/cross_entropy: 0.1742, train/hateful_memes/cross_entropy/avg: 0.3653, train/total_loss: 0.1742, train/total_loss/avg: 0.3653, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 18, num_updates: 1830, iterations: 1830, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 786ms, time_since_start: 52m 39s 252ms, eta: 03m 52s 591ms\n",
      "\u001b[32m2024-07-23T14:31:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1840/2000, train/hateful_memes/cross_entropy: 0.1758, train/hateful_memes/cross_entropy/avg: 0.3646, train/total_loss: 0.1758, train/total_loss/avg: 0.3646, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 18, num_updates: 1840, iterations: 1840, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 925ms, time_since_start: 52m 53s 177ms, eta: 03m 58s 400ms\n",
      "\u001b[32m2024-07-23T14:31:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/2000, train/hateful_memes/cross_entropy: 0.1758, train/hateful_memes/cross_entropy/avg: 0.3631, train/total_loss: 0.1758, train/total_loss/avg: 0.3631, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 18, num_updates: 1850, iterations: 1850, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 958ms, time_since_start: 53m 06s 136ms, eta: 03m 27s 986ms\n",
      "\u001b[32m2024-07-23T14:32:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1860/2000, train/hateful_memes/cross_entropy: 0.1651, train/hateful_memes/cross_entropy/avg: 0.3617, train/total_loss: 0.1651, train/total_loss/avg: 0.3617, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 18, num_updates: 1860, iterations: 1860, max_updates: 2000, lr: 0., ups: 0.71, time: 14s 111ms, time_since_start: 53m 20s 248ms, eta: 03m 31s 395ms\n",
      "\u001b[32m2024-07-23T14:32:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1870/2000, train/hateful_memes/cross_entropy: 0.1646, train/hateful_memes/cross_entropy/avg: 0.3603, train/total_loss: 0.1646, train/total_loss/avg: 0.3603, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 18, num_updates: 1870, iterations: 1870, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 769ms, time_since_start: 53m 33s 017ms, eta: 02m 57s 625ms\n",
      "\u001b[32m2024-07-23T14:32:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1880/2000, train/hateful_memes/cross_entropy: 0.1646, train/hateful_memes/cross_entropy/avg: 0.3597, train/total_loss: 0.1646, train/total_loss/avg: 0.3597, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 18, num_updates: 1880, iterations: 1880, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 863ms, time_since_start: 53m 46s 881ms, eta: 02m 58s 012ms\n",
      "\u001b[32m2024-07-23T14:32:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1890/2000, train/hateful_memes/cross_entropy: 0.1646, train/hateful_memes/cross_entropy/avg: 0.3585, train/total_loss: 0.1646, train/total_loss/avg: 0.3585, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 18, num_updates: 1890, iterations: 1890, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 814ms, time_since_start: 53m 59s 696ms, eta: 02m 30s 824ms\n",
      "\u001b[32m2024-07-23T14:32:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, train/hateful_memes/cross_entropy: 0.1646, train/hateful_memes/cross_entropy/avg: 0.3575, train/total_loss: 0.1646, train/total_loss/avg: 0.3575, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 18, num_updates: 1900, iterations: 1900, max_updates: 2000, lr: 0., ups: 0.71, time: 14s 133ms, time_since_start: 54m 13s 829ms, eta: 02m 31s 225ms\n",
      "\u001b[32m2024-07-23T14:32:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:32:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:33:03 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:33:03 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:33:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:33:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:33:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:33:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, val/hateful_memes/cross_entropy: 1.7578, val/total_loss: 1.7578, val/hateful_memes/accuracy: 0.5660, val/hateful_memes/binary_f1: 0.3887, val/hateful_memes/roc_auc: 0.6750, num_updates: 1900, epoch: 18, iterations: 1900, max_updates: 2000, val_time: 41s 634ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:33:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1910/2000, train/hateful_memes/cross_entropy: 0.1646, train/hateful_memes/cross_entropy/avg: 0.3569, train/total_loss: 0.1646, train/total_loss/avg: 0.3569, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 18, num_updates: 1910, iterations: 1910, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 065ms, time_since_start: 55m 08s 536ms, eta: 02m 05s 821ms\n",
      "\u001b[32m2024-07-23T14:34:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1920/2000, train/hateful_memes/cross_entropy: 0.1663, train/hateful_memes/cross_entropy/avg: 0.3561, train/total_loss: 0.1663, train/total_loss/avg: 0.3561, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 18, num_updates: 1920, iterations: 1920, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 804ms, time_since_start: 55m 22s 340ms, eta: 01m 58s 162ms\n",
      "\u001b[32m2024-07-23T14:34:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1930/2000, train/hateful_memes/cross_entropy: 0.1663, train/hateful_memes/cross_entropy/avg: 0.3549, train/total_loss: 0.1663, train/total_loss/avg: 0.3549, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 19, num_updates: 1930, iterations: 1930, max_updates: 2000, lr: 0., ups: 0.91, time: 11s 279ms, time_since_start: 55m 33s 620ms, eta: 01m 24s 484ms\n",
      "\u001b[32m2024-07-23T14:34:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1940/2000, train/hateful_memes/cross_entropy: 0.1656, train/hateful_memes/cross_entropy/avg: 0.3539, train/total_loss: 0.1656, train/total_loss/avg: 0.3539, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 19, num_updates: 1940, iterations: 1940, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 573ms, time_since_start: 55m 47s 193ms, eta: 01m 27s 139ms\n",
      "\u001b[32m2024-07-23T14:34:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/2000, train/hateful_memes/cross_entropy: 0.1656, train/hateful_memes/cross_entropy/avg: 0.3527, train/total_loss: 0.1656, train/total_loss/avg: 0.3527, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 19, num_updates: 1950, iterations: 1950, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 605ms, time_since_start: 55m 59s 799ms, eta: 01m 07s 441ms\n",
      "\u001b[32m2024-07-23T14:34:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1960/2000, train/hateful_memes/cross_entropy: 0.1656, train/hateful_memes/cross_entropy/avg: 0.3514, train/total_loss: 0.1656, train/total_loss/avg: 0.3514, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 19, num_updates: 1960, iterations: 1960, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 617ms, time_since_start: 56m 13s 416ms, eta: 58s 281ms\n",
      "\u001b[32m2024-07-23T14:35:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1970/2000, train/hateful_memes/cross_entropy: 0.1471, train/hateful_memes/cross_entropy/avg: 0.3502, train/total_loss: 0.1471, train/total_loss/avg: 0.3502, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 19, num_updates: 1970, iterations: 1970, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 626ms, time_since_start: 56m 26s 043ms, eta: 40s 531ms\n",
      "\u001b[32m2024-07-23T14:35:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1980/2000, train/hateful_memes/cross_entropy: 0.1471, train/hateful_memes/cross_entropy/avg: 0.3491, train/total_loss: 0.1471, train/total_loss/avg: 0.3491, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 19, num_updates: 1980, iterations: 1980, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 692ms, time_since_start: 56m 39s 735ms, eta: 29s 301ms\n",
      "\u001b[32m2024-07-23T14:35:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1990/2000, train/hateful_memes/cross_entropy: 0.1357, train/hateful_memes/cross_entropy/avg: 0.3478, train/total_loss: 0.1357, train/total_loss/avg: 0.3478, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 19, num_updates: 1990, iterations: 1990, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 690ms, time_since_start: 56m 52s 426ms, eta: 13s 578ms\n",
      "\u001b[32m2024-07-23T14:35:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-23T14:35:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:35:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:36:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:36:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, train/hateful_memes/cross_entropy: 0.1357, train/hateful_memes/cross_entropy/avg: 0.3469, train/total_loss: 0.1357, train/total_loss/avg: 0.3469, max mem: 13427.0, experiment: hateful_memes_lr1e5, epoch: 19, num_updates: 2000, iterations: 2000, max_updates: 2000, lr: 0., ups: 0.37, time: 27s 460ms, time_since_start: 57m 19s 887ms, eta: 0ms\n",
      "\u001b[32m2024-07-23T14:36:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T14:36:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T14:36:10 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:36:10 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:36:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T14:36:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T14:36:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T14:36:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, val/hateful_memes/cross_entropy: 1.8071, val/total_loss: 1.8071, val/hateful_memes/accuracy: 0.5740, val/hateful_memes/binary_f1: 0.4067, val/hateful_memes/roc_auc: 0.6748, num_updates: 2000, epoch: 19, iterations: 2000, max_updates: 2000, val_time: 21s 920ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.695792\n",
      "\u001b[32m2024-07-23T14:36:23 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2024-07-23T14:36:23 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2024-07-23T14:36:23 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2024-07-23T14:36:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2024-07-23T14:36:32 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 900\n",
      "\u001b[32m2024-07-23T14:36:32 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 900\n",
      "\u001b[32m2024-07-23T14:36:32 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 9\n",
      "\u001b[32m2024-07-23T14:36:36 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2024-07-23T14:36:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:08<00:00,  1.19s/it]\n",
      "\u001b[32m2024-07-23T14:36:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T14:36:45 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T14:36:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, val/hateful_memes/cross_entropy: 1.0143, val/total_loss: 1.0143, val/hateful_memes/accuracy: 0.5760, val/hateful_memes/binary_f1: 0.3765, val/hateful_memes/roc_auc: 0.6958\n",
      "\u001b[32m2024-07-23T14:36:45 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 58m 04s 057ms\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.139 MB of 0.139 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/cross_entropy ███▇▇▇▆▆▆▅▆▅▆▅▅▅▄▅▄▃▄▃▅▂▂▃▃▃▂▃▂▂▁▃▂▁▃▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/learning_rate ▁▁████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/total_loss ███▇▇▇▆▆▆▅▆▅▆▅▅▅▄▅▄▃▄▃▅▂▂▃▃▃▂▃▂▂▁▃▂▁▃▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val/hateful_memes/accuracy ▁▂▄▄▅▄▇▇▅█▆▄▅▄▅▆▅▆▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val/hateful_memes/binary_f1 ▁▃▅▅▆▅▇█▆█▆▅▆▅▆▆▆▆▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/cross_entropy ▁▁▁▂▂▃▁▂▃▂▃▅▃▆▅▅█▅▃▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/hateful_memes/roc_auc ▁▃▅▆▇▇█▇███▇█▇▇█▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val/total_loss ▁▁▁▂▂▃▁▂▃▂▃▅▃▆▅▅█▅▃▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/cross_entropy 0.18034\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/total_loss 0.18034\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step 900\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val/hateful_memes/accuracy 0.576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val/hateful_memes/binary_f1 0.37647\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/cross_entropy 0.89592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/hateful_memes/roc_auc 0.69579\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val/total_loss 0.89592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhateful_memes_lr1e5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/1dcj8yho\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240723_133842-1dcj8yho/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "Exception ignored in: <function WandbLogger.__del__ at 0x7f36f9f93f70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf/utils/logger.py\", line 457, in __del__\n",
      "    self._wandb.finish()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 4262, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 390, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2100, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2108, in _finish\n",
      "    tel.feature.finish = True\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
      "    self._run._telemetry_callback(self._obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 798, in _telemetry_callback\n",
      "    self._telemetry_flush()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 809, in _telemetry_flush\n",
      "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 101, in _publish_telemetry\n",
      "    self._publish(rec)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_roberta/defaults.yaml \\\n",
    "model=visual_roberta \\\n",
    "dataset=hateful_memes \\\n",
    "training.log_interval=10 \\\n",
    "training.seed=2024 \\\n",
    "training.batch_size=80 \\\n",
    "training.evaluation_interval=100 \\\n",
    "training.experiment_name=hateful_memes_lr1e5 \\\n",
    "training.max_updates=2000 \\\n",
    "optimizer.params.lr=1e-5 \\\n",
    "training.fp16=True \\\n",
    "scheduler.params.num_warmup_steps=200 \\\n",
    "checkpoint.max_to_keep=1 \\\n",
    "training.wandb.enabled=True \\\n",
    "run_type=train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_roberta/defaults.yaml\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_roberta\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 10\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option training.seed to 2024\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 80\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 100\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option training.experiment_name to hateful_memes_lr8e5\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 2000\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 8e-5\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option training.fp16 to True\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 200\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option training.wandb.enabled to True\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_roberta/defaults.yaml', 'model=visual_roberta', 'dataset=hateful_memes', 'training.log_interval=10', 'training.seed=2024', 'training.batch_size=80', 'training.evaluation_interval=100', 'training.experiment_name=hateful_memes_lr8e5', 'training.max_updates=2000', 'optimizer.params.lr=8e-5', 'training.fp16=True', 'scheduler.params.num_warmup_steps=200', 'checkpoint.max_to_keep=1', 'training.wandb.enabled=True', 'run_type=train_val'])\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf_cli.run: \u001b[0mTorch version: 1.11.0+cu102\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf_cli.run: \u001b[0mUsing seed 2024\n",
      "\u001b[32m2024-07-23T12:33:44 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/zeus/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/zeus/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/zeus/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at /home/zeus/.cache/huggingface/transformers/dfe8f1ad04cb25b61a647e3d13620f9bf0a0f51d277897b232a5735297134132.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\u001b[32m2024-07-23T12:33:45 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T12:33:45 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T12:33:45 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-23T12:33:45 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2024-07-23T12:33:45 | torch.distributed.nn.jit.instantiator: \u001b[0mCreated a temporary directory at /tmp/tmpdl16qprs\n",
      "\u001b[32m2024-07-23T12:33:45 | torch.distributed.nn.jit.instantiator: \u001b[0mWriting /tmp/tmpdl16qprs/_remote_module_non_sriptable.py\n",
      "HI BRO {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': ['cross_entropy'], 'model': 'visual_roberta'}\n",
      "SELFIE {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': ['cross_entropy'], 'model': 'visual_roberta'}\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "SELF ROBERT CONFIG RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "roberta-base\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/zeus/.cache/torch/mmf/distributed_-1/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VisualRobertaBase: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing VisualRobertaBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualRobertaBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualRobertaBase were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.token_type_embeddings_visual.weight', 'roberta.embeddings.projection.weight', 'roberta.embeddings.projection.bias', 'roberta.embeddings.position_ids', 'roberta.embeddings.position_embeddings_visual.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-07-23T12:33:51 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2024-07-23T12:33:51 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikrant17\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/wandb/run-20240723_123352-wir6cua1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhateful_memes_lr8e5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/wir6cua1\u001b[0m\n",
      "\u001b[32m2024-07-23T12:33:55 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2024-07-23T12:33:55 | mmf.trainers.mmf_trainer: \u001b[0mVisualRoberta(\n",
      "  (model): VisualRobertaForClassification(\n",
      "    (roberta): VisualRobertaBase(\n",
      "      (embeddings): RobertaVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(1, 768)\n",
      "        (position_embeddings_visual): Embedding(514, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2024-07-23T12:33:55 | mmf.utils.general: \u001b[0mTotal Parameters: 127208450. Trained Parameters: 127208450\n",
      "\u001b[32m2024-07-23T12:33:55 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2024-07-23T12:34:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10/2000, train/hateful_memes/cross_entropy: 0.8099, train/hateful_memes/cross_entropy/avg: 0.8099, train/total_loss: 0.8099, train/total_loss/avg: 0.8099, max mem: 13359.0, experiment: hateful_memes_lr8e5, epoch: 1, num_updates: 10, iterations: 10, max_updates: 2000, lr: 0., ups: 0.40, time: 25s 924ms, time_since_start: 30s 116ms, eta: 01h 32m 112ms\n",
      "\u001b[32m2024-07-23T12:34:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20/2000, train/hateful_memes/cross_entropy: 0.6583, train/hateful_memes/cross_entropy/avg: 0.7341, train/total_loss: 0.6583, train/total_loss/avg: 0.7341, max mem: 13359.0, experiment: hateful_memes_lr8e5, epoch: 1, num_updates: 20, iterations: 20, max_updates: 2000, lr: 0.00001, ups: 0.67, time: 15s 861ms, time_since_start: 45s 978ms, eta: 56m 362ms\n",
      "\u001b[32m2024-07-23T12:34:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 30/2000, train/hateful_memes/cross_entropy: 0.6903, train/hateful_memes/cross_entropy/avg: 0.7195, train/total_loss: 0.6903, train/total_loss/avg: 0.7195, max mem: 13359.0, experiment: hateful_memes_lr8e5, epoch: 1, num_updates: 30, iterations: 30, max_updates: 2000, lr: 0.00001, ups: 0.50, time: 20s 937ms, time_since_start: 01m 06s 915ms, eta: 01h 13m 33s 325ms\n",
      "\u001b[32m2024-07-23T12:35:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 40/2000, train/hateful_memes/cross_entropy: 0.6690, train/hateful_memes/cross_entropy/avg: 0.7069, train/total_loss: 0.6690, train/total_loss/avg: 0.7069, max mem: 13359.0, experiment: hateful_memes_lr8e5, epoch: 1, num_updates: 40, iterations: 40, max_updates: 2000, lr: 0.00002, ups: 0.56, time: 18s 384ms, time_since_start: 01m 25s 299ms, eta: 01h 04m 15s 526ms\n",
      "\u001b[32m2024-07-23T12:35:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/2000, train/hateful_memes/cross_entropy: 0.6798, train/hateful_memes/cross_entropy/avg: 0.7015, train/total_loss: 0.6798, train/total_loss/avg: 0.7015, max mem: 13359.0, experiment: hateful_memes_lr8e5, epoch: 1, num_updates: 50, iterations: 50, max_updates: 2000, lr: 0.00002, ups: 0.56, time: 18s 886ms, time_since_start: 01m 44s 185ms, eta: 01h 05m 40s 632ms\n",
      "\u001b[32m2024-07-23T12:35:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 60/2000, train/hateful_memes/cross_entropy: 0.6690, train/hateful_memes/cross_entropy/avg: 0.6849, train/total_loss: 0.6690, train/total_loss/avg: 0.6849, max mem: 13359.0, experiment: hateful_memes_lr8e5, epoch: 1, num_updates: 60, iterations: 60, max_updates: 2000, lr: 0.00002, ups: 0.43, time: 23s 190ms, time_since_start: 02m 07s 376ms, eta: 01h 20m 13s 939ms\n",
      "\u001b[32m2024-07-23T12:36:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 70/2000, train/hateful_memes/cross_entropy: 0.6798, train/hateful_memes/cross_entropy/avg: 0.6855, train/total_loss: 0.6798, train/total_loss/avg: 0.6855, max mem: 13359.0, experiment: hateful_memes_lr8e5, epoch: 1, num_updates: 70, iterations: 70, max_updates: 2000, lr: 0.00003, ups: 0.62, time: 16s 878ms, time_since_start: 02m 24s 255ms, eta: 58m 05s 615ms\n",
      "\u001b[32m2024-07-23T12:36:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 80/2000, train/hateful_memes/cross_entropy: 0.6690, train/hateful_memes/cross_entropy/avg: 0.6822, train/total_loss: 0.6690, train/total_loss/avg: 0.6822, max mem: 13359.0, experiment: hateful_memes_lr8e5, epoch: 1, num_updates: 80, iterations: 80, max_updates: 2000, lr: 0.00003, ups: 0.48, time: 21s 593ms, time_since_start: 02m 45s 848ms, eta: 01h 13m 56s 119ms\n",
      "\u001b[32m2024-07-23T12:36:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 90/2000, train/hateful_memes/cross_entropy: 0.6798, train/hateful_memes/cross_entropy/avg: 0.6828, train/total_loss: 0.6798, train/total_loss/avg: 0.6828, max mem: 13359.0, experiment: hateful_memes_lr8e5, epoch: 1, num_updates: 90, iterations: 90, max_updates: 2000, lr: 0.00004, ups: 0.45, time: 22s 327ms, time_since_start: 03m 08s 175ms, eta: 01h 16m 02s 997ms\n",
      "\u001b[32m2024-07-23T12:37:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, train/hateful_memes/cross_entropy: 0.6690, train/hateful_memes/cross_entropy/avg: 0.6800, train/total_loss: 0.6690, train/total_loss/avg: 0.6800, max mem: 13359.0, experiment: hateful_memes_lr8e5, epoch: 1, num_updates: 100, iterations: 100, max_updates: 2000, lr: 0.00004, ups: 0.43, time: 23s 946ms, time_since_start: 03m 32s 121ms, eta: 01h 21m 08s 264ms\n",
      "\u001b[32m2024-07-23T12:37:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T12:37:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T12:37:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T12:37:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T12:37:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T12:37:35 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T12:37:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T12:37:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T12:37:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, val/hateful_memes/cross_entropy: 0.7607, val/total_loss: 0.7607, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5483, num_updates: 100, epoch: 1, iterations: 100, max_updates: 2000, val_time: 32s 224ms, best_update: 100, best_iteration: 100, best_val/hateful_memes/roc_auc: 0.548272\n",
      "\u001b[32m2024-07-23T12:38:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 110/2000, train/hateful_memes/cross_entropy: 0.6690, train/hateful_memes/cross_entropy/avg: 0.6758, train/total_loss: 0.6690, train/total_loss/avg: 0.6758, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 2, num_updates: 110, iterations: 110, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 008ms, time_since_start: 04m 16s 363ms, eta: 40m 28s 464ms\n",
      "\u001b[32m2024-07-23T12:38:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 120/2000, train/hateful_memes/cross_entropy: 0.6690, train/hateful_memes/cross_entropy/avg: 0.6786, train/total_loss: 0.6690, train/total_loss/avg: 0.6786, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 2, num_updates: 120, iterations: 120, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 728ms, time_since_start: 04m 29s 092ms, eta: 42m 40s 539ms\n",
      "\u001b[32m2024-07-23T12:38:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 130/2000, train/hateful_memes/cross_entropy: 0.6690, train/hateful_memes/cross_entropy/avg: 0.6717, train/total_loss: 0.6690, train/total_loss/avg: 0.6717, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 2, num_updates: 130, iterations: 130, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 570ms, time_since_start: 04m 41s 663ms, eta: 41m 55s 269ms\n",
      "\u001b[32m2024-07-23T12:38:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 140/2000, train/hateful_memes/cross_entropy: 0.6590, train/hateful_memes/cross_entropy/avg: 0.6596, train/total_loss: 0.6590, train/total_loss/avg: 0.6596, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 2, num_updates: 140, iterations: 140, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 552ms, time_since_start: 04m 54s 215ms, eta: 41m 38s 110ms\n",
      "\u001b[32m2024-07-23T12:38:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/2000, train/hateful_memes/cross_entropy: 0.6590, train/hateful_memes/cross_entropy/avg: 0.6573, train/total_loss: 0.6590, train/total_loss/avg: 0.6573, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 2, num_updates: 150, iterations: 150, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 680ms, time_since_start: 05m 06s 896ms, eta: 41m 50s 207ms\n",
      "\u001b[32m2024-07-23T12:39:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 160/2000, train/hateful_memes/cross_entropy: 0.6583, train/hateful_memes/cross_entropy/avg: 0.6507, train/total_loss: 0.6583, train/total_loss/avg: 0.6507, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 2, num_updates: 160, iterations: 160, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 641ms, time_since_start: 05m 19s 538ms, eta: 41m 28s 936ms\n",
      "\u001b[32m2024-07-23T12:39:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 170/2000, train/hateful_memes/cross_entropy: 0.6583, train/hateful_memes/cross_entropy/avg: 0.6493, train/total_loss: 0.6583, train/total_loss/avg: 0.6493, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 2, num_updates: 170, iterations: 170, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 593ms, time_since_start: 05m 32s 132ms, eta: 41m 06s 011ms\n",
      "\u001b[32m2024-07-23T12:39:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 180/2000, train/hateful_memes/cross_entropy: 0.6544, train/hateful_memes/cross_entropy/avg: 0.6483, train/total_loss: 0.6544, train/total_loss/avg: 0.6483, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 2, num_updates: 180, iterations: 180, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 537ms, time_since_start: 05m 44s 669ms, eta: 40m 41s 562ms\n",
      "\u001b[32m2024-07-23T12:39:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 190/2000, train/hateful_memes/cross_entropy: 0.6544, train/hateful_memes/cross_entropy/avg: 0.6485, train/total_loss: 0.6544, train/total_loss/avg: 0.6485, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 2, num_updates: 190, iterations: 190, max_updates: 2000, lr: 0.00008, ups: 0.83, time: 12s 533ms, time_since_start: 05m 57s 203ms, eta: 40m 27s 353ms\n",
      "\u001b[32m2024-07-23T12:40:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, train/hateful_memes/cross_entropy: 0.6544, train/hateful_memes/cross_entropy/avg: 0.6496, train/total_loss: 0.6544, train/total_loss/avg: 0.6496, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 2, num_updates: 200, iterations: 200, max_updates: 2000, lr: 0.00008, ups: 0.83, time: 12s 657ms, time_since_start: 06m 09s 860ms, eta: 40m 37s 823ms\n",
      "\u001b[32m2024-07-23T12:40:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T12:40:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T12:40:09 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T12:40:09 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T12:40:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T12:40:12 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T12:40:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T12:40:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T12:40:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, val/hateful_memes/cross_entropy: 0.7041, val/total_loss: 0.7041, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5491, num_updates: 200, epoch: 2, iterations: 200, max_updates: 2000, val_time: 26s 084ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.549068\n",
      "\u001b[32m2024-07-23T12:40:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 210/2000, train/hateful_memes/cross_entropy: 0.6544, train/hateful_memes/cross_entropy/avg: 0.6501, train/total_loss: 0.6544, train/total_loss/avg: 0.6501, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 2, num_updates: 210, iterations: 210, max_updates: 2000, lr: 0.00008, ups: 0.83, time: 12s 922ms, time_since_start: 06m 48s 876ms, eta: 41m 15s 103ms\n",
      "\u001b[32m2024-07-23T12:40:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 220/2000, train/hateful_memes/cross_entropy: 0.6544, train/hateful_memes/cross_entropy/avg: 0.6509, train/total_loss: 0.6544, train/total_loss/avg: 0.6509, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 3, num_updates: 220, iterations: 220, max_updates: 2000, lr: 0.00008, ups: 0.91, time: 11s 756ms, time_since_start: 07m 632ms, eta: 37m 19s 112ms\n",
      "\u001b[32m2024-07-23T12:41:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 230/2000, train/hateful_memes/cross_entropy: 0.6544, train/hateful_memes/cross_entropy/avg: 0.6523, train/total_loss: 0.6544, train/total_loss/avg: 0.6523, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 3, num_updates: 230, iterations: 230, max_updates: 2000, lr: 0.00008, ups: 0.83, time: 12s 491ms, time_since_start: 07m 13s 124ms, eta: 39m 25s 759ms\n",
      "\u001b[32m2024-07-23T12:41:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 240/2000, train/hateful_memes/cross_entropy: 0.6544, train/hateful_memes/cross_entropy/avg: 0.6539, train/total_loss: 0.6544, train/total_loss/avg: 0.6539, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 3, num_updates: 240, iterations: 240, max_updates: 2000, lr: 0.00008, ups: 0.83, time: 12s 760ms, time_since_start: 07m 25s 884ms, eta: 40m 02s 979ms\n",
      "\u001b[32m2024-07-23T12:41:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/2000, train/hateful_memes/cross_entropy: 0.6544, train/hateful_memes/cross_entropy/avg: 0.6542, train/total_loss: 0.6544, train/total_loss/avg: 0.6542, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 3, num_updates: 250, iterations: 250, max_updates: 2000, lr: 0.00008, ups: 0.83, time: 12s 376ms, time_since_start: 07m 38s 261ms, eta: 38m 37s 582ms\n",
      "\u001b[32m2024-07-23T12:41:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 260/2000, train/hateful_memes/cross_entropy: 0.6590, train/hateful_memes/cross_entropy/avg: 0.6546, train/total_loss: 0.6590, train/total_loss/avg: 0.6546, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 3, num_updates: 260, iterations: 260, max_updates: 2000, lr: 0.00008, ups: 0.83, time: 12s 771ms, time_since_start: 07m 51s 032ms, eta: 39m 37s 740ms\n",
      "\u001b[32m2024-07-23T12:41:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 270/2000, train/hateful_memes/cross_entropy: 0.6544, train/hateful_memes/cross_entropy/avg: 0.6538, train/total_loss: 0.6544, train/total_loss/avg: 0.6538, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 3, num_updates: 270, iterations: 270, max_updates: 2000, lr: 0.00008, ups: 0.83, time: 12s 434ms, time_since_start: 08m 03s 466ms, eta: 38m 21s 773ms\n",
      "\u001b[32m2024-07-23T12:42:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 280/2000, train/hateful_memes/cross_entropy: 0.6514, train/hateful_memes/cross_entropy/avg: 0.6537, train/total_loss: 0.6514, train/total_loss/avg: 0.6537, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 3, num_updates: 280, iterations: 280, max_updates: 2000, lr: 0.00008, ups: 0.83, time: 12s 910ms, time_since_start: 08m 16s 377ms, eta: 39m 36s 063ms\n",
      "\u001b[32m2024-07-23T12:42:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 290/2000, train/hateful_memes/cross_entropy: 0.6514, train/hateful_memes/cross_entropy/avg: 0.6538, train/total_loss: 0.6514, train/total_loss/avg: 0.6538, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 3, num_updates: 290, iterations: 290, max_updates: 2000, lr: 0.00008, ups: 0.83, time: 12s 468ms, time_since_start: 08m 28s 845ms, eta: 38m 01s 288ms\n",
      "\u001b[32m2024-07-23T12:42:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, train/hateful_memes/cross_entropy: 0.6509, train/hateful_memes/cross_entropy/avg: 0.6529, train/total_loss: 0.6509, train/total_loss/avg: 0.6529, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 3, num_updates: 300, iterations: 300, max_updates: 2000, lr: 0.00008, ups: 0.83, time: 12s 908ms, time_since_start: 08m 41s 754ms, eta: 39m 08s 091ms\n",
      "\u001b[32m2024-07-23T12:42:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T12:42:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T12:42:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T12:42:40 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T12:42:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T12:42:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T12:42:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T12:42:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, val/hateful_memes/cross_entropy: 0.7102, val/total_loss: 0.7102, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5034, num_updates: 300, epoch: 3, iterations: 300, max_updates: 2000, val_time: 22s 482ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.549068\n",
      "\u001b[32m2024-07-23T12:43:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 310/2000, train/hateful_memes/cross_entropy: 0.6509, train/hateful_memes/cross_entropy/avg: 0.6521, train/total_loss: 0.6509, train/total_loss/avg: 0.6521, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 3, num_updates: 310, iterations: 310, max_updates: 2000, lr: 0.00008, ups: 0.77, time: 13s 319ms, time_since_start: 09m 17s 564ms, eta: 40m 08s 646ms\n",
      "\u001b[32m2024-07-23T12:43:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 320/2000, train/hateful_memes/cross_entropy: 0.6509, train/hateful_memes/cross_entropy/avg: 0.6525, train/total_loss: 0.6509, train/total_loss/avg: 0.6525, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 3, num_updates: 320, iterations: 320, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 175ms, time_since_start: 09m 29s 740ms, eta: 36m 28s 619ms\n",
      "\u001b[32m2024-07-23T12:43:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 330/2000, train/hateful_memes/cross_entropy: 0.6514, train/hateful_memes/cross_entropy/avg: 0.6529, train/total_loss: 0.6514, train/total_loss/avg: 0.6529, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 4, num_updates: 330, iterations: 330, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 819ms, time_since_start: 09m 42s 560ms, eta: 38m 10s 805ms\n",
      "\u001b[32m2024-07-23T12:43:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 340/2000, train/hateful_memes/cross_entropy: 0.6514, train/hateful_memes/cross_entropy/avg: 0.6522, train/total_loss: 0.6514, train/total_loss/avg: 0.6522, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 4, num_updates: 340, iterations: 340, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 282ms, time_since_start: 09m 54s 843ms, eta: 36m 21s 704ms\n",
      "\u001b[32m2024-07-23T12:43:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/2000, train/hateful_memes/cross_entropy: 0.6514, train/hateful_memes/cross_entropy/avg: 0.6510, train/total_loss: 0.6514, train/total_loss/avg: 0.6510, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 4, num_updates: 350, iterations: 350, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 133ms, time_since_start: 10m 07s 976ms, eta: 38m 38s 720ms\n",
      "\u001b[32m2024-07-23T12:44:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 360/2000, train/hateful_memes/cross_entropy: 0.6556, train/hateful_memes/cross_entropy/avg: 0.6519, train/total_loss: 0.6556, train/total_loss/avg: 0.6519, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 4, num_updates: 360, iterations: 360, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 288ms, time_since_start: 10m 20s 264ms, eta: 35m 56s 318ms\n",
      "\u001b[32m2024-07-23T12:44:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 370/2000, train/hateful_memes/cross_entropy: 0.6556, train/hateful_memes/cross_entropy/avg: 0.6506, train/total_loss: 0.6556, train/total_loss/avg: 0.6506, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 4, num_updates: 370, iterations: 370, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 045ms, time_since_start: 10m 33s 310ms, eta: 37m 55s 249ms\n",
      "\u001b[32m2024-07-23T12:44:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 380/2000, train/hateful_memes/cross_entropy: 0.6590, train/hateful_memes/cross_entropy/avg: 0.6509, train/total_loss: 0.6590, train/total_loss/avg: 0.6509, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 4, num_updates: 380, iterations: 380, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 295ms, time_since_start: 10m 45s 605ms, eta: 35m 31s 372ms\n",
      "\u001b[32m2024-07-23T12:44:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 390/2000, train/hateful_memes/cross_entropy: 0.6629, train/hateful_memes/cross_entropy/avg: 0.6527, train/total_loss: 0.6629, train/total_loss/avg: 0.6527, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 4, num_updates: 390, iterations: 390, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 170ms, time_since_start: 10m 58s 776ms, eta: 37m 48s 903ms\n",
      "\u001b[32m2024-07-23T12:45:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, train/hateful_memes/cross_entropy: 0.6629, train/hateful_memes/cross_entropy/avg: 0.6537, train/total_loss: 0.6629, train/total_loss/avg: 0.6537, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 4, num_updates: 400, iterations: 400, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 174ms, time_since_start: 11m 10s 951ms, eta: 34m 44s 273ms\n",
      "\u001b[32m2024-07-23T12:45:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T12:45:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T12:45:10 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T12:45:10 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T12:45:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T12:45:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T12:45:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T12:45:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, val/hateful_memes/cross_entropy: 0.6942, val/total_loss: 0.6942, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5100, num_updates: 400, epoch: 4, iterations: 400, max_updates: 2000, val_time: 20s 984ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.549068\n",
      "\u001b[32m2024-07-23T12:45:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 410/2000, train/hateful_memes/cross_entropy: 0.6629, train/hateful_memes/cross_entropy/avg: 0.6538, train/total_loss: 0.6629, train/total_loss/avg: 0.6538, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 4, num_updates: 410, iterations: 410, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 343ms, time_since_start: 11m 45s 287ms, eta: 37m 50s 168ms\n",
      "\u001b[32m2024-07-23T12:45:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 420/2000, train/hateful_memes/cross_entropy: 0.6587, train/hateful_memes/cross_entropy/avg: 0.6538, train/total_loss: 0.6587, train/total_loss/avg: 0.6538, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 4, num_updates: 420, iterations: 420, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 367ms, time_since_start: 11m 57s 654ms, eta: 34m 50s 793ms\n",
      "\u001b[32m2024-07-23T12:46:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 430/2000, train/hateful_memes/cross_entropy: 0.6556, train/hateful_memes/cross_entropy/avg: 0.6526, train/total_loss: 0.6556, train/total_loss/avg: 0.6526, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 5, num_updates: 430, iterations: 430, max_updates: 2000, lr: 0.00007, ups: 0.91, time: 11s 953ms, time_since_start: 12m 09s 607ms, eta: 33m 28s 016ms\n",
      "\u001b[32m2024-07-23T12:46:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 440/2000, train/hateful_memes/cross_entropy: 0.6537, train/hateful_memes/cross_entropy/avg: 0.6526, train/total_loss: 0.6537, train/total_loss/avg: 0.6526, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 5, num_updates: 440, iterations: 440, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 905ms, time_since_start: 12m 22s 513ms, eta: 35m 54s 176ms\n",
      "\u001b[32m2024-07-23T12:46:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/2000, train/hateful_memes/cross_entropy: 0.6509, train/hateful_memes/cross_entropy/avg: 0.6519, train/total_loss: 0.6509, train/total_loss/avg: 0.6519, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 5, num_updates: 450, iterations: 450, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 479ms, time_since_start: 12m 35s 992ms, eta: 37m 15s 582ms\n",
      "\u001b[32m2024-07-23T12:46:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 460/2000, train/hateful_memes/cross_entropy: 0.6501, train/hateful_memes/cross_entropy/avg: 0.6512, train/total_loss: 0.6501, train/total_loss/avg: 0.6512, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 5, num_updates: 460, iterations: 460, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 869ms, time_since_start: 12m 48s 862ms, eta: 35m 20s 679ms\n",
      "\u001b[32m2024-07-23T12:46:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 470/2000, train/hateful_memes/cross_entropy: 0.6509, train/hateful_memes/cross_entropy/avg: 0.6513, train/total_loss: 0.6509, train/total_loss/avg: 0.6513, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 5, num_updates: 470, iterations: 470, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 538ms, time_since_start: 13m 02s 401ms, eta: 36m 56s 449ms\n",
      "\u001b[32m2024-07-23T12:47:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 480/2000, train/hateful_memes/cross_entropy: 0.6537, train/hateful_memes/cross_entropy/avg: 0.6517, train/total_loss: 0.6537, train/total_loss/avg: 0.6517, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 5, num_updates: 480, iterations: 480, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 039ms, time_since_start: 13m 15s 440ms, eta: 35m 20s 697ms\n",
      "\u001b[32m2024-07-23T12:47:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 490/2000, train/hateful_memes/cross_entropy: 0.6501, train/hateful_memes/cross_entropy/avg: 0.6516, train/total_loss: 0.6501, train/total_loss/avg: 0.6516, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 5, num_updates: 490, iterations: 490, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 654ms, time_since_start: 13m 29s 094ms, eta: 36m 46s 098ms\n",
      "\u001b[32m2024-07-23T12:47:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, train/hateful_memes/cross_entropy: 0.6501, train/hateful_memes/cross_entropy/avg: 0.6512, train/total_loss: 0.6501, train/total_loss/avg: 0.6512, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 5, num_updates: 500, iterations: 500, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 002ms, time_since_start: 13m 42s 097ms, eta: 34m 46s 938ms\n",
      "\u001b[32m2024-07-23T12:47:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T12:47:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T12:47:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T12:47:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T12:47:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T12:47:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T12:47:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T12:47:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, val/hateful_memes/cross_entropy: 0.7268, val/total_loss: 0.7268, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5146, num_updates: 500, epoch: 5, iterations: 500, max_updates: 2000, val_time: 23s 006ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.549068\n",
      "\u001b[32m2024-07-23T12:48:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 510/2000, train/hateful_memes/cross_entropy: 0.6537, train/hateful_memes/cross_entropy/avg: 0.6530, train/total_loss: 0.6537, train/total_loss/avg: 0.6530, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 5, num_updates: 510, iterations: 510, max_updates: 2000, lr: 0.00007, ups: 0.71, time: 14s 015ms, time_since_start: 14m 19s 127ms, eta: 37m 14s 497ms\n",
      "\u001b[32m2024-07-23T12:48:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 520/2000, train/hateful_memes/cross_entropy: 0.6537, train/hateful_memes/cross_entropy/avg: 0.6532, train/total_loss: 0.6537, train/total_loss/avg: 0.6532, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 5, num_updates: 520, iterations: 520, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 046ms, time_since_start: 14m 32s 174ms, eta: 34m 26s 110ms\n",
      "\u001b[32m2024-07-23T12:48:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 530/2000, train/hateful_memes/cross_entropy: 0.6501, train/hateful_memes/cross_entropy/avg: 0.6531, train/total_loss: 0.6501, train/total_loss/avg: 0.6531, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 5, num_updates: 530, iterations: 530, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 418ms, time_since_start: 14m 45s 592ms, eta: 35m 10s 592ms\n",
      "\u001b[32m2024-07-23T12:48:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 540/2000, train/hateful_memes/cross_entropy: 0.6537, train/hateful_memes/cross_entropy/avg: 0.6533, train/total_loss: 0.6537, train/total_loss/avg: 0.6533, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 6, num_updates: 540, iterations: 540, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 275ms, time_since_start: 14m 57s 868ms, eta: 31m 57s 639ms\n",
      "\u001b[32m2024-07-23T12:49:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/2000, train/hateful_memes/cross_entropy: 0.6537, train/hateful_memes/cross_entropy/avg: 0.6531, train/total_loss: 0.6537, train/total_loss/avg: 0.6531, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 6, num_updates: 550, iterations: 550, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 788ms, time_since_start: 15m 10s 656ms, eta: 33m 04s 160ms\n",
      "\u001b[32m2024-07-23T12:49:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 560/2000, train/hateful_memes/cross_entropy: 0.6501, train/hateful_memes/cross_entropy/avg: 0.6530, train/total_loss: 0.6501, train/total_loss/avg: 0.6530, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 6, num_updates: 560, iterations: 560, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 689ms, time_since_start: 15m 23s 346ms, eta: 32m 35s 209ms\n",
      "\u001b[32m2024-07-23T12:49:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 570/2000, train/hateful_memes/cross_entropy: 0.6537, train/hateful_memes/cross_entropy/avg: 0.6536, train/total_loss: 0.6537, train/total_loss/avg: 0.6536, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 6, num_updates: 570, iterations: 570, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 642ms, time_since_start: 15m 35s 988ms, eta: 32m 14s 364ms\n",
      "\u001b[32m2024-07-23T12:49:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 580/2000, train/hateful_memes/cross_entropy: 0.6537, train/hateful_memes/cross_entropy/avg: 0.6540, train/total_loss: 0.6537, train/total_loss/avg: 0.6540, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 6, num_updates: 580, iterations: 580, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 705ms, time_since_start: 15m 48s 694ms, eta: 32m 10s 507ms\n",
      "\u001b[32m2024-07-23T12:49:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 590/2000, train/hateful_memes/cross_entropy: 0.6501, train/hateful_memes/cross_entropy/avg: 0.6537, train/total_loss: 0.6501, train/total_loss/avg: 0.6537, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 6, num_updates: 590, iterations: 590, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 700ms, time_since_start: 16m 01s 394ms, eta: 31m 56s 118ms\n",
      "\u001b[32m2024-07-23T12:50:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, train/hateful_memes/cross_entropy: 0.6500, train/hateful_memes/cross_entropy/avg: 0.6536, train/total_loss: 0.6500, train/total_loss/avg: 0.6536, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 6, num_updates: 600, iterations: 600, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 611ms, time_since_start: 16m 14s 006ms, eta: 31m 29s 211ms\n",
      "\u001b[32m2024-07-23T12:50:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T12:50:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T12:50:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T12:50:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T12:50:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T12:50:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T12:50:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T12:50:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, val/hateful_memes/cross_entropy: 0.7455, val/total_loss: 0.7455, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5221, num_updates: 600, epoch: 6, iterations: 600, max_updates: 2000, val_time: 21s 191ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.549068\n",
      "\u001b[32m2024-07-23T12:50:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 610/2000, train/hateful_memes/cross_entropy: 0.6500, train/hateful_memes/cross_entropy/avg: 0.6538, train/total_loss: 0.6500, train/total_loss/avg: 0.6538, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 6, num_updates: 610, iterations: 610, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 388ms, time_since_start: 16m 48s 593ms, eta: 33m 11s 261ms\n",
      "\u001b[32m2024-07-23T12:50:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 620/2000, train/hateful_memes/cross_entropy: 0.6500, train/hateful_memes/cross_entropy/avg: 0.6544, train/total_loss: 0.6500, train/total_loss/avg: 0.6544, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 6, num_updates: 620, iterations: 620, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 762ms, time_since_start: 17m 01s 356ms, eta: 31m 24s 577ms\n",
      "\u001b[32m2024-07-23T12:51:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 630/2000, train/hateful_memes/cross_entropy: 0.6500, train/hateful_memes/cross_entropy/avg: 0.6537, train/total_loss: 0.6500, train/total_loss/avg: 0.6537, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 6, num_updates: 630, iterations: 630, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 803ms, time_since_start: 17m 14s 160ms, eta: 31m 16s 822ms\n",
      "\u001b[32m2024-07-23T12:51:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 640/2000, train/hateful_memes/cross_entropy: 0.6500, train/hateful_memes/cross_entropy/avg: 0.6545, train/total_loss: 0.6500, train/total_loss/avg: 0.6545, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 6, num_updates: 640, iterations: 640, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 200ms, time_since_start: 17m 26s 360ms, eta: 29m 35s 353ms\n",
      "\u001b[32m2024-07-23T12:51:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/2000, train/hateful_memes/cross_entropy: 0.6580, train/hateful_memes/cross_entropy/avg: 0.6550, train/total_loss: 0.6580, train/total_loss/avg: 0.6550, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 7, num_updates: 650, iterations: 650, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 006ms, time_since_start: 17m 38s 367ms, eta: 28m 54s 380ms\n",
      "\u001b[32m2024-07-23T12:51:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 660/2000, train/hateful_memes/cross_entropy: 0.6626, train/hateful_memes/cross_entropy/avg: 0.6551, train/total_loss: 0.6626, train/total_loss/avg: 0.6551, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 7, num_updates: 660, iterations: 660, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 304ms, time_since_start: 17m 51s 671ms, eta: 31m 47s 665ms\n",
      "\u001b[32m2024-07-23T12:51:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 670/2000, train/hateful_memes/cross_entropy: 0.6636, train/hateful_memes/cross_entropy/avg: 0.6553, train/total_loss: 0.6636, train/total_loss/avg: 0.6553, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 7, num_updates: 670, iterations: 670, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 448ms, time_since_start: 18m 04s 120ms, eta: 29m 31s 555ms\n",
      "\u001b[32m2024-07-23T12:52:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 680/2000, train/hateful_memes/cross_entropy: 0.6626, train/hateful_memes/cross_entropy/avg: 0.6550, train/total_loss: 0.6626, train/total_loss/avg: 0.6550, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 7, num_updates: 680, iterations: 680, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 434ms, time_since_start: 18m 17s 555ms, eta: 31m 37s 519ms\n",
      "\u001b[32m2024-07-23T12:52:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 690/2000, train/hateful_memes/cross_entropy: 0.6626, train/hateful_memes/cross_entropy/avg: 0.6551, train/total_loss: 0.6626, train/total_loss/avg: 0.6551, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 7, num_updates: 690, iterations: 690, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 426ms, time_since_start: 18m 29s 981ms, eta: 29m 01s 789ms\n",
      "\u001b[32m2024-07-23T12:52:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, train/hateful_memes/cross_entropy: 0.6626, train/hateful_memes/cross_entropy/avg: 0.6552, train/total_loss: 0.6626, train/total_loss/avg: 0.6552, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 7, num_updates: 700, iterations: 700, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 361ms, time_since_start: 18m 43s 342ms, eta: 30m 58s 545ms\n",
      "\u001b[32m2024-07-23T12:52:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T12:52:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T12:52:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T12:52:43 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T12:52:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T12:52:46 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T12:52:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T12:53:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T12:53:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, val/hateful_memes/cross_entropy: 0.7052, val/total_loss: 0.7052, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5498, num_updates: 700, epoch: 7, iterations: 700, max_updates: 2000, val_time: 34s 856ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.549808\n",
      "\u001b[32m2024-07-23T12:53:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 710/2000, train/hateful_memes/cross_entropy: 0.6626, train/hateful_memes/cross_entropy/avg: 0.6563, train/total_loss: 0.6626, train/total_loss/avg: 0.6563, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 7, num_updates: 710, iterations: 710, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 911ms, time_since_start: 19m 31s 118ms, eta: 29m 42s 172ms\n",
      "\u001b[32m2024-07-23T12:53:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 720/2000, train/hateful_memes/cross_entropy: 0.6625, train/hateful_memes/cross_entropy/avg: 0.6560, train/total_loss: 0.6625, train/total_loss/avg: 0.6560, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 7, num_updates: 720, iterations: 720, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 185ms, time_since_start: 19m 44s 304ms, eta: 30m 05s 956ms\n",
      "\u001b[32m2024-07-23T12:53:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 730/2000, train/hateful_memes/cross_entropy: 0.6626, train/hateful_memes/cross_entropy/avg: 0.6561, train/total_loss: 0.6626, train/total_loss/avg: 0.6561, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 7, num_updates: 730, iterations: 730, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 263ms, time_since_start: 19m 56s 567ms, eta: 27m 46s 448ms\n",
      "\u001b[32m2024-07-23T12:54:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 740/2000, train/hateful_memes/cross_entropy: 0.6625, train/hateful_memes/cross_entropy/avg: 0.6559, train/total_loss: 0.6625, train/total_loss/avg: 0.6559, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 7, num_updates: 740, iterations: 740, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 218ms, time_since_start: 20m 09s 786ms, eta: 29m 42s 169ms\n",
      "\u001b[32m2024-07-23T12:54:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/2000, train/hateful_memes/cross_entropy: 0.6626, train/hateful_memes/cross_entropy/avg: 0.6562, train/total_loss: 0.6626, train/total_loss/avg: 0.6562, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 8, num_updates: 750, iterations: 750, max_updates: 2000, lr: 0.00006, ups: 0.91, time: 11s 909ms, time_since_start: 20m 21s 696ms, eta: 26m 32s 954ms\n",
      "\u001b[32m2024-07-23T12:54:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 760/2000, train/hateful_memes/cross_entropy: 0.6660, train/hateful_memes/cross_entropy/avg: 0.6564, train/total_loss: 0.6660, train/total_loss/avg: 0.6564, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 8, num_updates: 760, iterations: 760, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 981ms, time_since_start: 20m 34s 678ms, eta: 28m 42s 385ms\n",
      "\u001b[32m2024-07-23T12:54:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 770/2000, train/hateful_memes/cross_entropy: 0.6626, train/hateful_memes/cross_entropy/avg: 0.6562, train/total_loss: 0.6626, train/total_loss/avg: 0.6562, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 8, num_updates: 770, iterations: 770, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 569ms, time_since_start: 20m 48s 247ms, eta: 29m 45s 840ms\n",
      "\u001b[32m2024-07-23T12:54:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 780/2000, train/hateful_memes/cross_entropy: 0.6625, train/hateful_memes/cross_entropy/avg: 0.6562, train/total_loss: 0.6625, train/total_loss/avg: 0.6562, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 8, num_updates: 780, iterations: 780, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 774ms, time_since_start: 21m 01s 021ms, eta: 27m 47s 610ms\n",
      "\u001b[32m2024-07-23T12:55:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 790/2000, train/hateful_memes/cross_entropy: 0.6625, train/hateful_memes/cross_entropy/avg: 0.6559, train/total_loss: 0.6625, train/total_loss/avg: 0.6559, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 8, num_updates: 790, iterations: 790, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 551ms, time_since_start: 21m 14s 573ms, eta: 29m 14s 506ms\n",
      "\u001b[32m2024-07-23T12:55:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, train/hateful_memes/cross_entropy: 0.6626, train/hateful_memes/cross_entropy/avg: 0.6563, train/total_loss: 0.6626, train/total_loss/avg: 0.6563, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 8, num_updates: 800, iterations: 800, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 062ms, time_since_start: 21m 27s 636ms, eta: 27m 57s 247ms\n",
      "\u001b[32m2024-07-23T12:55:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T12:55:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T12:55:26 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T12:55:26 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T12:55:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T12:55:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T12:55:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T12:55:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, val/hateful_memes/cross_entropy: 0.7440, val/total_loss: 0.7440, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5465, num_updates: 800, epoch: 8, iterations: 800, max_updates: 2000, val_time: 22s 103ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.549808\n",
      "\u001b[32m2024-07-23T12:55:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 810/2000, train/hateful_memes/cross_entropy: 0.6625, train/hateful_memes/cross_entropy/avg: 0.6558, train/total_loss: 0.6625, train/total_loss/avg: 0.6558, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 8, num_updates: 810, iterations: 810, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 770ms, time_since_start: 22m 03s 517ms, eta: 29m 13s 409ms\n",
      "\u001b[32m2024-07-23T12:56:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 820/2000, train/hateful_memes/cross_entropy: 0.6625, train/hateful_memes/cross_entropy/avg: 0.6560, train/total_loss: 0.6625, train/total_loss/avg: 0.6560, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 8, num_updates: 820, iterations: 820, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 062ms, time_since_start: 22m 16s 580ms, eta: 27m 29s 305ms\n",
      "\u001b[32m2024-07-23T12:56:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 830/2000, train/hateful_memes/cross_entropy: 0.6625, train/hateful_memes/cross_entropy/avg: 0.6556, train/total_loss: 0.6625, train/total_loss/avg: 0.6556, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 8, num_updates: 830, iterations: 830, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 478ms, time_since_start: 22m 30s 058ms, eta: 28m 07s 379ms\n",
      "\u001b[32m2024-07-23T12:56:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 840/2000, train/hateful_memes/cross_entropy: 0.6595, train/hateful_memes/cross_entropy/avg: 0.6554, train/total_loss: 0.6595, train/total_loss/avg: 0.6554, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 8, num_updates: 840, iterations: 840, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 983ms, time_since_start: 22m 43s 042ms, eta: 26m 51s 564ms\n",
      "\u001b[32m2024-07-23T12:56:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/2000, train/hateful_memes/cross_entropy: 0.6565, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6565, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 8, num_updates: 850, iterations: 850, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 343ms, time_since_start: 22m 56s 385ms, eta: 27m 21s 882ms\n",
      "\u001b[32m2024-07-23T12:56:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 860/2000, train/hateful_memes/cross_entropy: 0.6510, train/hateful_memes/cross_entropy/avg: 0.6548, train/total_loss: 0.6510, train/total_loss/avg: 0.6548, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 9, num_updates: 860, iterations: 860, max_updates: 2000, lr: 0.00005, ups: 0.91, time: 11s 302ms, time_since_start: 23m 07s 688ms, eta: 22m 58s 659ms\n",
      "\u001b[32m2024-07-23T12:57:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 870/2000, train/hateful_memes/cross_entropy: 0.6510, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6510, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 9, num_updates: 870, iterations: 870, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 894ms, time_since_start: 23m 20s 582ms, eta: 25m 59s 087ms\n",
      "\u001b[32m2024-07-23T12:57:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 880/2000, train/hateful_memes/cross_entropy: 0.6510, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6510, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 9, num_updates: 880, iterations: 880, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 534ms, time_since_start: 23m 33s 117ms, eta: 25m 02s 187ms\n",
      "\u001b[32m2024-07-23T12:57:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 890/2000, train/hateful_memes/cross_entropy: 0.6510, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6510, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 9, num_updates: 890, iterations: 890, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 959ms, time_since_start: 23m 46s 077ms, eta: 25m 39s 248ms\n",
      "\u001b[32m2024-07-23T12:57:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, train/hateful_memes/cross_entropy: 0.6498, train/hateful_memes/cross_entropy/avg: 0.6545, train/total_loss: 0.6498, train/total_loss/avg: 0.6545, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 9, num_updates: 900, iterations: 900, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 651ms, time_since_start: 23m 58s 729ms, eta: 24m 49s 118ms\n",
      "\u001b[32m2024-07-23T12:57:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T12:57:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T12:57:57 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T12:57:57 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T12:57:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T12:58:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T12:58:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T12:58:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, val/hateful_memes/cross_entropy: 0.7323, val/total_loss: 0.7323, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.4930, num_updates: 900, epoch: 9, iterations: 900, max_updates: 2000, val_time: 41s 802ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.549808\n",
      "\u001b[32m2024-07-23T12:58:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 910/2000, train/hateful_memes/cross_entropy: 0.6492, train/hateful_memes/cross_entropy/avg: 0.6545, train/total_loss: 0.6492, train/total_loss/avg: 0.6545, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 9, num_updates: 910, iterations: 910, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 344ms, time_since_start: 24m 53s 886ms, eta: 25m 56s 362ms\n",
      "\u001b[32m2024-07-23T12:58:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 920/2000, train/hateful_memes/cross_entropy: 0.6498, train/hateful_memes/cross_entropy/avg: 0.6545, train/total_loss: 0.6498, train/total_loss/avg: 0.6545, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 9, num_updates: 920, iterations: 920, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 626ms, time_since_start: 25m 06s 512ms, eta: 24m 19s 073ms\n",
      "\u001b[32m2024-07-23T12:59:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 930/2000, train/hateful_memes/cross_entropy: 0.6492, train/hateful_memes/cross_entropy/avg: 0.6542, train/total_loss: 0.6492, train/total_loss/avg: 0.6542, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 9, num_updates: 930, iterations: 930, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 763ms, time_since_start: 25m 19s 276ms, eta: 24m 21s 293ms\n",
      "\u001b[32m2024-07-23T12:59:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 940/2000, train/hateful_memes/cross_entropy: 0.6492, train/hateful_memes/cross_entropy/avg: 0.6540, train/total_loss: 0.6492, train/total_loss/avg: 0.6540, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 9, num_updates: 940, iterations: 940, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 416ms, time_since_start: 25m 31s 692ms, eta: 23m 28s 304ms\n",
      "\u001b[32m2024-07-23T12:59:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/2000, train/hateful_memes/cross_entropy: 0.6423, train/hateful_memes/cross_entropy/avg: 0.6537, train/total_loss: 0.6423, train/total_loss/avg: 0.6537, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 9, num_updates: 950, iterations: 950, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 868ms, time_since_start: 25m 44s 561ms, eta: 24m 05s 822ms\n",
      "\u001b[32m2024-07-23T12:59:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 960/2000, train/hateful_memes/cross_entropy: 0.6423, train/hateful_memes/cross_entropy/avg: 0.6538, train/total_loss: 0.6423, train/total_loss/avg: 0.6538, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 9, num_updates: 960, iterations: 960, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 325ms, time_since_start: 25m 56s 887ms, eta: 22m 51s 577ms\n",
      "\u001b[32m2024-07-23T13:00:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 970/2000, train/hateful_memes/cross_entropy: 0.6410, train/hateful_memes/cross_entropy/avg: 0.6534, train/total_loss: 0.6410, train/total_loss/avg: 0.6534, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 10, num_updates: 970, iterations: 970, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 766ms, time_since_start: 26m 09s 653ms, eta: 23m 27s 002ms\n",
      "\u001b[32m2024-07-23T13:00:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 980/2000, train/hateful_memes/cross_entropy: 0.6336, train/hateful_memes/cross_entropy/avg: 0.6531, train/total_loss: 0.6336, train/total_loss/avg: 0.6531, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 10, num_updates: 980, iterations: 980, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 937ms, time_since_start: 26m 23s 590ms, eta: 25m 21s 091ms\n",
      "\u001b[32m2024-07-23T13:00:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 990/2000, train/hateful_memes/cross_entropy: 0.6336, train/hateful_memes/cross_entropy/avg: 0.6529, train/total_loss: 0.6336, train/total_loss/avg: 0.6529, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 10, num_updates: 990, iterations: 990, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 716ms, time_since_start: 26m 36s 307ms, eta: 22m 54s 267ms\n",
      "\u001b[32m2024-07-23T13:00:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-23T13:00:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:00:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:00:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:00:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, train/hateful_memes/cross_entropy: 0.6336, train/hateful_memes/cross_entropy/avg: 0.6529, train/total_loss: 0.6336, train/total_loss/avg: 0.6529, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 10, num_updates: 1000, iterations: 1000, max_updates: 2000, lr: 0.00004, ups: 0.36, time: 28s 061ms, time_since_start: 27m 04s 368ms, eta: 50m 02s 554ms\n",
      "\u001b[32m2024-07-23T13:00:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:00:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:01:03 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:01:03 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:01:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:01:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:01:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:01:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, val/hateful_memes/cross_entropy: 0.7537, val/total_loss: 0.7537, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5454, num_updates: 1000, epoch: 10, iterations: 1000, max_updates: 2000, val_time: 15s 307ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.549808\n",
      "\u001b[32m2024-07-23T13:01:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1010/2000, train/hateful_memes/cross_entropy: 0.6410, train/hateful_memes/cross_entropy/avg: 0.6531, train/total_loss: 0.6410, train/total_loss/avg: 0.6531, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 10, num_updates: 1010, iterations: 1010, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 324ms, time_since_start: 27m 33s 002ms, eta: 23m 31s 430ms\n",
      "\u001b[32m2024-07-23T13:01:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1020/2000, train/hateful_memes/cross_entropy: 0.6410, train/hateful_memes/cross_entropy/avg: 0.6533, train/total_loss: 0.6410, train/total_loss/avg: 0.6533, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 10, num_updates: 1020, iterations: 1020, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 893ms, time_since_start: 27m 46s 895ms, eta: 24m 16s 827ms\n",
      "\u001b[32m2024-07-23T13:01:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1030/2000, train/hateful_memes/cross_entropy: 0.6492, train/hateful_memes/cross_entropy/avg: 0.6533, train/total_loss: 0.6492, train/total_loss/avg: 0.6533, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 10, num_updates: 1030, iterations: 1030, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 914ms, time_since_start: 27m 59s 810ms, eta: 22m 20s 405ms\n",
      "\u001b[32m2024-07-23T13:02:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1040/2000, train/hateful_memes/cross_entropy: 0.6498, train/hateful_memes/cross_entropy/avg: 0.6537, train/total_loss: 0.6498, train/total_loss/avg: 0.6537, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 10, num_updates: 1040, iterations: 1040, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 904ms, time_since_start: 28m 13s 715ms, eta: 23m 48s 318ms\n",
      "\u001b[32m2024-07-23T13:02:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/2000, train/hateful_memes/cross_entropy: 0.6510, train/hateful_memes/cross_entropy/avg: 0.6538, train/total_loss: 0.6510, train/total_loss/avg: 0.6538, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 10, num_updates: 1050, iterations: 1050, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 684ms, time_since_start: 28m 26s 399ms, eta: 21m 29s 372ms\n",
      "\u001b[32m2024-07-23T13:02:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1060/2000, train/hateful_memes/cross_entropy: 0.6553, train/hateful_memes/cross_entropy/avg: 0.6541, train/total_loss: 0.6553, train/total_loss/avg: 0.6541, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 10, num_updates: 1060, iterations: 1060, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 774ms, time_since_start: 28m 40s 173ms, eta: 23m 05s 405ms\n",
      "\u001b[32m2024-07-23T13:02:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1070/2000, train/hateful_memes/cross_entropy: 0.6542, train/hateful_memes/cross_entropy/avg: 0.6541, train/total_loss: 0.6542, train/total_loss/avg: 0.6541, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 10, num_updates: 1070, iterations: 1070, max_updates: 2000, lr: 0.00004, ups: 0.91, time: 11s 002ms, time_since_start: 28m 51s 175ms, eta: 18m 14s 824ms\n",
      "\u001b[32m2024-07-23T13:02:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1080/2000, train/hateful_memes/cross_entropy: 0.6542, train/hateful_memes/cross_entropy/avg: 0.6539, train/total_loss: 0.6542, train/total_loss/avg: 0.6539, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 11, num_updates: 1080, iterations: 1080, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 698ms, time_since_start: 29m 04s 874ms, eta: 22m 28s 516ms\n",
      "\u001b[32m2024-07-23T13:03:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1090/2000, train/hateful_memes/cross_entropy: 0.6492, train/hateful_memes/cross_entropy/avg: 0.6537, train/total_loss: 0.6492, train/total_loss/avg: 0.6537, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 11, num_updates: 1090, iterations: 1090, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 393ms, time_since_start: 29m 17s 268ms, eta: 20m 06s 765ms\n",
      "\u001b[32m2024-07-23T13:03:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, train/hateful_memes/cross_entropy: 0.6492, train/hateful_memes/cross_entropy/avg: 0.6533, train/total_loss: 0.6492, train/total_loss/avg: 0.6533, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 11, num_updates: 1100, iterations: 1100, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 450ms, time_since_start: 29m 30s 718ms, eta: 21m 35s 252ms\n",
      "\u001b[32m2024-07-23T13:03:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:03:22 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:03:30 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:03:30 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:03:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:03:34 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T13:03:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:04:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:04:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, val/hateful_memes/cross_entropy: 0.7420, val/total_loss: 0.7420, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5536, num_updates: 1100, epoch: 11, iterations: 1100, max_updates: 2000, val_time: 41s 320ms, best_update: 1100, best_iteration: 1100, best_val/hateful_memes/roc_auc: 0.553576\n",
      "\u001b[32m2024-07-23T13:04:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1110/2000, train/hateful_memes/cross_entropy: 0.6542, train/hateful_memes/cross_entropy/avg: 0.6535, train/total_loss: 0.6542, train/total_loss/avg: 0.6535, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 11, num_updates: 1110, iterations: 1110, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 815ms, time_since_start: 30m 24s 861ms, eta: 20m 20s 386ms\n",
      "\u001b[32m2024-07-23T13:04:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1120/2000, train/hateful_memes/cross_entropy: 0.6542, train/hateful_memes/cross_entropy/avg: 0.6538, train/total_loss: 0.6542, train/total_loss/avg: 0.6538, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 11, num_updates: 1120, iterations: 1120, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 440ms, time_since_start: 30m 38s 302ms, eta: 21m 05s 554ms\n",
      "\u001b[32m2024-07-23T13:04:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1130/2000, train/hateful_memes/cross_entropy: 0.6553, train/hateful_memes/cross_entropy/avg: 0.6541, train/total_loss: 0.6553, train/total_loss/avg: 0.6541, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 11, num_updates: 1130, iterations: 1130, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 529ms, time_since_start: 30m 50s 832ms, eta: 19m 26s 401ms\n",
      "\u001b[32m2024-07-23T13:04:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1140/2000, train/hateful_memes/cross_entropy: 0.6568, train/hateful_memes/cross_entropy/avg: 0.6545, train/total_loss: 0.6568, train/total_loss/avg: 0.6545, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 11, num_updates: 1140, iterations: 1140, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 566ms, time_since_start: 31m 04s 398ms, eta: 20m 48s 389ms\n",
      "\u001b[32m2024-07-23T13:05:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/2000, train/hateful_memes/cross_entropy: 0.6599, train/hateful_memes/cross_entropy/avg: 0.6546, train/total_loss: 0.6599, train/total_loss/avg: 0.6546, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 11, num_updates: 1150, iterations: 1150, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 361ms, time_since_start: 31m 16s 759ms, eta: 18m 44s 236ms\n",
      "\u001b[32m2024-07-23T13:05:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1160/2000, train/hateful_memes/cross_entropy: 0.6599, train/hateful_memes/cross_entropy/avg: 0.6545, train/total_loss: 0.6599, train/total_loss/avg: 0.6545, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 11, num_updates: 1160, iterations: 1160, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 398ms, time_since_start: 31m 30s 158ms, eta: 20m 04s 293ms\n",
      "\u001b[32m2024-07-23T13:05:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1170/2000, train/hateful_memes/cross_entropy: 0.6660, train/hateful_memes/cross_entropy/avg: 0.6546, train/total_loss: 0.6660, train/total_loss/avg: 0.6546, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 11, num_updates: 1170, iterations: 1170, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 471ms, time_since_start: 31m 42s 630ms, eta: 18m 27s 612ms\n",
      "\u001b[32m2024-07-23T13:05:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1180/2000, train/hateful_memes/cross_entropy: 0.6692, train/hateful_memes/cross_entropy/avg: 0.6548, train/total_loss: 0.6692, train/total_loss/avg: 0.6548, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 12, num_updates: 1180, iterations: 1180, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 140ms, time_since_start: 31m 54s 771ms, eta: 17m 45s 240ms\n",
      "\u001b[32m2024-07-23T13:05:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1190/2000, train/hateful_memes/cross_entropy: 0.6697, train/hateful_memes/cross_entropy/avg: 0.6551, train/total_loss: 0.6697, train/total_loss/avg: 0.6551, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 12, num_updates: 1190, iterations: 1190, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 389ms, time_since_start: 32m 08s 160ms, eta: 19m 20s 481ms\n",
      "\u001b[32m2024-07-23T13:06:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, train/hateful_memes/cross_entropy: 0.6697, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6697, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 12, num_updates: 1200, iterations: 1200, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 787ms, time_since_start: 32m 20s 947ms, eta: 18m 14s 569ms\n",
      "\u001b[32m2024-07-23T13:06:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:06:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:06:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:06:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:06:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:06:24 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-23T13:06:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:06:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:06:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, val/hateful_memes/cross_entropy: 0.7201, val/total_loss: 0.7201, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5554, num_updates: 1200, epoch: 12, iterations: 1200, max_updates: 2000, val_time: 33s 603ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.555384\n",
      "\u001b[32m2024-07-23T13:07:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1210/2000, train/hateful_memes/cross_entropy: 0.6692, train/hateful_memes/cross_entropy/avg: 0.6550, train/total_loss: 0.6692, train/total_loss/avg: 0.6550, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 12, num_updates: 1210, iterations: 1210, max_updates: 2000, lr: 0.00004, ups: 0.71, time: 14s 131ms, time_since_start: 33m 08s 691ms, eta: 19m 54s 553ms\n",
      "\u001b[32m2024-07-23T13:07:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1220/2000, train/hateful_memes/cross_entropy: 0.6692, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6692, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 12, num_updates: 1220, iterations: 1220, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 745ms, time_since_start: 33m 21s 436ms, eta: 17m 43s 736ms\n",
      "\u001b[32m2024-07-23T13:07:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1230/2000, train/hateful_memes/cross_entropy: 0.6697, train/hateful_memes/cross_entropy/avg: 0.6557, train/total_loss: 0.6697, train/total_loss/avg: 0.6557, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 12, num_updates: 1230, iterations: 1230, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 050ms, time_since_start: 33m 34s 487ms, eta: 17m 55s 246ms\n",
      "\u001b[32m2024-07-23T13:07:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1240/2000, train/hateful_memes/cross_entropy: 0.6697, train/hateful_memes/cross_entropy/avg: 0.6560, train/total_loss: 0.6697, train/total_loss/avg: 0.6560, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 12, num_updates: 1240, iterations: 1240, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 611ms, time_since_start: 33m 47s 099ms, eta: 17m 05s 585ms\n",
      "\u001b[32m2024-07-23T13:07:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/2000, train/hateful_memes/cross_entropy: 0.6708, train/hateful_memes/cross_entropy/avg: 0.6564, train/total_loss: 0.6708, train/total_loss/avg: 0.6564, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 12, num_updates: 1250, iterations: 1250, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 054ms, time_since_start: 34m 153ms, eta: 17m 27s 612ms\n",
      "\u001b[32m2024-07-23T13:08:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1260/2000, train/hateful_memes/cross_entropy: 0.6692, train/hateful_memes/cross_entropy/avg: 0.6560, train/total_loss: 0.6692, train/total_loss/avg: 0.6560, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 12, num_updates: 1260, iterations: 1260, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 762ms, time_since_start: 34m 12s 916ms, eta: 16m 50s 568ms\n",
      "\u001b[32m2024-07-23T13:08:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1270/2000, train/hateful_memes/cross_entropy: 0.6692, train/hateful_memes/cross_entropy/avg: 0.6556, train/total_loss: 0.6692, train/total_loss/avg: 0.6556, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 12, num_updates: 1270, iterations: 1270, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 112ms, time_since_start: 34m 26s 029ms, eta: 17m 04s 236ms\n",
      "\u001b[32m2024-07-23T13:08:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1280/2000, train/hateful_memes/cross_entropy: 0.6692, train/hateful_memes/cross_entropy/avg: 0.6556, train/total_loss: 0.6692, train/total_loss/avg: 0.6556, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 12, num_updates: 1280, iterations: 1280, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 676ms, time_since_start: 34m 38s 705ms, eta: 16m 16s 607ms\n",
      "\u001b[32m2024-07-23T13:08:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1290/2000, train/hateful_memes/cross_entropy: 0.6692, train/hateful_memes/cross_entropy/avg: 0.6556, train/total_loss: 0.6692, train/total_loss/avg: 0.6556, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 13, num_updates: 1290, iterations: 1290, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 214ms, time_since_start: 34m 50s 920ms, eta: 15m 27s 927ms\n",
      "\u001b[32m2024-07-23T13:08:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, train/hateful_memes/cross_entropy: 0.6692, train/hateful_memes/cross_entropy/avg: 0.6552, train/total_loss: 0.6692, train/total_loss/avg: 0.6552, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 13, num_updates: 1300, iterations: 1300, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 363ms, time_since_start: 35m 03s 283ms, eta: 15m 26s 013ms\n",
      "\u001b[32m2024-07-23T13:08:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:08:54 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:09:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:09:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:09:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:09:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:09:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:09:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, val/hateful_memes/cross_entropy: 0.7409, val/total_loss: 0.7409, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5168, num_updates: 1300, epoch: 13, iterations: 1300, max_updates: 2000, val_time: 21s 539ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.555384\n",
      "\u001b[32m2024-07-23T13:09:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1310/2000, train/hateful_memes/cross_entropy: 0.6627, train/hateful_memes/cross_entropy/avg: 0.6550, train/total_loss: 0.6627, train/total_loss/avg: 0.6550, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 13, num_updates: 1310, iterations: 1310, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 782ms, time_since_start: 35m 38s 615ms, eta: 16m 57s 549ms\n",
      "\u001b[32m2024-07-23T13:09:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1320/2000, train/hateful_memes/cross_entropy: 0.6610, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6610, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 13, num_updates: 1320, iterations: 1320, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 596ms, time_since_start: 35m 51s 212ms, eta: 15m 16s 547ms\n",
      "\u001b[32m2024-07-23T13:09:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1330/2000, train/hateful_memes/cross_entropy: 0.6610, train/hateful_memes/cross_entropy/avg: 0.6550, train/total_loss: 0.6610, train/total_loss/avg: 0.6550, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 13, num_updates: 1330, iterations: 1330, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 392ms, time_since_start: 36m 04s 605ms, eta: 16m 145ms\n",
      "\u001b[32m2024-07-23T13:10:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1340/2000, train/hateful_memes/cross_entropy: 0.6610, train/hateful_memes/cross_entropy/avg: 0.6553, train/total_loss: 0.6610, train/total_loss/avg: 0.6553, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 13, num_updates: 1340, iterations: 1340, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 385ms, time_since_start: 36m 16s 990ms, eta: 14m 34s 683ms\n",
      "\u001b[32m2024-07-23T13:10:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/2000, train/hateful_memes/cross_entropy: 0.6574, train/hateful_memes/cross_entropy/avg: 0.6552, train/total_loss: 0.6574, train/total_loss/avg: 0.6552, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 13, num_updates: 1350, iterations: 1350, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 283ms, time_since_start: 36m 30s 274ms, eta: 15m 23s 854ms\n",
      "\u001b[32m2024-07-23T13:10:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1360/2000, train/hateful_memes/cross_entropy: 0.6610, train/hateful_memes/cross_entropy/avg: 0.6553, train/total_loss: 0.6610, train/total_loss/avg: 0.6553, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 13, num_updates: 1360, iterations: 1360, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 327ms, time_since_start: 36m 42s 601ms, eta: 14m 04s 190ms\n",
      "\u001b[32m2024-07-23T13:10:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1370/2000, train/hateful_memes/cross_entropy: 0.6597, train/hateful_memes/cross_entropy/avg: 0.6553, train/total_loss: 0.6597, train/total_loss/avg: 0.6553, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 13, num_updates: 1370, iterations: 1370, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 202ms, time_since_start: 36m 55s 804ms, eta: 14m 50s 001ms\n",
      "\u001b[32m2024-07-23T13:10:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1380/2000, train/hateful_memes/cross_entropy: 0.6597, train/hateful_memes/cross_entropy/avg: 0.6554, train/total_loss: 0.6597, train/total_loss/avg: 0.6554, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 13, num_updates: 1380, iterations: 1380, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 295ms, time_since_start: 37m 08s 100ms, eta: 13m 35s 716ms\n",
      "\u001b[32m2024-07-23T13:11:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1390/2000, train/hateful_memes/cross_entropy: 0.6574, train/hateful_memes/cross_entropy/avg: 0.6553, train/total_loss: 0.6574, train/total_loss/avg: 0.6553, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 13, num_updates: 1390, iterations: 1390, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 389ms, time_since_start: 37m 20s 490ms, eta: 13m 28s 694ms\n",
      "\u001b[32m2024-07-23T13:11:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, train/hateful_memes/cross_entropy: 0.6597, train/hateful_memes/cross_entropy/avg: 0.6554, train/total_loss: 0.6597, train/total_loss/avg: 0.6554, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 14, num_updates: 1400, iterations: 1400, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 650ms, time_since_start: 37m 33s 140ms, eta: 13m 32s 137ms\n",
      "\u001b[32m2024-07-23T13:11:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:11:24 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:11:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:11:32 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:11:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:11:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:11:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:11:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, val/hateful_memes/cross_entropy: 0.7365, val/total_loss: 0.7365, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5426, num_updates: 1400, epoch: 14, iterations: 1400, max_updates: 2000, val_time: 21s 834ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.555384\n",
      "\u001b[32m2024-07-23T13:11:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1410/2000, train/hateful_memes/cross_entropy: 0.6597, train/hateful_memes/cross_entropy/avg: 0.6560, train/total_loss: 0.6597, train/total_loss/avg: 0.6560, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 14, num_updates: 1410, iterations: 1410, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 768ms, time_since_start: 38m 07s 751ms, eta: 13m 26s 092ms\n",
      "\u001b[32m2024-07-23T13:12:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1420/2000, train/hateful_memes/cross_entropy: 0.6613, train/hateful_memes/cross_entropy/avg: 0.6561, train/total_loss: 0.6613, train/total_loss/avg: 0.6561, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 14, num_updates: 1420, iterations: 1420, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 202ms, time_since_start: 38m 20s 953ms, eta: 13m 39s 349ms\n",
      "\u001b[32m2024-07-23T13:12:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1430/2000, train/hateful_memes/cross_entropy: 0.6597, train/hateful_memes/cross_entropy/avg: 0.6559, train/total_loss: 0.6597, train/total_loss/avg: 0.6559, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 14, num_updates: 1430, iterations: 1430, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 510ms, time_since_start: 38m 33s 464ms, eta: 12m 43s 009ms\n",
      "\u001b[32m2024-07-23T13:12:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1440/2000, train/hateful_memes/cross_entropy: 0.6574, train/hateful_memes/cross_entropy/avg: 0.6558, train/total_loss: 0.6574, train/total_loss/avg: 0.6558, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 14, num_updates: 1440, iterations: 1440, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 047ms, time_since_start: 38m 46s 511ms, eta: 13m 01s 795ms\n",
      "\u001b[32m2024-07-23T13:12:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/2000, train/hateful_memes/cross_entropy: 0.6574, train/hateful_memes/cross_entropy/avg: 0.6560, train/total_loss: 0.6574, train/total_loss/avg: 0.6560, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 14, num_updates: 1450, iterations: 1450, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 586ms, time_since_start: 38m 59s 097ms, eta: 12m 20s 699ms\n",
      "\u001b[32m2024-07-23T13:13:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1460/2000, train/hateful_memes/cross_entropy: 0.6574, train/hateful_memes/cross_entropy/avg: 0.6557, train/total_loss: 0.6574, train/total_loss/avg: 0.6557, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 14, num_updates: 1460, iterations: 1460, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 046ms, time_since_start: 39m 12s 144ms, eta: 12m 33s 821ms\n",
      "\u001b[32m2024-07-23T13:13:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1470/2000, train/hateful_memes/cross_entropy: 0.6597, train/hateful_memes/cross_entropy/avg: 0.6557, train/total_loss: 0.6597, train/total_loss/avg: 0.6557, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 14, num_updates: 1470, iterations: 1470, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 610ms, time_since_start: 39m 24s 754ms, eta: 11m 55s 119ms\n",
      "\u001b[32m2024-07-23T13:13:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1480/2000, train/hateful_memes/cross_entropy: 0.6597, train/hateful_memes/cross_entropy/avg: 0.6558, train/total_loss: 0.6597, train/total_loss/avg: 0.6558, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 14, num_updates: 1480, iterations: 1480, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 102ms, time_since_start: 39m 37s 856ms, eta: 12m 08s 999ms\n",
      "\u001b[32m2024-07-23T13:13:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1490/2000, train/hateful_memes/cross_entropy: 0.6613, train/hateful_memes/cross_entropy/avg: 0.6560, train/total_loss: 0.6613, train/total_loss/avg: 0.6560, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 14, num_updates: 1490, iterations: 1490, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 590ms, time_since_start: 39m 50s 447ms, eta: 11m 27s 072ms\n",
      "\u001b[32m2024-07-23T13:13:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, train/hateful_memes/cross_entropy: 0.6613, train/hateful_memes/cross_entropy/avg: 0.6559, train/total_loss: 0.6613, train/total_loss/avg: 0.6559, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 15, num_updates: 1500, iterations: 1500, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 411ms, time_since_start: 40m 02s 858ms, eta: 11m 03s 990ms\n",
      "\u001b[32m2024-07-23T13:13:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:13:54 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:14:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:14:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:14:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:14:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:14:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:14:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, val/hateful_memes/cross_entropy: 0.7466, val/total_loss: 0.7466, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5195, num_updates: 1500, epoch: 15, iterations: 1500, max_updates: 2000, val_time: 22s 412ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.555384\n",
      "\u001b[32m2024-07-23T13:14:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1510/2000, train/hateful_memes/cross_entropy: 0.6617, train/hateful_memes/cross_entropy/avg: 0.6561, train/total_loss: 0.6617, train/total_loss/avg: 0.6561, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 15, num_updates: 1510, iterations: 1510, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 696ms, time_since_start: 40m 37s 974ms, eta: 11m 05s 676ms\n",
      "\u001b[32m2024-07-23T13:14:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1520/2000, train/hateful_memes/cross_entropy: 0.6617, train/hateful_memes/cross_entropy/avg: 0.6561, train/total_loss: 0.6617, train/total_loss/avg: 0.6561, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 15, num_updates: 1520, iterations: 1520, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 984ms, time_since_start: 40m 51s 959ms, eta: 11m 58s 251ms\n",
      "\u001b[32m2024-07-23T13:14:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1530/2000, train/hateful_memes/cross_entropy: 0.6613, train/hateful_memes/cross_entropy/avg: 0.6560, train/total_loss: 0.6613, train/total_loss/avg: 0.6560, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 15, num_updates: 1530, iterations: 1530, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 688ms, time_since_start: 41m 04s 648ms, eta: 10m 38s 127ms\n",
      "\u001b[32m2024-07-23T13:15:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1540/2000, train/hateful_memes/cross_entropy: 0.6597, train/hateful_memes/cross_entropy/avg: 0.6559, train/total_loss: 0.6597, train/total_loss/avg: 0.6559, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 15, num_updates: 1540, iterations: 1540, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 742ms, time_since_start: 41m 18s 391ms, eta: 11m 16s 425ms\n",
      "\u001b[32m2024-07-23T13:15:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/2000, train/hateful_memes/cross_entropy: 0.6597, train/hateful_memes/cross_entropy/avg: 0.6555, train/total_loss: 0.6597, train/total_loss/avg: 0.6555, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 15, num_updates: 1550, iterations: 1550, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 746ms, time_since_start: 41m 31s 137ms, eta: 10m 13s 722ms\n",
      "\u001b[32m2024-07-23T13:15:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1560/2000, train/hateful_memes/cross_entropy: 0.6597, train/hateful_memes/cross_entropy/avg: 0.6556, train/total_loss: 0.6597, train/total_loss/avg: 0.6556, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 15, num_updates: 1560, iterations: 1560, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 790ms, time_since_start: 41m 44s 927ms, eta: 10m 49s 254ms\n",
      "\u001b[32m2024-07-23T13:15:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1570/2000, train/hateful_memes/cross_entropy: 0.6548, train/hateful_memes/cross_entropy/avg: 0.6556, train/total_loss: 0.6548, train/total_loss/avg: 0.6556, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 15, num_updates: 1570, iterations: 1570, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 539ms, time_since_start: 41m 57s 467ms, eta: 09m 36s 930ms\n",
      "\u001b[32m2024-07-23T13:16:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1580/2000, train/hateful_memes/cross_entropy: 0.6548, train/hateful_memes/cross_entropy/avg: 0.6557, train/total_loss: 0.6548, train/total_loss/avg: 0.6557, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 15, num_updates: 1580, iterations: 1580, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 520ms, time_since_start: 42m 10s 987ms, eta: 10m 07s 605ms\n",
      "\u001b[32m2024-07-23T13:16:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1590/2000, train/hateful_memes/cross_entropy: 0.6548, train/hateful_memes/cross_entropy/avg: 0.6556, train/total_loss: 0.6548, train/total_loss/avg: 0.6556, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 15, num_updates: 1590, iterations: 1590, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 587ms, time_since_start: 42m 23s 574ms, eta: 09m 12s 207ms\n",
      "\u001b[32m2024-07-23T13:16:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, train/hateful_memes/cross_entropy: 0.6542, train/hateful_memes/cross_entropy/avg: 0.6553, train/total_loss: 0.6542, train/total_loss/avg: 0.6553, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 15, num_updates: 1600, iterations: 1600, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 580ms, time_since_start: 42m 37s 155ms, eta: 09m 41s 227ms\n",
      "\u001b[32m2024-07-23T13:16:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:16:28 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:16:36 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:16:36 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:16:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:16:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:16:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:16:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, val/hateful_memes/cross_entropy: 0.7413, val/total_loss: 0.7413, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5269, num_updates: 1600, epoch: 15, iterations: 1600, max_updates: 2000, val_time: 21s 812ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.555384\n",
      "\u001b[32m2024-07-23T13:17:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1610/2000, train/hateful_memes/cross_entropy: 0.6542, train/hateful_memes/cross_entropy/avg: 0.6555, train/total_loss: 0.6542, train/total_loss/avg: 0.6555, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 16, num_updates: 1610, iterations: 1610, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 052ms, time_since_start: 43m 12s 028ms, eta: 09m 04s 693ms\n",
      "\u001b[32m2024-07-23T13:17:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1620/2000, train/hateful_memes/cross_entropy: 0.6542, train/hateful_memes/cross_entropy/avg: 0.6557, train/total_loss: 0.6542, train/total_loss/avg: 0.6557, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 16, num_updates: 1620, iterations: 1620, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 912ms, time_since_start: 43m 24s 940ms, eta: 08m 45s 015ms\n",
      "\u001b[32m2024-07-23T13:17:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1630/2000, train/hateful_memes/cross_entropy: 0.6548, train/hateful_memes/cross_entropy/avg: 0.6557, train/total_loss: 0.6548, train/total_loss/avg: 0.6557, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 16, num_updates: 1630, iterations: 1630, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 906ms, time_since_start: 43m 38s 847ms, eta: 09m 10s 554ms\n",
      "\u001b[32m2024-07-23T13:17:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1640/2000, train/hateful_memes/cross_entropy: 0.6588, train/hateful_memes/cross_entropy/avg: 0.6560, train/total_loss: 0.6588, train/total_loss/avg: 0.6560, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 16, num_updates: 1640, iterations: 1640, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 744ms, time_since_start: 43m 51s 592ms, eta: 08m 10s 936ms\n",
      "\u001b[32m2024-07-23T13:17:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/2000, train/hateful_memes/cross_entropy: 0.6548, train/hateful_memes/cross_entropy/avg: 0.6559, train/total_loss: 0.6548, train/total_loss/avg: 0.6559, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 16, num_updates: 1650, iterations: 1650, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 812ms, time_since_start: 44m 05s 404ms, eta: 08m 37s 273ms\n",
      "\u001b[32m2024-07-23T13:18:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1660/2000, train/hateful_memes/cross_entropy: 0.6548, train/hateful_memes/cross_entropy/avg: 0.6558, train/total_loss: 0.6548, train/total_loss/avg: 0.6558, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 16, num_updates: 1660, iterations: 1660, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 722ms, time_since_start: 44m 18s 127ms, eta: 07m 42s 852ms\n",
      "\u001b[32m2024-07-23T13:18:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1670/2000, train/hateful_memes/cross_entropy: 0.6542, train/hateful_memes/cross_entropy/avg: 0.6558, train/total_loss: 0.6542, train/total_loss/avg: 0.6558, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 16, num_updates: 1670, iterations: 1670, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 966ms, time_since_start: 44m 32s 093ms, eta: 08m 13s 150ms\n",
      "\u001b[32m2024-07-23T13:18:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1680/2000, train/hateful_memes/cross_entropy: 0.6503, train/hateful_memes/cross_entropy/avg: 0.6555, train/total_loss: 0.6503, train/total_loss/avg: 0.6555, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 16, num_updates: 1680, iterations: 1680, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 900ms, time_since_start: 44m 44s 994ms, eta: 07m 21s 726ms\n",
      "\u001b[32m2024-07-23T13:18:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1690/2000, train/hateful_memes/cross_entropy: 0.6503, train/hateful_memes/cross_entropy/avg: 0.6557, train/total_loss: 0.6503, train/total_loss/avg: 0.6557, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 16, num_updates: 1690, iterations: 1690, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 906ms, time_since_start: 44m 58s 901ms, eta: 07m 41s 290ms\n",
      "\u001b[32m2024-07-23T13:19:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, train/hateful_memes/cross_entropy: 0.6503, train/hateful_memes/cross_entropy/avg: 0.6557, train/total_loss: 0.6503, train/total_loss/avg: 0.6557, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 16, num_updates: 1700, iterations: 1700, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 770ms, time_since_start: 45m 11s 671ms, eta: 06m 49s 917ms\n",
      "\u001b[32m2024-07-23T13:19:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:19:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:19:10 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:19:10 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:19:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:19:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:19:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:19:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, val/hateful_memes/cross_entropy: 0.7447, val/total_loss: 0.7447, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5245, num_updates: 1700, epoch: 16, iterations: 1700, max_updates: 2000, val_time: 20s 570ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.555384\n",
      "\u001b[32m2024-07-23T13:19:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1710/2000, train/hateful_memes/cross_entropy: 0.6489, train/hateful_memes/cross_entropy/avg: 0.6555, train/total_loss: 0.6489, train/total_loss/avg: 0.6555, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 16, num_updates: 1710, iterations: 1710, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 608ms, time_since_start: 45m 45s 858ms, eta: 07m 02s 275ms\n",
      "\u001b[32m2024-07-23T13:19:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1720/2000, train/hateful_memes/cross_entropy: 0.6489, train/hateful_memes/cross_entropy/avg: 0.6556, train/total_loss: 0.6489, train/total_loss/avg: 0.6556, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 17, num_updates: 1720, iterations: 1720, max_updates: 2000, lr: 0.00001, ups: 0.91, time: 11s 881ms, time_since_start: 45m 57s 739ms, eta: 05m 55s 959ms\n",
      "\u001b[32m2024-07-23T13:20:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1730/2000, train/hateful_memes/cross_entropy: 0.6482, train/hateful_memes/cross_entropy/avg: 0.6556, train/total_loss: 0.6482, train/total_loss/avg: 0.6556, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 17, num_updates: 1730, iterations: 1730, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 306ms, time_since_start: 46m 11s 046ms, eta: 06m 24s 435ms\n",
      "\u001b[32m2024-07-23T13:20:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1740/2000, train/hateful_memes/cross_entropy: 0.6482, train/hateful_memes/cross_entropy/avg: 0.6555, train/total_loss: 0.6482, train/total_loss/avg: 0.6555, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 17, num_updates: 1740, iterations: 1740, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 366ms, time_since_start: 46m 23s 412ms, eta: 05m 44s 035ms\n",
      "\u001b[32m2024-07-23T13:20:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/2000, train/hateful_memes/cross_entropy: 0.6482, train/hateful_memes/cross_entropy/avg: 0.6554, train/total_loss: 0.6482, train/total_loss/avg: 0.6554, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 17, num_updates: 1750, iterations: 1750, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 186ms, time_since_start: 46m 36s 599ms, eta: 05m 52s 751ms\n",
      "\u001b[32m2024-07-23T13:20:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1760/2000, train/hateful_memes/cross_entropy: 0.6476, train/hateful_memes/cross_entropy/avg: 0.6553, train/total_loss: 0.6476, train/total_loss/avg: 0.6553, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 17, num_updates: 1760, iterations: 1760, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 363ms, time_since_start: 46m 48s 963ms, eta: 05m 17s 498ms\n",
      "\u001b[32m2024-07-23T13:20:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1770/2000, train/hateful_memes/cross_entropy: 0.6476, train/hateful_memes/cross_entropy/avg: 0.6554, train/total_loss: 0.6476, train/total_loss/avg: 0.6554, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 17, num_updates: 1770, iterations: 1770, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 170ms, time_since_start: 47m 02s 134ms, eta: 05m 24s 132ms\n",
      "\u001b[32m2024-07-23T13:21:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1780/2000, train/hateful_memes/cross_entropy: 0.6456, train/hateful_memes/cross_entropy/avg: 0.6552, train/total_loss: 0.6456, train/total_loss/avg: 0.6552, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 17, num_updates: 1780, iterations: 1780, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 416ms, time_since_start: 47m 14s 550ms, eta: 04m 52s 285ms\n",
      "\u001b[32m2024-07-23T13:21:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1790/2000, train/hateful_memes/cross_entropy: 0.6456, train/hateful_memes/cross_entropy/avg: 0.6551, train/total_loss: 0.6456, train/total_loss/avg: 0.6551, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 17, num_updates: 1790, iterations: 1790, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 442ms, time_since_start: 47m 27s 993ms, eta: 05m 02s 062ms\n",
      "\u001b[32m2024-07-23T13:21:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, train/hateful_memes/cross_entropy: 0.6456, train/hateful_memes/cross_entropy/avg: 0.6550, train/total_loss: 0.6456, train/total_loss/avg: 0.6550, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 17, num_updates: 1800, iterations: 1800, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 442ms, time_since_start: 47m 40s 435ms, eta: 04m 26s 264ms\n",
      "\u001b[32m2024-07-23T13:21:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:21:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:21:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:21:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:21:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:21:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:21:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:21:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, val/hateful_memes/cross_entropy: 0.7283, val/total_loss: 0.7283, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5190, num_updates: 1800, epoch: 17, iterations: 1800, max_updates: 2000, val_time: 21s 411ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.555384\n",
      "\u001b[32m2024-07-23T13:22:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1810/2000, train/hateful_memes/cross_entropy: 0.6451, train/hateful_memes/cross_entropy/avg: 0.6548, train/total_loss: 0.6451, train/total_loss/avg: 0.6548, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 17, num_updates: 1810, iterations: 1810, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 644ms, time_since_start: 48m 15s 510ms, eta: 04m 37s 386ms\n",
      "\u001b[32m2024-07-23T13:22:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1820/2000, train/hateful_memes/cross_entropy: 0.6441, train/hateful_memes/cross_entropy/avg: 0.6545, train/total_loss: 0.6441, train/total_loss/avg: 0.6545, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 18, num_updates: 1820, iterations: 1820, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 647ms, time_since_start: 48m 28s 157ms, eta: 04m 03s 586ms\n",
      "\u001b[32m2024-07-23T13:22:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1830/2000, train/hateful_memes/cross_entropy: 0.6441, train/hateful_memes/cross_entropy/avg: 0.6546, train/total_loss: 0.6441, train/total_loss/avg: 0.6546, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 18, num_updates: 1830, iterations: 1830, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 376ms, time_since_start: 48m 40s 534ms, eta: 03m 45s 131ms\n",
      "\u001b[32m2024-07-23T13:22:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1840/2000, train/hateful_memes/cross_entropy: 0.6441, train/hateful_memes/cross_entropy/avg: 0.6547, train/total_loss: 0.6441, train/total_loss/avg: 0.6547, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 18, num_updates: 1840, iterations: 1840, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 201ms, time_since_start: 48m 53s 735ms, eta: 03m 46s 005ms\n",
      "\u001b[32m2024-07-23T13:22:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/2000, train/hateful_memes/cross_entropy: 0.6451, train/hateful_memes/cross_entropy/avg: 0.6546, train/total_loss: 0.6451, train/total_loss/avg: 0.6546, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 18, num_updates: 1850, iterations: 1850, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 262ms, time_since_start: 49m 05s 998ms, eta: 03m 16s 817ms\n",
      "\u001b[32m2024-07-23T13:23:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1860/2000, train/hateful_memes/cross_entropy: 0.6451, train/hateful_memes/cross_entropy/avg: 0.6547, train/total_loss: 0.6451, train/total_loss/avg: 0.6547, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 18, num_updates: 1860, iterations: 1860, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 131ms, time_since_start: 49m 19s 129ms, eta: 03m 16s 709ms\n",
      "\u001b[32m2024-07-23T13:23:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1870/2000, train/hateful_memes/cross_entropy: 0.6451, train/hateful_memes/cross_entropy/avg: 0.6550, train/total_loss: 0.6451, train/total_loss/avg: 0.6550, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 18, num_updates: 1870, iterations: 1870, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 330ms, time_since_start: 49m 31s 460ms, eta: 02m 51s 519ms\n",
      "\u001b[32m2024-07-23T13:23:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1880/2000, train/hateful_memes/cross_entropy: 0.6456, train/hateful_memes/cross_entropy/avg: 0.6550, train/total_loss: 0.6456, train/total_loss/avg: 0.6550, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 18, num_updates: 1880, iterations: 1880, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 219ms, time_since_start: 49m 44s 680ms, eta: 02m 49s 739ms\n",
      "\u001b[32m2024-07-23T13:23:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1890/2000, train/hateful_memes/cross_entropy: 0.6451, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6451, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 18, num_updates: 1890, iterations: 1890, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 310ms, time_since_start: 49m 56s 990ms, eta: 02m 24s 891ms\n",
      "\u001b[32m2024-07-23T13:24:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, train/hateful_memes/cross_entropy: 0.6441, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6441, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 18, num_updates: 1900, iterations: 1900, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 230ms, time_since_start: 50m 10s 221ms, eta: 02m 21s 568ms\n",
      "\u001b[32m2024-07-23T13:24:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:24:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:24:09 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:24:09 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:24:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:24:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:24:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:24:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, val/hateful_memes/cross_entropy: 0.7315, val/total_loss: 0.7315, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5222, num_updates: 1900, epoch: 18, iterations: 1900, max_updates: 2000, val_time: 41s 812ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.555384\n",
      "\u001b[32m2024-07-23T13:24:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1910/2000, train/hateful_memes/cross_entropy: 0.6451, train/hateful_memes/cross_entropy/avg: 0.6551, train/total_loss: 0.6451, train/total_loss/avg: 0.6551, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 18, num_updates: 1910, iterations: 1910, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 728ms, time_since_start: 51m 04s 771ms, eta: 02m 02s 577ms\n",
      "\u001b[32m2024-07-23T13:25:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1920/2000, train/hateful_memes/cross_entropy: 0.6441, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6441, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 18, num_updates: 1920, iterations: 1920, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 197ms, time_since_start: 51m 17s 968ms, eta: 01m 52s 972ms\n",
      "\u001b[32m2024-07-23T13:25:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1930/2000, train/hateful_memes/cross_entropy: 0.6451, train/hateful_memes/cross_entropy/avg: 0.6551, train/total_loss: 0.6451, train/total_loss/avg: 0.6551, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 19, num_updates: 1930, iterations: 1930, max_updates: 2000, lr: 0., ups: 0.91, time: 11s 291ms, time_since_start: 51m 29s 260ms, eta: 01m 24s 574ms\n",
      "\u001b[32m2024-07-23T13:25:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1940/2000, train/hateful_memes/cross_entropy: 0.6441, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6441, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 19, num_updates: 1940, iterations: 1940, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 523ms, time_since_start: 51m 42s 783ms, eta: 01m 26s 817ms\n",
      "\u001b[32m2024-07-23T13:25:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/2000, train/hateful_memes/cross_entropy: 0.6427, train/hateful_memes/cross_entropy/avg: 0.6547, train/total_loss: 0.6427, train/total_loss/avg: 0.6547, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 19, num_updates: 1950, iterations: 1950, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 627ms, time_since_start: 51m 55s 411ms, eta: 01m 07s 557ms\n",
      "\u001b[32m2024-07-23T13:26:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1960/2000, train/hateful_memes/cross_entropy: 0.6441, train/hateful_memes/cross_entropy/avg: 0.6548, train/total_loss: 0.6441, train/total_loss/avg: 0.6548, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 19, num_updates: 1960, iterations: 1960, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 749ms, time_since_start: 52m 09s 160ms, eta: 58s 848ms\n",
      "\u001b[32m2024-07-23T13:26:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1970/2000, train/hateful_memes/cross_entropy: 0.6441, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6441, train/total_loss/avg: 0.6549, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 19, num_updates: 1970, iterations: 1970, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 539ms, time_since_start: 52m 21s 700ms, eta: 40s 250ms\n",
      "\u001b[32m2024-07-23T13:26:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1980/2000, train/hateful_memes/cross_entropy: 0.6441, train/hateful_memes/cross_entropy/avg: 0.6547, train/total_loss: 0.6441, train/total_loss/avg: 0.6547, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 19, num_updates: 1980, iterations: 1980, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 368ms, time_since_start: 52m 35s 068ms, eta: 28s 607ms\n",
      "\u001b[32m2024-07-23T13:26:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1990/2000, train/hateful_memes/cross_entropy: 0.6441, train/hateful_memes/cross_entropy/avg: 0.6544, train/total_loss: 0.6441, train/total_loss/avg: 0.6544, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 19, num_updates: 1990, iterations: 1990, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 481ms, time_since_start: 52m 47s 549ms, eta: 13s 354ms\n",
      "\u001b[32m2024-07-23T13:26:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-23T13:26:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:26:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:27:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:27:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, train/hateful_memes/cross_entropy: 0.6456, train/hateful_memes/cross_entropy/avg: 0.6544, train/total_loss: 0.6456, train/total_loss/avg: 0.6544, max mem: 13427.0, experiment: hateful_memes_lr8e5, epoch: 19, num_updates: 2000, iterations: 2000, max_updates: 2000, lr: 0., ups: 0.37, time: 27s 208ms, time_since_start: 53m 14s 757ms, eta: 0ms\n",
      "\u001b[32m2024-07-23T13:27:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-23T13:27:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-23T13:27:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:27:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:27:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-23T13:27:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-23T13:27:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-23T13:27:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, val/hateful_memes/cross_entropy: 0.7368, val/total_loss: 0.7368, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5194, num_updates: 2000, epoch: 19, iterations: 2000, max_updates: 2000, val_time: 23s 804ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.555384\n",
      "\u001b[32m2024-07-23T13:27:30 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2024-07-23T13:27:30 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2024-07-23T13:27:30 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2024-07-23T13:27:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2024-07-23T13:27:38 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1200\n",
      "\u001b[32m2024-07-23T13:27:38 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1200\n",
      "\u001b[32m2024-07-23T13:27:38 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 12\n",
      "\u001b[32m2024-07-23T13:27:42 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2024-07-23T13:27:42 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.13s/it]\n",
      "\u001b[32m2024-07-23T13:27:49 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-23T13:27:49 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-23T13:27:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, val/hateful_memes/cross_entropy: 0.7201, val/total_loss: 0.7201, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5556\n",
      "\u001b[32m2024-07-23T13:27:50 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 53m 59s 014ms\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.139 MB of 0.139 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/cross_entropy ▆▃█▁▆▆▆▇▆▄▆▅▇▅▅▆▇▅▆▄▆▅▇▆▅▆▇▆▅▇▅▇█▃▅▄▆▆▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/learning_rate ▂▃▅▆██▇▇▇▇▇▆▆▆▆▅▅▅▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/total_loss ▆▃█▁▆▆▆▇▆▄▆▅▇▅▅▆▇▅▆▄▆▅▇▆▅▆▇▆▅▇▅▇█▃▅▄▆▆▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val/hateful_memes/accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val/hateful_memes/binary_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/cross_entropy ▃▄▃▃▆▅▄▄▃▂▃▂█▅▅▂▇▁▁▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/hateful_memes/roc_auc ▇▇▂▃▃▄▇▇▁▇██▄▇▄▅▅▄▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val/total_loss ▃▄▃▃▆▅▄▄▃▂▃▂█▅▅▂▇▁▁▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/cross_entropy 0.65114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/total_loss 0.65114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step 1200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val/hateful_memes/accuracy 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val/hateful_memes/binary_f1 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/cross_entropy 0.74337\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/hateful_memes/roc_auc 0.55565\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val/total_loss 0.74337\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhateful_memes_lr8e5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/wir6cua1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240723_123352-wir6cua1/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "Exception ignored in: <function WandbLogger.__del__ at 0x7f8d3fb0bf70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf/utils/logger.py\", line 457, in __del__\n",
      "    self._wandb.finish()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 4262, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 390, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2100, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2108, in _finish\n",
      "    tel.feature.finish = True\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
      "    self._run._telemetry_callback(self._obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 798, in _telemetry_callback\n",
      "    self._telemetry_flush()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 809, in _telemetry_flush\n",
      "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 101, in _publish_telemetry\n",
      "    self._publish(rec)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_roberta/defaults.yaml \\\n",
    "model=visual_roberta \\\n",
    "dataset=hateful_memes \\\n",
    "training.log_interval=10 \\\n",
    "training.seed=2024 \\\n",
    "training.batch_size=80 \\\n",
    "training.evaluation_interval=100 \\\n",
    "training.experiment_name=hateful_memes_lr8e5 \\\n",
    "training.max_updates=2000 \\\n",
    "optimizer.params.lr=8e-5 \\\n",
    "training.fp16=True \\\n",
    "scheduler.params.num_warmup_steps=200 \\\n",
    "checkpoint.max_to_keep=1 \\\n",
    "training.wandb.enabled=True \\\n",
    "run_type=train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/bin/mmf_run\", line 8, in <module>\n",
      "    sys.exit(run())\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf_cli/run.py\", line 91, in run\n",
      "    configuration = Configuration(args)\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf/utils/configuration.py\", line 329, in __init__\n",
      "    self._default_config = _merge_with_dotlist(\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf/utils/configuration.py\", line 187, in _merge_with_dotlist\n",
      "    assert len(opt) == 2, f\"{opt} has no value\"\n",
      "AssertionError: [''] has no value\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_roberta/defaults.yaml \\\n",
    "model=visual_roberta \\\n",
    "dataset=hateful_memes \\\n",
    "training.log_interval=10 \\\n",
    "training.seed=2024 \\\n",
    "training.batch_size=80 \\\n",
    "training.evaluation_interval=100 \\\n",
    "training.experiment_name=hateful_memes_lr7e5_ls1 \\\n",
    "training.max_updates=2000 \\\n",
    "optimizer.params.lr=7e-5 \\\n",
    "training.fp16=True \\\n",
    "scheduler.params.num_warmup_steps=200 \\\n",
    "checkpoint.max_to_keep=1 \\\n",
    "training.wandb.enabled=True \\\n",
    "run_type=train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/bin/mmf_run\", line 8, in <module>\n",
      "    sys.exit(run())\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf_cli/run.py\", line 91, in run\n",
      "    configuration = Configuration(args)\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf/utils/configuration.py\", line 329, in __init__\n",
      "    self._default_config = _merge_with_dotlist(\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf/utils/configuration.py\", line 187, in _merge_with_dotlist\n",
      "    assert len(opt) == 2, f\"{opt} has no value\"\n",
      "AssertionError: [''] has no value\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_roberta/defaults.yaml \\\n",
    "model=visual_roberta \\\n",
    "dataset=hateful_memes \\\n",
    "training.log_interval=10 \\\n",
    "training.seed=2024 \\\n",
    "training.batch_size=80 \\\n",
    "training.evaluation_interval=100 \\\n",
    "training.experiment_name=hateful_memes_lr7e5_ls1 \\\n",
    "training.max_updates=2000 \\\n",
    "optimizer.params.lr=7e-5 \\\n",
    "training.fp16=True \\\n",
    "scheduler.params.num_warmup_steps=200 \\\n",
    "checkpoint.max_to_keep=1 \\\n",
    "training.wandb.enabled=True \\\n",
    "run_type=train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_roberta/defaults.yaml\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_roberta\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 10\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option training.seed to 2024\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 80\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 100\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option training.experiment_name to hateful_memes_lr6e5_fl_gamma_2_cosine_5k\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 5000\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 6e-5\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option training.fp16 to True\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 400\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option training.wandb.enabled to True\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_roberta/defaults.yaml', 'model=visual_roberta', 'dataset=hateful_memes', 'training.log_interval=10', 'training.seed=2024', 'training.batch_size=80', 'training.evaluation_interval=100', 'training.experiment_name=hateful_memes_lr6e5_fl_gamma_2_cosine_5k', 'training.max_updates=5000', 'optimizer.params.lr=6e-5', 'training.fp16=True', 'scheduler.type=warmup_cosine', 'scheduler.params.num_warmup_steps=400', 'checkpoint.max_to_keep=1', 'training.wandb.enabled=True', 'run_type=train_val'])\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf_cli.run: \u001b[0mTorch version: 1.11.0+cu102\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf_cli.run: \u001b[0mUsing seed 2024\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/zeus/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/zeus/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/zeus/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at /home/zeus/.cache/huggingface/transformers/dfe8f1ad04cb25b61a647e3d13620f9bf0a0f51d277897b232a5735297134132.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-24T18:12:01 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2024-07-24T18:12:01 | torch.distributed.nn.jit.instantiator: \u001b[0mCreated a temporary directory at /tmp/tmppklid1xx\n",
      "\u001b[32m2024-07-24T18:12:01 | torch.distributed.nn.jit.instantiator: \u001b[0mWriting /tmp/tmppklid1xx/_remote_module_non_sriptable.py\n",
      "HI BRO {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': [{'type': 'focal_loss', 'params': {'gamma': 2}}], 'model': 'visual_roberta'}\n",
      "SELFIE {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': [{'type': 'focal_loss', 'params': {'gamma': 2}}], 'model': 'visual_roberta'}\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    {\n",
      "      \"params\": {\n",
      "        \"gamma\": 2\n",
      "      },\n",
      "      \"type\": \"focal_loss\"\n",
      "    }\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "SELF ROBERT CONFIG RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    {\n",
      "      \"params\": {\n",
      "        \"gamma\": 2\n",
      "      },\n",
      "      \"type\": \"focal_loss\"\n",
      "    }\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "roberta-base\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/zeus/.cache/torch/mmf/distributed_-1/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VisualRobertaBase: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing VisualRobertaBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualRobertaBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualRobertaBase were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_embeddings_visual.weight', 'roberta.embeddings.position_ids', 'roberta.embeddings.token_type_embeddings_visual.weight', 'roberta.embeddings.projection.weight', 'roberta.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-07-24T18:12:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2024-07-24T18:12:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikrant17\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/wandb/run-20240724_181209-tumxozw7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhateful_memes_lr6e5_fl_gamma_2_cosine_5k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/tumxozw7\u001b[0m\n",
      "\u001b[32m2024-07-24T18:12:11 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2024-07-24T18:12:11 | mmf.trainers.mmf_trainer: \u001b[0mVisualRoberta(\n",
      "  (model): VisualRobertaForClassification(\n",
      "    (roberta): VisualRobertaBase(\n",
      "      (embeddings): RobertaVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(1, 768)\n",
      "        (position_embeddings_visual): Embedding(514, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): FocalLoss()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2024-07-24T18:12:11 | mmf.utils.general: \u001b[0mTotal Parameters: 127208450. Trained Parameters: 127208450\n",
      "\u001b[32m2024-07-24T18:12:11 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2024-07-24T18:12:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10/5000, train/hateful_memes/focal_loss: 0.2137, train/hateful_memes/focal_loss/avg: 0.2137, train/total_loss: 0.2137, train/total_loss/avg: 0.2137, max mem: 13359.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 1, num_updates: 10, iterations: 10, max_updates: 5000, lr: 0., ups: 0.77, time: 13s 786ms, time_since_start: 17s 999ms, eta: 02h 02m 41s 283ms\n",
      "\u001b[32m2024-07-24T18:12:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20/5000, train/hateful_memes/focal_loss: 0.1723, train/hateful_memes/focal_loss/avg: 0.1930, train/total_loss: 0.1723, train/total_loss/avg: 0.1930, max mem: 13359.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 1, num_updates: 20, iterations: 20, max_updates: 5000, lr: 0., ups: 0.83, time: 12s 593ms, time_since_start: 30s 593ms, eta: 01h 51m 50s 737ms\n",
      "\u001b[32m2024-07-24T18:12:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 30/5000, train/hateful_memes/focal_loss: 0.1815, train/hateful_memes/focal_loss/avg: 0.1892, train/total_loss: 0.1815, train/total_loss/avg: 0.1892, max mem: 13359.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 1, num_updates: 30, iterations: 30, max_updates: 5000, lr: 0., ups: 0.83, time: 12s 937ms, time_since_start: 43s 530ms, eta: 01h 54m 40s 108ms\n",
      "\u001b[32m2024-07-24T18:13:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 40/5000, train/hateful_memes/focal_loss: 0.1759, train/hateful_memes/focal_loss/avg: 0.1858, train/total_loss: 0.1759, train/total_loss/avg: 0.1858, max mem: 13359.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 1, num_updates: 40, iterations: 40, max_updates: 5000, lr: 0.00001, ups: 0.83, time: 12s 529ms, time_since_start: 56s 060ms, eta: 01h 50m 49s 822ms\n",
      "\u001b[32m2024-07-24T18:13:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/5000, train/hateful_memes/focal_loss: 0.1759, train/hateful_memes/focal_loss/avg: 0.1790, train/total_loss: 0.1759, train/total_loss/avg: 0.1790, max mem: 13359.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 1, num_updates: 50, iterations: 50, max_updates: 5000, lr: 0.00001, ups: 0.83, time: 12s 972ms, time_since_start: 01m 09s 033ms, eta: 01h 54m 31s 141ms\n",
      "\u001b[32m2024-07-24T18:13:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 60/5000, train/hateful_memes/focal_loss: 0.1723, train/hateful_memes/focal_loss/avg: 0.1754, train/total_loss: 0.1723, train/total_loss/avg: 0.1754, max mem: 13359.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 1, num_updates: 60, iterations: 60, max_updates: 5000, lr: 0.00001, ups: 0.83, time: 12s 690ms, time_since_start: 01m 21s 723ms, eta: 01h 51m 47s 809ms\n",
      "\u001b[32m2024-07-24T18:13:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 70/5000, train/hateful_memes/focal_loss: 0.1759, train/hateful_memes/focal_loss/avg: 0.1756, train/total_loss: 0.1759, train/total_loss/avg: 0.1756, max mem: 13359.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 1, num_updates: 70, iterations: 70, max_updates: 5000, lr: 0.00001, ups: 0.77, time: 13s 110ms, time_since_start: 01m 34s 834ms, eta: 01h 55m 15s 954ms\n",
      "\u001b[32m2024-07-24T18:13:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 80/5000, train/hateful_memes/focal_loss: 0.1723, train/hateful_memes/focal_loss/avg: 0.1751, train/total_loss: 0.1723, train/total_loss/avg: 0.1751, max mem: 13359.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 1, num_updates: 80, iterations: 80, max_updates: 5000, lr: 0.00001, ups: 0.83, time: 12s 839ms, time_since_start: 01m 47s 674ms, eta: 01h 52m 39s 422ms\n",
      "\u001b[32m2024-07-24T18:14:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 90/5000, train/hateful_memes/focal_loss: 0.1759, train/hateful_memes/focal_loss/avg: 0.1757, train/total_loss: 0.1759, train/total_loss/avg: 0.1757, max mem: 13359.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 1, num_updates: 90, iterations: 90, max_updates: 5000, lr: 0.00001, ups: 0.77, time: 13s 255ms, time_since_start: 02m 930ms, eta: 01h 56m 04s 217ms\n",
      "\u001b[32m2024-07-24T18:14:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/5000, train/hateful_memes/focal_loss: 0.1723, train/hateful_memes/focal_loss/avg: 0.1752, train/total_loss: 0.1723, train/total_loss/avg: 0.1752, max mem: 13359.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 1, num_updates: 100, iterations: 100, max_updates: 5000, lr: 0.00002, ups: 0.83, time: 12s 678ms, time_since_start: 02m 13s 609ms, eta: 01h 50m 47s 496ms\n",
      "\u001b[32m2024-07-24T18:14:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T18:14:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T18:14:30 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T18:14:30 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T18:14:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T18:14:34 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T18:14:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T18:14:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T18:14:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/5000, val/hateful_memes/focal_loss: 0.1848, val/total_loss: 0.1848, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5455, num_updates: 100, epoch: 1, iterations: 100, max_updates: 5000, val_time: 31s 382ms, best_update: 100, best_iteration: 100, best_val/hateful_memes/roc_auc: 0.545520\n",
      "\u001b[32m2024-07-24T18:15:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 110/5000, train/hateful_memes/focal_loss: 0.1723, train/hateful_memes/focal_loss/avg: 0.1750, train/total_loss: 0.1723, train/total_loss/avg: 0.1750, max mem: 13427.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 2, num_updates: 110, iterations: 110, max_updates: 5000, lr: 0.00002, ups: 0.91, time: 11s 892ms, time_since_start: 02m 56s 891ms, eta: 01h 43m 42s 571ms\n",
      "\u001b[32m2024-07-24T18:15:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 120/5000, train/hateful_memes/focal_loss: 0.1723, train/hateful_memes/focal_loss/avg: 0.1758, train/total_loss: 0.1723, train/total_loss/avg: 0.1758, max mem: 13427.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 2, num_updates: 120, iterations: 120, max_updates: 5000, lr: 0.00002, ups: 0.83, time: 12s 725ms, time_since_start: 03m 09s 617ms, eta: 01h 50m 44s 807ms\n",
      "\u001b[32m2024-07-24T18:15:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 130/5000, train/hateful_memes/focal_loss: 0.1723, train/hateful_memes/focal_loss/avg: 0.1749, train/total_loss: 0.1723, train/total_loss/avg: 0.1749, max mem: 13427.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 2, num_updates: 130, iterations: 130, max_updates: 5000, lr: 0.00002, ups: 0.77, time: 13s 668ms, time_since_start: 03m 23s 285ms, eta: 01h 58m 42s 410ms\n",
      "\u001b[32m2024-07-24T18:15:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 140/5000, train/hateful_memes/focal_loss: 0.1722, train/hateful_memes/focal_loss/avg: 0.1721, train/total_loss: 0.1722, train/total_loss/avg: 0.1721, max mem: 13427.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 2, num_updates: 140, iterations: 140, max_updates: 5000, lr: 0.00002, ups: 0.83, time: 12s 704ms, time_since_start: 03m 35s 989ms, eta: 01h 50m 06s 408ms\n",
      "\u001b[32m2024-07-24T18:15:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/5000, train/hateful_memes/focal_loss: 0.1723, train/hateful_memes/focal_loss/avg: 0.1726, train/total_loss: 0.1723, train/total_loss/avg: 0.1726, max mem: 13427.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 2, num_updates: 150, iterations: 150, max_updates: 5000, lr: 0.00002, ups: 0.77, time: 13s 312ms, time_since_start: 03m 49s 302ms, eta: 01h 55m 08s 781ms\n",
      "\u001b[32m2024-07-24T18:16:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 160/5000, train/hateful_memes/focal_loss: 0.1722, train/hateful_memes/focal_loss/avg: 0.1715, train/total_loss: 0.1722, train/total_loss/avg: 0.1715, max mem: 13427.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 2, num_updates: 160, iterations: 160, max_updates: 5000, lr: 0.00002, ups: 0.83, time: 12s 826ms, time_since_start: 04m 02s 129ms, eta: 01h 50m 42s 567ms\n",
      "\u001b[32m2024-07-24T18:16:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 170/5000, train/hateful_memes/focal_loss: 0.1722, train/hateful_memes/focal_loss/avg: 0.1708, train/total_loss: 0.1722, train/total_loss/avg: 0.1708, max mem: 13427.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 2, num_updates: 170, iterations: 170, max_updates: 5000, lr: 0.00003, ups: 0.77, time: 13s 397ms, time_since_start: 04m 15s 526ms, eta: 01h 55m 23s 769ms\n",
      "\u001b[32m2024-07-24T18:16:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 180/5000, train/hateful_memes/focal_loss: 0.1715, train/hateful_memes/focal_loss/avg: 0.1696, train/total_loss: 0.1715, train/total_loss/avg: 0.1696, max mem: 13427.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 2, num_updates: 180, iterations: 180, max_updates: 5000, lr: 0.00003, ups: 0.83, time: 12s 949ms, time_since_start: 04m 28s 475ms, eta: 01h 51m 18s 372ms\n",
      "\u001b[32m2024-07-24T18:16:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 190/5000, train/hateful_memes/focal_loss: 0.1715, train/hateful_memes/focal_loss/avg: 0.1692, train/total_loss: 0.1715, train/total_loss/avg: 0.1692, max mem: 13427.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 2, num_updates: 190, iterations: 190, max_updates: 5000, lr: 0.00003, ups: 0.77, time: 13s 500ms, time_since_start: 04m 41s 976ms, eta: 01h 55m 48s 497ms\n",
      "\u001b[32m2024-07-24T18:17:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/5000, train/hateful_memes/focal_loss: 0.1714, train/hateful_memes/focal_loss/avg: 0.1687, train/total_loss: 0.1714, train/total_loss/avg: 0.1687, max mem: 13427.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 2, num_updates: 200, iterations: 200, max_updates: 5000, lr: 0.00003, ups: 0.83, time: 12s 914ms, time_since_start: 04m 54s 890ms, eta: 01h 50m 32s 810ms\n",
      "\u001b[32m2024-07-24T18:17:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T18:17:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T18:17:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T18:17:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T18:17:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T18:17:15 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T18:17:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T18:17:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T18:17:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/5000, val/hateful_memes/focal_loss: 0.2222, val/total_loss: 0.2222, val/hateful_memes/accuracy: 0.5100, val/hateful_memes/binary_f1: 0.0541, val/hateful_memes/roc_auc: 0.5623, num_updates: 200, epoch: 2, iterations: 200, max_updates: 5000, val_time: 34s 260ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.562256\n",
      "\u001b[32m2024-07-24T18:17:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 210/5000, train/hateful_memes/focal_loss: 0.1709, train/hateful_memes/focal_loss/avg: 0.1688, train/total_loss: 0.1709, train/total_loss/avg: 0.1688, max mem: 13427.0, experiment: hateful_memes_lr6e5_fl_gamma_2_cosine_5k, epoch: 2, num_updates: 210, iterations: 210, max_updates: 5000, lr: 0.00003, ups: 0.71, time: 14s 145ms, time_since_start: 05m 43s 304ms, eta: 02h 49s 806ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_roberta/defaults.yaml \\\n",
    "model=visual_roberta \\\n",
    "dataset=hateful_memes \\\n",
    "training.log_interval=10 \\\n",
    "training.seed=2024 \\\n",
    "training.batch_size=80 \\\n",
    "training.evaluation_interval=100 \\\n",
    "training.experiment_name=hateful_memes_lr6e5_fl_gamma_2_cosine_5k \\\n",
    "training.max_updates=5000 \\\n",
    "optimizer.params.lr=6e-5 \\\n",
    "training.fp16=True \\\n",
    "scheduler.type=warmup_cosine \\\n",
    "scheduler.params.num_warmup_steps=400 \\\n",
    "checkpoint.max_to_keep=1 \\\n",
    "training.wandb.enabled=True \\\n",
    "run_type=train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_roberta/defaults.yaml\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_roberta\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 10\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.seed to 2024\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 80\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 100\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.experiment_name to hateful_memes_lr7e5_lb.1\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 2000\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 7e-5\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.fp16 to True\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 200\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.wandb.enabled to True\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_roberta/defaults.yaml', 'model=visual_roberta', 'dataset=hateful_memes', 'training.log_interval=10', 'training.seed=2024', 'training.batch_size=80', 'training.evaluation_interval=100', 'training.experiment_name=hateful_memes_lr7e5_lb.1', 'training.max_updates=2000', 'optimizer.params.lr=7e-5', 'training.fp16=True', 'scheduler.params.num_warmup_steps=200', 'checkpoint.max_to_keep=1', 'training.wandb.enabled=True', 'run_type=train_val'])\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf_cli.run: \u001b[0mTorch version: 1.11.0+cu102\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf_cli.run: \u001b[0mUsing seed 2024\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/zeus/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/zeus/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/zeus/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at /home/zeus/.cache/huggingface/transformers/dfe8f1ad04cb25b61a647e3d13620f9bf0a0f51d277897b232a5735297134132.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2024-07-24T12:56:34 | torch.distributed.nn.jit.instantiator: \u001b[0mCreated a temporary directory at /tmp/tmpe259sirv\n",
      "\u001b[32m2024-07-24T12:56:34 | torch.distributed.nn.jit.instantiator: \u001b[0mWriting /tmp/tmpe259sirv/_remote_module_non_sriptable.py\n",
      "HI BRO {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': [{'type': 'label_smoothing_cross_entropy', 'params': {'label_smoothing': 0.1}}], 'model': 'visual_roberta'}\n",
      "SELFIE {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': [{'type': 'label_smoothing_cross_entropy', 'params': {'label_smoothing': 0.1}}], 'model': 'visual_roberta'}\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    {\n",
      "      \"params\": {\n",
      "        \"label_smoothing\": 0.1\n",
      "      },\n",
      "      \"type\": \"label_smoothing_cross_entropy\"\n",
      "    }\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "SELF ROBERT CONFIG RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    {\n",
      "      \"params\": {\n",
      "        \"label_smoothing\": 0.1\n",
      "      },\n",
      "      \"type\": \"label_smoothing_cross_entropy\"\n",
      "    }\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "roberta-base\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/zeus/.cache/torch/mmf/distributed_-1/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VisualRobertaBase: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing VisualRobertaBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualRobertaBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualRobertaBase were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'roberta.embeddings.position_embeddings_visual.weight', 'roberta.embeddings.projection.weight', 'roberta.embeddings.token_type_embeddings_visual.weight', 'roberta.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-07-24T12:56:42 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2024-07-24T12:56:42 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikrant17\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/wandb/run-20240724_125643-o9ljzyvo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhateful_memes_lr7e5_lb.1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/o9ljzyvo\u001b[0m\n",
      "\u001b[32m2024-07-24T12:56:46 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2024-07-24T12:56:46 | mmf.trainers.mmf_trainer: \u001b[0mVisualRoberta(\n",
      "  (model): VisualRobertaForClassification(\n",
      "    (roberta): VisualRobertaBase(\n",
      "      (embeddings): RobertaVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(1, 768)\n",
      "        (position_embeddings_visual): Embedding(514, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): LabelSmoothingCrossEntropyLoss()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2024-07-24T12:56:46 | mmf.utils.general: \u001b[0mTotal Parameters: 127208450. Trained Parameters: 127208450\n",
      "\u001b[32m2024-07-24T12:56:46 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2024-07-24T12:57:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.7760, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.7760, train/total_loss: 0.7760, train/total_loss/avg: 0.7760, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 10, iterations: 10, max_updates: 2000, lr: 0., ups: 0.34, time: 29s 246ms, time_since_start: 33s 455ms, eta: 01h 43m 47s 476ms\n",
      "\u001b[32m2024-07-24T12:57:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6741, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.7251, train/total_loss: 0.6741, train/total_loss/avg: 0.7251, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 20, iterations: 20, max_updates: 2000, lr: 0.00001, ups: 0.43, time: 23s 005ms, time_since_start: 56s 460ms, eta: 01h 21m 13s 881ms\n",
      "\u001b[32m2024-07-24T12:58:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 30/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6930, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.7144, train/total_loss: 0.6930, train/total_loss/avg: 0.7144, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 30, iterations: 30, max_updates: 2000, lr: 0.00001, ups: 0.36, time: 28s 464ms, time_since_start: 01m 24s 924ms, eta: 01h 39m 59s 944ms\n",
      "\u001b[32m2024-07-24T12:58:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 40/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6834, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.7066, train/total_loss: 0.6834, train/total_loss/avg: 0.7066, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 40, iterations: 40, max_updates: 2000, lr: 0.00001, ups: 0.48, time: 21s 237ms, time_since_start: 01m 46s 162ms, eta: 01h 14m 13s 868ms\n",
      "\u001b[32m2024-07-24T12:58:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6889, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.7031, train/total_loss: 0.6889, train/total_loss/avg: 0.7031, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 50, iterations: 50, max_updates: 2000, lr: 0.00002, ups: 0.42, time: 24s 646ms, time_since_start: 02m 10s 808ms, eta: 01h 25m 42s 536ms\n",
      "\u001b[32m2024-07-24T12:59:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 60/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6834, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6928, train/total_loss: 0.6834, train/total_loss/avg: 0.6928, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 60, iterations: 60, max_updates: 2000, lr: 0.00002, ups: 0.53, time: 19s 497ms, time_since_start: 02m 30s 305ms, eta: 01h 07m 27s 205ms\n",
      "\u001b[32m2024-07-24T12:59:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 70/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6889, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6934, train/total_loss: 0.6889, train/total_loss/avg: 0.6934, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 70, iterations: 70, max_updates: 2000, lr: 0.00002, ups: 0.36, time: 28s 443ms, time_since_start: 02m 58s 749ms, eta: 01h 37m 53s 945ms\n",
      "\u001b[32m2024-07-24T13:00:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 80/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6834, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6899, train/total_loss: 0.6834, train/total_loss/avg: 0.6899, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 80, iterations: 80, max_updates: 2000, lr: 0.00003, ups: 0.43, time: 23s 943ms, time_since_start: 03m 22s 693ms, eta: 01h 21m 58s 917ms\n",
      "\u001b[32m2024-07-24T13:00:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 90/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6834, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6883, train/total_loss: 0.6834, train/total_loss/avg: 0.6883, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 90, iterations: 90, max_updates: 2000, lr: 0.00003, ups: 0.36, time: 28s 669ms, time_since_start: 03m 51s 362ms, eta: 01h 37m 39s 094ms\n",
      "\u001b[32m2024-07-24T13:00:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6784, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6873, train/total_loss: 0.6784, train/total_loss/avg: 0.6873, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 100, iterations: 100, max_updates: 2000, lr: 0.00003, ups: 0.40, time: 25s 720ms, time_since_start: 04m 17s 083ms, eta: 01h 27m 09s 048ms\n",
      "\u001b[32m2024-07-24T13:00:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:00:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:01:26 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:01:26 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:01:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:01:30 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:01:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:01:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:01:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.6948, val/total_loss: 0.6948, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5358, num_updates: 100, epoch: 1, iterations: 100, max_updates: 2000, val_time: 51s 042ms, best_update: 100, best_iteration: 100, best_val/hateful_memes/roc_auc: 0.535776\n",
      "\u001b[32m2024-07-24T13:02:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 110/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6784, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6841, train/total_loss: 0.6784, train/total_loss/avg: 0.6841, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 110, iterations: 110, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 098ms, time_since_start: 05m 20s 232ms, eta: 40m 46s 736ms\n",
      "\u001b[32m2024-07-24T13:02:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 120/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6784, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6840, train/total_loss: 0.6784, train/total_loss/avg: 0.6840, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 120, iterations: 120, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 776ms, time_since_start: 05m 33s 008ms, eta: 42m 50s 116ms\n",
      "\u001b[32m2024-07-24T13:02:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 130/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6784, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6816, train/total_loss: 0.6784, train/total_loss/avg: 0.6816, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 130, iterations: 130, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 928ms, time_since_start: 05m 45s 936ms, eta: 43m 06s 796ms\n",
      "\u001b[32m2024-07-24T13:02:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 140/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6782, train/total_loss: 0.6758, train/total_loss/avg: 0.6782, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 140, iterations: 140, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 941ms, time_since_start: 05m 58s 878ms, eta: 42m 55s 668ms\n",
      "\u001b[32m2024-07-24T13:02:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6784, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6786, train/total_loss: 0.6784, train/total_loss/avg: 0.6786, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 150, iterations: 150, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 837ms, time_since_start: 06m 11s 715ms, eta: 42m 21s 102ms\n",
      "\u001b[32m2024-07-24T13:03:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 160/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6763, train/total_loss: 0.6758, train/total_loss/avg: 0.6763, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 160, iterations: 160, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 927ms, time_since_start: 06m 24s 643ms, eta: 42m 25s 131ms\n",
      "\u001b[32m2024-07-24T13:03:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 170/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6751, train/total_loss: 0.6758, train/total_loss/avg: 0.6751, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 170, iterations: 170, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 950ms, time_since_start: 06m 37s 593ms, eta: 42m 15s 788ms\n",
      "\u001b[32m2024-07-24T13:03:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 180/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6741, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6737, train/total_loss: 0.6741, train/total_loss/avg: 0.6737, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 180, iterations: 180, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 976ms, time_since_start: 06m 50s 569ms, eta: 42m 07s 003ms\n",
      "\u001b[32m2024-07-24T13:03:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 190/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6741, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6731, train/total_loss: 0.6741, train/total_loss/avg: 0.6731, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 190, iterations: 190, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 937ms, time_since_start: 07m 03s 507ms, eta: 41m 45s 581ms\n",
      "\u001b[32m2024-07-24T13:03:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6741, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6735, train/total_loss: 0.6741, train/total_loss/avg: 0.6735, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 200, iterations: 200, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 021ms, time_since_start: 07m 16s 528ms, eta: 41m 47s 851ms\n",
      "\u001b[32m2024-07-24T13:03:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:03:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:04:07 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:04:07 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:04:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:04:10 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:04:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:04:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:04:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.6986, val/total_loss: 0.6986, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5394, num_updates: 200, epoch: 2, iterations: 200, max_updates: 2000, val_time: 31s 837ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.539368\n",
      "\u001b[32m2024-07-24T13:04:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 210/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6740, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6735, train/total_loss: 0.6740, train/total_loss/avg: 0.6735, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 210, iterations: 210, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 991ms, time_since_start: 08m 01s 364ms, eta: 41m 28s 178ms\n",
      "\u001b[32m2024-07-24T13:04:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 220/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6740, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6741, train/total_loss: 0.6740, train/total_loss/avg: 0.6741, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 220, iterations: 220, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 489ms, time_since_start: 08m 13s 853ms, eta: 39m 38s 727ms\n",
      "\u001b[32m2024-07-24T13:05:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 230/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6740, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6746, train/total_loss: 0.6740, train/total_loss/avg: 0.6746, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 230, iterations: 230, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 592ms, time_since_start: 08m 26s 446ms, eta: 39m 44s 854ms\n",
      "\u001b[32m2024-07-24T13:05:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 240/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6655, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6742, train/total_loss: 0.6655, train/total_loss/avg: 0.6742, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 240, iterations: 240, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 614ms, time_since_start: 08m 40s 060ms, eta: 42m 43s 813ms\n",
      "\u001b[32m2024-07-24T13:05:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6655, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6738, train/total_loss: 0.6655, train/total_loss/avg: 0.6738, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 250, iterations: 250, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 675ms, time_since_start: 08m 52s 735ms, eta: 39m 33s 426ms\n",
      "\u001b[32m2024-07-24T13:05:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 260/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6655, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6729, train/total_loss: 0.6655, train/total_loss/avg: 0.6729, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 260, iterations: 260, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 833ms, time_since_start: 09m 06s 568ms, eta: 42m 55s 437ms\n",
      "\u001b[32m2024-07-24T13:06:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 270/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6639, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6721, train/total_loss: 0.6639, train/total_loss/avg: 0.6721, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 270, iterations: 270, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 771ms, time_since_start: 09m 19s 339ms, eta: 39m 24s 101ms\n",
      "\u001b[32m2024-07-24T13:06:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 280/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6632, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6712, train/total_loss: 0.6632, train/total_loss/avg: 0.6712, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 280, iterations: 280, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 775ms, time_since_start: 09m 33s 115ms, eta: 42m 15s 208ms\n",
      "\u001b[32m2024-07-24T13:06:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 290/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6632, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6713, train/total_loss: 0.6632, train/total_loss/avg: 0.6713, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 290, iterations: 290, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 889ms, time_since_start: 09m 46s 004ms, eta: 39m 18s 407ms\n",
      "\u001b[32m2024-07-24T13:06:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6550, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6703, train/total_loss: 0.6550, train/total_loss/avg: 0.6703, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 300, iterations: 300, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 838ms, time_since_start: 09m 59s 843ms, eta: 41m 57s 274ms\n",
      "\u001b[32m2024-07-24T13:06:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:06:42 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:06:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:06:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:06:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:06:54 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:07:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:07:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:07:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7083, val/total_loss: 0.7083, val/hateful_memes/accuracy: 0.5400, val/hateful_memes/binary_f1: 0.3429, val/hateful_memes/roc_auc: 0.5788, num_updates: 300, epoch: 3, iterations: 300, max_updates: 2000, val_time: 30s 929ms, best_update: 300, best_iteration: 300, best_val/hateful_memes/roc_auc: 0.578804\n",
      "\u001b[32m2024-07-24T13:07:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 310/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6550, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6685, train/total_loss: 0.6550, train/total_loss/avg: 0.6685, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 310, iterations: 310, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 283ms, time_since_start: 10m 44s 064ms, eta: 40m 02s 141ms\n",
      "\u001b[32m2024-07-24T13:07:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 320/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6517, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6678, train/total_loss: 0.6517, train/total_loss/avg: 0.6678, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 320, iterations: 320, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 853ms, time_since_start: 10m 56s 917ms, eta: 38m 30s 498ms\n",
      "\u001b[32m2024-07-24T13:07:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 330/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6506, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6672, train/total_loss: 0.6506, train/total_loss/avg: 0.6672, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 330, iterations: 330, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 429ms, time_since_start: 11m 09s 347ms, eta: 37m 01s 036ms\n",
      "\u001b[32m2024-07-24T13:08:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 340/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6526, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6668, train/total_loss: 0.6526, train/total_loss/avg: 0.6668, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 340, iterations: 340, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 955ms, time_since_start: 11m 22s 302ms, eta: 38m 21s 200ms\n",
      "\u001b[32m2024-07-24T13:08:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6506, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6652, train/total_loss: 0.6506, train/total_loss/avg: 0.6652, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 350, iterations: 350, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 750ms, time_since_start: 11m 35s 053ms, eta: 37m 31s 159ms\n",
      "\u001b[32m2024-07-24T13:08:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 360/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6526, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6653, train/total_loss: 0.6526, train/total_loss/avg: 0.6653, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 360, iterations: 360, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 860ms, time_since_start: 11m 47s 914ms, eta: 37m 36s 773ms\n",
      "\u001b[32m2024-07-24T13:08:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 370/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6506, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6630, train/total_loss: 0.6506, train/total_loss/avg: 0.6630, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 370, iterations: 370, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 687ms, time_since_start: 12m 601ms, eta: 36m 52s 767ms\n",
      "\u001b[32m2024-07-24T13:08:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 380/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6506, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6617, train/total_loss: 0.6506, train/total_loss/avg: 0.6617, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 380, iterations: 380, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 082ms, time_since_start: 12m 13s 683ms, eta: 37m 47s 682ms\n",
      "\u001b[32m2024-07-24T13:09:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 390/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6496, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6609, train/total_loss: 0.6496, train/total_loss/avg: 0.6609, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 390, iterations: 390, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 015ms, time_since_start: 12m 26s 698ms, eta: 37m 22s 120ms\n",
      "\u001b[32m2024-07-24T13:09:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6484, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6598, train/total_loss: 0.6484, train/total_loss/avg: 0.6598, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 400, iterations: 400, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 882ms, time_since_start: 12m 39s 581ms, eta: 36m 45s 516ms\n",
      "\u001b[32m2024-07-24T13:09:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:09:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:09:30 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:09:30 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:09:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:09:34 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:09:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:09:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:09:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7066, val/total_loss: 0.7066, val/hateful_memes/accuracy: 0.5920, val/hateful_memes/binary_f1: 0.5785, val/hateful_memes/roc_auc: 0.6231, num_updates: 400, epoch: 4, iterations: 400, max_updates: 2000, val_time: 33s 556ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.623056\n",
      "\u001b[32m2024-07-24T13:10:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 410/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6472, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6584, train/total_loss: 0.6472, train/total_loss/avg: 0.6584, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 410, iterations: 410, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 646ms, time_since_start: 13m 26s 792ms, eta: 38m 41s 696ms\n",
      "\u001b[32m2024-07-24T13:10:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 420/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6460, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6574, train/total_loss: 0.6460, train/total_loss/avg: 0.6574, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 420, iterations: 420, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 001ms, time_since_start: 13m 39s 794ms, eta: 36m 38s 015ms\n",
      "\u001b[32m2024-07-24T13:10:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 430/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6401, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6556, train/total_loss: 0.6401, train/total_loss/avg: 0.6556, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 430, iterations: 430, max_updates: 2000, lr: 0.00006, ups: 0.91, time: 11s 595ms, time_since_start: 13m 51s 390ms, eta: 32m 27s 994ms\n",
      "\u001b[32m2024-07-24T13:10:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 440/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6281, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6550, train/total_loss: 0.6281, train/total_loss/avg: 0.6550, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 440, iterations: 440, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 541ms, time_since_start: 14m 03s 931ms, eta: 34m 53s 349ms\n",
      "\u001b[32m2024-07-24T13:10:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6272, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6535, train/total_loss: 0.6272, train/total_loss/avg: 0.6535, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 450, iterations: 450, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 021ms, time_since_start: 14m 16s 952ms, eta: 35m 59s 617ms\n",
      "\u001b[32m2024-07-24T13:11:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 460/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6165, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6521, train/total_loss: 0.6165, train/total_loss/avg: 0.6521, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 460, iterations: 460, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 671ms, time_since_start: 14m 29s 623ms, eta: 34m 47s 937ms\n",
      "\u001b[32m2024-07-24T13:11:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 470/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6165, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6516, train/total_loss: 0.6165, train/total_loss/avg: 0.6516, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 470, iterations: 470, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 026ms, time_since_start: 14m 42s 650ms, eta: 35m 32s 590ms\n",
      "\u001b[32m2024-07-24T13:11:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 480/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6165, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6530, train/total_loss: 0.6165, train/total_loss/avg: 0.6530, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 480, iterations: 480, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 628ms, time_since_start: 14m 55s 278ms, eta: 34m 13s 866ms\n",
      "\u001b[32m2024-07-24T13:11:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 490/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6160, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6516, train/total_loss: 0.6160, train/total_loss/avg: 0.6516, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 490, iterations: 490, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 207ms, time_since_start: 15m 08s 486ms, eta: 35m 33s 946ms\n",
      "\u001b[32m2024-07-24T13:12:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6155, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6506, train/total_loss: 0.6155, train/total_loss/avg: 0.6506, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 500, iterations: 500, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 666ms, time_since_start: 15m 21s 153ms, eta: 33m 53s 006ms\n",
      "\u001b[32m2024-07-24T13:12:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:12:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:12:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:12:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:12:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:12:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:12:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:12:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7492, val/total_loss: 0.7492, val/hateful_memes/accuracy: 0.5720, val/hateful_memes/binary_f1: 0.4368, val/hateful_memes/roc_auc: 0.6207, num_updates: 500, epoch: 5, iterations: 500, max_updates: 2000, val_time: 21s 807ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.623056\n",
      "\u001b[32m2024-07-24T13:12:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 510/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6160, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6505, train/total_loss: 0.6160, train/total_loss/avg: 0.6505, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 510, iterations: 510, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 564ms, time_since_start: 15m 56s 532ms, eta: 36m 02s 549ms\n",
      "\u001b[32m2024-07-24T13:12:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 520/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6160, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6505, train/total_loss: 0.6160, train/total_loss/avg: 0.6505, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 520, iterations: 520, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 677ms, time_since_start: 16m 09s 209ms, eta: 33m 27s 543ms\n",
      "\u001b[32m2024-07-24T13:13:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 530/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6160, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6505, train/total_loss: 0.6160, train/total_loss/avg: 0.6505, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 530, iterations: 530, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 117ms, time_since_start: 16m 22s 327ms, eta: 34m 23s 305ms\n",
      "\u001b[32m2024-07-24T13:13:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 540/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6160, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6504, train/total_loss: 0.6160, train/total_loss/avg: 0.6504, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 540, iterations: 540, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 227ms, time_since_start: 16m 34s 555ms, eta: 31m 50s 205ms\n",
      "\u001b[32m2024-07-24T13:13:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6165, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6500, train/total_loss: 0.6165, train/total_loss/avg: 0.6500, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 550, iterations: 550, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 407ms, time_since_start: 16m 46s 962ms, eta: 32m 05s 059ms\n",
      "\u001b[32m2024-07-24T13:13:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 560/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6165, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6496, train/total_loss: 0.6165, train/total_loss/avg: 0.6496, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 560, iterations: 560, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 171ms, time_since_start: 17m 134ms, eta: 33m 49s 496ms\n",
      "\u001b[32m2024-07-24T13:13:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 570/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6492, train/total_loss: 0.6254, train/total_loss/avg: 0.6492, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 570, iterations: 570, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 346ms, time_since_start: 17m 12s 480ms, eta: 31m 29s 074ms\n",
      "\u001b[32m2024-07-24T13:14:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 580/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6480, train/total_loss: 0.6254, train/total_loss/avg: 0.6480, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 580, iterations: 580, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 339ms, time_since_start: 17m 25s 820ms, eta: 33m 46s 811ms\n",
      "\u001b[32m2024-07-24T13:14:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 590/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6193, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6475, train/total_loss: 0.6193, train/total_loss/avg: 0.6475, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 590, iterations: 590, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 516ms, time_since_start: 17m 38s 336ms, eta: 31m 28s 346ms\n",
      "\u001b[32m2024-07-24T13:14:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6472, train/total_loss: 0.6254, train/total_loss/avg: 0.6472, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 600, iterations: 600, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 503ms, time_since_start: 17m 51s 839ms, eta: 33m 42s 779ms\n",
      "\u001b[32m2024-07-24T13:14:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:14:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:14:42 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:14:42 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:14:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:14:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:14:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:14:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7273, val/total_loss: 0.7273, val/hateful_memes/accuracy: 0.5480, val/hateful_memes/binary_f1: 0.2614, val/hateful_memes/roc_auc: 0.6123, num_updates: 600, epoch: 6, iterations: 600, max_updates: 2000, val_time: 22s 787ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.623056\n",
      "\u001b[32m2024-07-24T13:15:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 610/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6463, train/total_loss: 0.6254, train/total_loss/avg: 0.6463, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 610, iterations: 610, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 898ms, time_since_start: 18m 27s 532ms, eta: 31m 58s 340ms\n",
      "\u001b[32m2024-07-24T13:15:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 620/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6261, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6462, train/total_loss: 0.6261, train/total_loss/avg: 0.6462, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 620, iterations: 620, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 569ms, time_since_start: 18m 41s 102ms, eta: 33m 23s 713ms\n",
      "\u001b[32m2024-07-24T13:15:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 630/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6261, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6452, train/total_loss: 0.6261, train/total_loss/avg: 0.6452, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 630, iterations: 630, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 573ms, time_since_start: 18m 53s 676ms, eta: 30m 43s 156ms\n",
      "\u001b[32m2024-07-24T13:15:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 640/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6261, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6453, train/total_loss: 0.6261, train/total_loss/avg: 0.6453, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 640, iterations: 640, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 485ms, time_since_start: 19m 06s 161ms, eta: 30m 16s 840ms\n",
      "\u001b[32m2024-07-24T13:16:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6261, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6442, train/total_loss: 0.6261, train/total_loss/avg: 0.6442, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 650, iterations: 650, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 115ms, time_since_start: 19m 18s 277ms, eta: 29m 10s 134ms\n",
      "\u001b[32m2024-07-24T13:16:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 660/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6263, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6440, train/total_loss: 0.6263, train/total_loss/avg: 0.6440, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 660, iterations: 660, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 809ms, time_since_start: 19m 32s 086ms, eta: 33m 041ms\n",
      "\u001b[32m2024-07-24T13:16:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 670/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6261, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6431, train/total_loss: 0.6261, train/total_loss/avg: 0.6431, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 670, iterations: 670, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 661ms, time_since_start: 19m 44s 747ms, eta: 30m 01s 795ms\n",
      "\u001b[32m2024-07-24T13:16:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 680/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6416, train/total_loss: 0.6254, train/total_loss/avg: 0.6416, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 680, iterations: 680, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 801ms, time_since_start: 19m 58s 549ms, eta: 32m 29s 377ms\n",
      "\u001b[32m2024-07-24T13:16:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 690/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6403, train/total_loss: 0.6254, train/total_loss/avg: 0.6403, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 690, iterations: 690, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 690ms, time_since_start: 20m 11s 240ms, eta: 29m 38s 819ms\n",
      "\u001b[32m2024-07-24T13:17:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6391, train/total_loss: 0.6254, train/total_loss/avg: 0.6391, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 700, iterations: 700, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 689ms, time_since_start: 20m 24s 929ms, eta: 31m 44s 174ms\n",
      "\u001b[32m2024-07-24T13:17:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:17:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:17:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:17:16 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:17:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:17:19 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:17:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:17:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:17:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7438, val/total_loss: 0.7438, val/hateful_memes/accuracy: 0.5820, val/hateful_memes/binary_f1: 0.4079, val/hateful_memes/roc_auc: 0.6531, num_updates: 700, epoch: 7, iterations: 700, max_updates: 2000, val_time: 33s 222ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.653120\n",
      "\u001b[32m2024-07-24T13:17:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 710/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6395, train/total_loss: 0.6254, train/total_loss/avg: 0.6395, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 710, iterations: 710, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 543ms, time_since_start: 21m 11s 702ms, eta: 31m 09s 372ms\n",
      "\u001b[32m2024-07-24T13:18:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 720/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6193, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6386, train/total_loss: 0.6193, train/total_loss/avg: 0.6386, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 720, iterations: 720, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 698ms, time_since_start: 21m 25s 401ms, eta: 31m 16s 148ms\n",
      "\u001b[32m2024-07-24T13:18:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 730/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6040, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6381, train/total_loss: 0.6040, train/total_loss/avg: 0.6381, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 730, iterations: 730, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 786ms, time_since_start: 21m 38s 187ms, eta: 28m 57s 546ms\n",
      "\u001b[32m2024-07-24T13:18:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 740/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5939, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6372, train/total_loss: 0.5939, train/total_loss/avg: 0.6372, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 740, iterations: 740, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 623ms, time_since_start: 21m 51s 811ms, eta: 30m 36s 674ms\n",
      "\u001b[32m2024-07-24T13:18:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5838, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6360, train/total_loss: 0.5838, train/total_loss/avg: 0.6360, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 750, iterations: 750, max_updates: 2000, lr: 0.00005, ups: 0.91, time: 11s 854ms, time_since_start: 22m 03s 665ms, eta: 26m 25s 577ms\n",
      "\u001b[32m2024-07-24T13:18:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 760/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5838, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6354, train/total_loss: 0.5838, train/total_loss/avg: 0.6354, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 760, iterations: 760, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 017ms, time_since_start: 22m 16s 682ms, eta: 28m 47s 114ms\n",
      "\u001b[32m2024-07-24T13:19:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 770/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5829, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6346, train/total_loss: 0.5829, train/total_loss/avg: 0.6346, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 770, iterations: 770, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 403ms, time_since_start: 22m 30s 086ms, eta: 29m 23s 995ms\n",
      "\u001b[32m2024-07-24T13:19:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 780/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5829, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6338, train/total_loss: 0.5829, train/total_loss/avg: 0.6338, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 780, iterations: 780, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 852ms, time_since_start: 22m 42s 938ms, eta: 27m 57s 801ms\n",
      "\u001b[32m2024-07-24T13:19:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 790/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5768, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6322, train/total_loss: 0.5768, train/total_loss/avg: 0.6322, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 790, iterations: 790, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 461ms, time_since_start: 22m 56s 400ms, eta: 29m 02s 854ms\n",
      "\u001b[32m2024-07-24T13:19:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5768, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6318, train/total_loss: 0.5768, train/total_loss/avg: 0.6318, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 800, iterations: 800, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 057ms, time_since_start: 23m 09s 457ms, eta: 27m 56s 562ms\n",
      "\u001b[32m2024-07-24T13:19:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:19:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:19:59 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:19:59 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:20:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:20:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:20:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:20:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7178, val/total_loss: 0.7178, val/hateful_memes/accuracy: 0.5920, val/hateful_memes/binary_f1: 0.5660, val/hateful_memes/roc_auc: 0.6520, num_updates: 800, epoch: 8, iterations: 800, max_updates: 2000, val_time: 21s 903ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.653120\n",
      "\u001b[32m2024-07-24T13:20:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 810/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6311, train/total_loss: 0.5758, train/total_loss/avg: 0.6311, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 810, iterations: 810, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 736ms, time_since_start: 23m 45s 104ms, eta: 29m 09s 014ms\n",
      "\u001b[32m2024-07-24T13:20:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 820/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6307, train/total_loss: 0.5758, train/total_loss/avg: 0.6307, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 820, iterations: 820, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 818ms, time_since_start: 23m 57s 923ms, eta: 26m 58s 459ms\n",
      "\u001b[32m2024-07-24T13:20:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 830/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6303, train/total_loss: 0.5758, train/total_loss/avg: 0.6303, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 830, iterations: 830, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 489ms, time_since_start: 24m 11s 412ms, eta: 28m 08s 735ms\n",
      "\u001b[32m2024-07-24T13:21:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 840/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5751, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6293, train/total_loss: 0.5751, train/total_loss/avg: 0.6293, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 840, iterations: 840, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 043ms, time_since_start: 24m 24s 456ms, eta: 26m 59s 012ms\n",
      "\u001b[32m2024-07-24T13:21:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5734, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6284, train/total_loss: 0.5734, train/total_loss/avg: 0.6284, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 850, iterations: 850, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 354ms, time_since_start: 24m 37s 810ms, eta: 27m 23s 274ms\n",
      "\u001b[32m2024-07-24T13:21:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 860/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5705, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6275, train/total_loss: 0.5705, train/total_loss/avg: 0.6275, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 860, iterations: 860, max_updates: 2000, lr: 0.00004, ups: 0.91, time: 11s 556ms, time_since_start: 24m 49s 367ms, eta: 23m 29s 713ms\n",
      "\u001b[32m2024-07-24T13:21:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 870/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5704, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6264, train/total_loss: 0.5704, train/total_loss/avg: 0.6264, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 870, iterations: 870, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 990ms, time_since_start: 25m 02s 358ms, eta: 26m 10s 734ms\n",
      "\u001b[32m2024-07-24T13:21:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 880/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5704, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6254, train/total_loss: 0.5704, train/total_loss/avg: 0.6254, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 880, iterations: 880, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 905ms, time_since_start: 25m 15s 264ms, eta: 25m 46s 576ms\n",
      "\u001b[32m2024-07-24T13:22:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 890/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5704, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6247, train/total_loss: 0.5704, train/total_loss/avg: 0.6247, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 890, iterations: 890, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 991ms, time_since_start: 25m 28s 255ms, eta: 25m 43s 030ms\n",
      "\u001b[32m2024-07-24T13:22:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5704, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6239, train/total_loss: 0.5704, train/total_loss/avg: 0.6239, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 900, iterations: 900, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 166ms, time_since_start: 25m 41s 422ms, eta: 25m 49s 716ms\n",
      "\u001b[32m2024-07-24T13:22:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:22:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:22:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:22:32 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:22:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:22:36 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:22:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:22:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:22:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7499, val/total_loss: 0.7499, val/hateful_memes/accuracy: 0.5860, val/hateful_memes/binary_f1: 0.5012, val/hateful_memes/roc_auc: 0.6587, num_updates: 900, epoch: 9, iterations: 900, max_updates: 2000, val_time: 33s 512ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.658736\n",
      "\u001b[32m2024-07-24T13:23:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 910/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5692, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6222, train/total_loss: 0.5692, train/total_loss/avg: 0.6222, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 910, iterations: 910, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 349ms, time_since_start: 26m 28s 292ms, eta: 25m 56s 919ms\n",
      "\u001b[32m2024-07-24T13:23:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 920/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5514, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6213, train/total_loss: 0.5514, train/total_loss/avg: 0.6213, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 920, iterations: 920, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 125ms, time_since_start: 26m 41s 417ms, eta: 25m 16s 786ms\n",
      "\u001b[32m2024-07-24T13:23:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 930/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5513, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6203, train/total_loss: 0.5513, train/total_loss/avg: 0.6203, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 930, iterations: 930, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 970ms, time_since_start: 26m 54s 387ms, eta: 24m 44s 949ms\n",
      "\u001b[32m2024-07-24T13:23:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 940/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5513, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6197, train/total_loss: 0.5513, train/total_loss/avg: 0.6197, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 940, iterations: 940, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 010ms, time_since_start: 27m 07s 398ms, eta: 24m 35s 624ms\n",
      "\u001b[32m2024-07-24T13:24:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5514, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6191, train/total_loss: 0.5514, train/total_loss/avg: 0.6191, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 950, iterations: 950, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 012ms, time_since_start: 27m 20s 410ms, eta: 24m 21s 982ms\n",
      "\u001b[32m2024-07-24T13:24:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 960/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5513, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6181, train/total_loss: 0.5513, train/total_loss/avg: 0.6181, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 960, iterations: 960, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 775ms, time_since_start: 27m 33s 186ms, eta: 23m 41s 709ms\n",
      "\u001b[32m2024-07-24T13:24:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 970/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5444, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6165, train/total_loss: 0.5444, train/total_loss/avg: 0.6165, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 970, iterations: 970, max_updates: 2000, lr: 0.00004, ups: 0.91, time: 11s 774ms, time_since_start: 27m 44s 961ms, eta: 21m 37s 696ms\n",
      "\u001b[32m2024-07-24T13:24:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 980/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5441, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6152, train/total_loss: 0.5441, train/total_loss/avg: 0.6152, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 980, iterations: 980, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 228ms, time_since_start: 27m 58s 190ms, eta: 24m 03s 785ms\n",
      "\u001b[32m2024-07-24T13:24:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 990/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5441, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6140, train/total_loss: 0.5441, train/total_loss/avg: 0.6140, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 990, iterations: 990, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 848ms, time_since_start: 28m 11s 038ms, eta: 23m 08s 486ms\n",
      "\u001b[32m2024-07-24T13:25:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-24T13:25:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:25:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:25:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:25:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5441, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6134, train/total_loss: 0.5441, train/total_loss/avg: 0.6134, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1000, iterations: 1000, max_updates: 2000, lr: 0.00004, ups: 0.37, time: 27s 670ms, time_since_start: 28m 38s 709ms, eta: 49m 20s 767ms\n",
      "\u001b[32m2024-07-24T13:25:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:25:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:25:29 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:25:29 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:25:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:25:36 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:25:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:25:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:25:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7385, val/total_loss: 0.7385, val/hateful_memes/accuracy: 0.6060, val/hateful_memes/binary_f1: 0.5343, val/hateful_memes/roc_auc: 0.6612, num_updates: 1000, epoch: 10, iterations: 1000, max_updates: 2000, val_time: 33s 043ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.661152\n",
      "\u001b[32m2024-07-24T13:26:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1010/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5420, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6127, train/total_loss: 0.5420, train/total_loss/avg: 0.6127, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1010, iterations: 1010, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 047ms, time_since_start: 29m 24s 802ms, eta: 23m 02s 136ms\n",
      "\u001b[32m2024-07-24T13:26:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1020/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5385, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6119, train/total_loss: 0.5385, train/total_loss/avg: 0.6119, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1020, iterations: 1020, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 296ms, time_since_start: 29m 38s 099ms, eta: 23m 14s 317ms\n",
      "\u001b[32m2024-07-24T13:26:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1030/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5370, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6105, train/total_loss: 0.5370, train/total_loss/avg: 0.6105, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1030, iterations: 1030, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 905ms, time_since_start: 29m 51s 004ms, eta: 22m 19s 423ms\n",
      "\u001b[32m2024-07-24T13:26:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1040/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5370, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6103, train/total_loss: 0.5370, train/total_loss/avg: 0.6103, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1040, iterations: 1040, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 278ms, time_since_start: 30m 04s 282ms, eta: 22m 43s 950ms\n",
      "\u001b[32m2024-07-24T13:26:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5370, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6099, train/total_loss: 0.5370, train/total_loss/avg: 0.6099, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1050, iterations: 1050, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 756ms, time_since_start: 30m 17s 039ms, eta: 21m 36s 719ms\n",
      "\u001b[32m2024-07-24T13:27:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1060/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5297, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6090, train/total_loss: 0.5297, train/total_loss/avg: 0.6090, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1060, iterations: 1060, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 147ms, time_since_start: 30m 30s 186ms, eta: 22m 02s 331ms\n",
      "\u001b[32m2024-07-24T13:27:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1070/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5370, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6091, train/total_loss: 0.5370, train/total_loss/avg: 0.6091, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1070, iterations: 1070, max_updates: 2000, lr: 0.00004, ups: 0.91, time: 11s 290ms, time_since_start: 30m 41s 477ms, eta: 18m 43s 507ms\n",
      "\u001b[32m2024-07-24T13:27:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1080/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5297, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6083, train/total_loss: 0.5297, train/total_loss/avg: 0.6083, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1080, iterations: 1080, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 804ms, time_since_start: 30m 55s 281ms, eta: 22m 38s 900ms\n",
      "\u001b[32m2024-07-24T13:27:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1090/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5297, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6076, train/total_loss: 0.5297, train/total_loss/avg: 0.6076, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1090, iterations: 1090, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 756ms, time_since_start: 31m 08s 037ms, eta: 20m 42s 100ms\n",
      "\u001b[32m2024-07-24T13:28:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5276, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6061, train/total_loss: 0.5276, train/total_loss/avg: 0.6061, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1100, iterations: 1100, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 992ms, time_since_start: 31m 22s 030ms, eta: 22m 27s 468ms\n",
      "\u001b[32m2024-07-24T13:28:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:28:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:28:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:28:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:28:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:28:16 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:28:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:28:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:28:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7320, val/total_loss: 0.7320, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.5605, val/hateful_memes/roc_auc: 0.6633, num_updates: 1100, epoch: 11, iterations: 1100, max_updates: 2000, val_time: 32s 835ms, best_update: 1100, best_iteration: 1100, best_val/hateful_memes/roc_auc: 0.663296\n",
      "\u001b[32m2024-07-24T13:28:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1110/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5276, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6053, train/total_loss: 0.5276, train/total_loss/avg: 0.6053, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1110, iterations: 1110, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 312ms, time_since_start: 32m 08s 188ms, eta: 21m 07s 762ms\n",
      "\u001b[32m2024-07-24T13:29:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1120/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5251, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6045, train/total_loss: 0.5251, train/total_loss/avg: 0.6045, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1120, iterations: 1120, max_updates: 2000, lr: 0.00003, ups: 0.71, time: 14s 050ms, time_since_start: 32m 22s 239ms, eta: 22m 03s 028ms\n",
      "\u001b[32m2024-07-24T13:29:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1130/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5251, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6037, train/total_loss: 0.5251, train/total_loss/avg: 0.6037, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1130, iterations: 1130, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 809ms, time_since_start: 32m 35s 048ms, eta: 19m 52s 420ms\n",
      "\u001b[32m2024-07-24T13:29:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1140/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5251, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6030, train/total_loss: 0.5251, train/total_loss/avg: 0.6030, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1140, iterations: 1140, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 810ms, time_since_start: 32m 48s 858ms, eta: 21m 10s 815ms\n",
      "\u001b[32m2024-07-24T13:29:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5184, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6022, train/total_loss: 0.5184, train/total_loss/avg: 0.6022, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1150, iterations: 1150, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 705ms, time_since_start: 33m 01s 564ms, eta: 19m 15s 568ms\n",
      "\u001b[32m2024-07-24T13:29:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1160/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5184, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6018, train/total_loss: 0.5184, train/total_loss/avg: 0.6018, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1160, iterations: 1160, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 770ms, time_since_start: 33m 15s 334ms, eta: 20m 37s 662ms\n",
      "\u001b[32m2024-07-24T13:30:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1170/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5184, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6010, train/total_loss: 0.5184, train/total_loss/avg: 0.6010, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1170, iterations: 1170, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 766ms, time_since_start: 33m 28s 100ms, eta: 18m 53s 750ms\n",
      "\u001b[32m2024-07-24T13:30:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1180/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5184, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5996, train/total_loss: 0.5184, train/total_loss/avg: 0.5996, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1180, iterations: 1180, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 335ms, time_since_start: 33m 40s 435ms, eta: 18m 02s 293ms\n",
      "\u001b[32m2024-07-24T13:30:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1190/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5184, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5985, train/total_loss: 0.5184, train/total_loss/avg: 0.5985, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1190, iterations: 1190, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 947ms, time_since_start: 33m 54s 383ms, eta: 20m 08s 822ms\n",
      "\u001b[32m2024-07-24T13:30:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5149, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5971, train/total_loss: 0.5149, train/total_loss/avg: 0.5971, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1200, iterations: 1200, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 333ms, time_since_start: 34m 07s 716ms, eta: 19m 01s 369ms\n",
      "\u001b[32m2024-07-24T13:30:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:30:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:30:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:30:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:30:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:31:02 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:31:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:31:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:31:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8504, val/total_loss: 0.8504, val/hateful_memes/accuracy: 0.5820, val/hateful_memes/binary_f1: 0.4367, val/hateful_memes/roc_auc: 0.6638, num_updates: 1200, epoch: 12, iterations: 1200, max_updates: 2000, val_time: 31s 566ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.663840\n",
      "\u001b[32m2024-07-24T13:31:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1210/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5115, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5957, train/total_loss: 0.5115, train/total_loss/avg: 0.5957, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1210, iterations: 1210, max_updates: 2000, lr: 0.00003, ups: 0.71, time: 14s 562ms, time_since_start: 34m 53s 854ms, eta: 20m 30s 946ms\n",
      "\u001b[32m2024-07-24T13:31:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1220/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5112, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5943, train/total_loss: 0.5112, train/total_loss/avg: 0.5943, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1220, iterations: 1220, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 345ms, time_since_start: 35m 07s 199ms, eta: 18m 33s 851ms\n",
      "\u001b[32m2024-07-24T13:32:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1230/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5112, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5933, train/total_loss: 0.5112, train/total_loss/avg: 0.5933, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1230, iterations: 1230, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 949ms, time_since_start: 35m 21s 149ms, eta: 19m 09s 324ms\n",
      "\u001b[32m2024-07-24T13:32:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1240/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5105, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5923, train/total_loss: 0.5105, train/total_loss/avg: 0.5923, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1240, iterations: 1240, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 462ms, time_since_start: 35m 34s 612ms, eta: 18m 14s 803ms\n",
      "\u001b[32m2024-07-24T13:32:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5081, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5913, train/total_loss: 0.5081, train/total_loss/avg: 0.5913, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1250, iterations: 1250, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 916ms, time_since_start: 35m 48s 529ms, eta: 18m 36s 815ms\n",
      "\u001b[32m2024-07-24T13:32:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1260/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4936, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5905, train/total_loss: 0.4936, train/total_loss/avg: 0.5905, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1260, iterations: 1260, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 494ms, time_since_start: 36m 02s 023ms, eta: 17m 48s 483ms\n",
      "\u001b[32m2024-07-24T13:32:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1270/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4713, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5895, train/total_loss: 0.4713, train/total_loss/avg: 0.5895, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1270, iterations: 1270, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 893ms, time_since_start: 36m 15s 916ms, eta: 18m 05s 194ms\n",
      "\u001b[32m2024-07-24T13:33:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1280/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4697, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5885, train/total_loss: 0.4697, train/total_loss/avg: 0.5885, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1280, iterations: 1280, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 258ms, time_since_start: 36m 29s 175ms, eta: 17m 01s 420ms\n",
      "\u001b[32m2024-07-24T13:33:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1290/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4695, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5872, train/total_loss: 0.4695, train/total_loss/avg: 0.5872, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1290, iterations: 1290, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 222ms, time_since_start: 36m 41s 397ms, eta: 15m 28s 547ms\n",
      "\u001b[32m2024-07-24T13:33:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4695, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5859, train/total_loss: 0.4695, train/total_loss/avg: 0.5859, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1300, iterations: 1300, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 923ms, time_since_start: 36m 54s 321ms, eta: 16m 07s 943ms\n",
      "\u001b[32m2024-07-24T13:33:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:33:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:33:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:33:45 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:33:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:33:48 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:34:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:34:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:34:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8187, val/total_loss: 0.8187, val/hateful_memes/accuracy: 0.5980, val/hateful_memes/binary_f1: 0.4885, val/hateful_memes/roc_auc: 0.6647, num_updates: 1300, epoch: 13, iterations: 1300, max_updates: 2000, val_time: 41s 224ms, best_update: 1300, best_iteration: 1300, best_val/hateful_memes/roc_auc: 0.664704\n",
      "\u001b[32m2024-07-24T13:34:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1310/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4682, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5848, train/total_loss: 0.4682, train/total_loss/avg: 0.5848, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1310, iterations: 1310, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 553ms, time_since_start: 37m 49s 106ms, eta: 16m 40s 658ms\n",
      "\u001b[32m2024-07-24T13:34:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1320/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4639, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5833, train/total_loss: 0.4639, train/total_loss/avg: 0.5833, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1320, iterations: 1320, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 805ms, time_since_start: 38m 01s 912ms, eta: 15m 31s 754ms\n",
      "\u001b[32m2024-07-24T13:34:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1330/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4639, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5825, train/total_loss: 0.4639, train/total_loss/avg: 0.5825, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1330, iterations: 1330, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 205ms, time_since_start: 38m 15s 117ms, eta: 15m 46s 703ms\n",
      "\u001b[32m2024-07-24T13:35:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1340/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4639, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5817, train/total_loss: 0.4639, train/total_loss/avg: 0.5817, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1340, iterations: 1340, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 712ms, time_since_start: 38m 27s 829ms, eta: 14m 57s 730ms\n",
      "\u001b[32m2024-07-24T13:35:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4595, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5808, train/total_loss: 0.4595, train/total_loss/avg: 0.5808, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1350, iterations: 1350, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 279ms, time_since_start: 38m 41s 108ms, eta: 15m 23s 560ms\n",
      "\u001b[32m2024-07-24T13:35:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1360/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4595, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5800, train/total_loss: 0.4595, train/total_loss/avg: 0.5800, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1360, iterations: 1360, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 766ms, time_since_start: 38m 53s 875ms, eta: 14m 34s 261ms\n",
      "\u001b[32m2024-07-24T13:35:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1370/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4576, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5790, train/total_loss: 0.4576, train/total_loss/avg: 0.5790, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1370, iterations: 1370, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 283ms, time_since_start: 39m 07s 158ms, eta: 14m 55s 428ms\n",
      "\u001b[32m2024-07-24T13:36:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1380/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4595, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5782, train/total_loss: 0.4595, train/total_loss/avg: 0.5782, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1380, iterations: 1380, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 749ms, time_since_start: 39m 19s 908ms, eta: 14m 05s 804ms\n",
      "\u001b[32m2024-07-24T13:36:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1390/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4576, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5770, train/total_loss: 0.4576, train/total_loss/avg: 0.5770, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1390, iterations: 1390, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 551ms, time_since_start: 39m 32s 460ms, eta: 13m 39s 266ms\n",
      "\u001b[32m2024-07-24T13:36:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4576, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5760, train/total_loss: 0.4576, train/total_loss/avg: 0.5760, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1400, iterations: 1400, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 811ms, time_since_start: 39m 45s 271ms, eta: 13m 42s 481ms\n",
      "\u001b[32m2024-07-24T13:36:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:36:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:36:36 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:36:36 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:36:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:36:40 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:36:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:37:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:37:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8292, val/total_loss: 0.8292, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.5200, val/hateful_memes/roc_auc: 0.6734, num_updates: 1400, epoch: 14, iterations: 1400, max_updates: 2000, val_time: 32s 709ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.673392\n",
      "\u001b[32m2024-07-24T13:37:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1410/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4595, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5755, train/total_loss: 0.4595, train/total_loss/avg: 0.5755, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1410, iterations: 1410, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 594ms, time_since_start: 40m 30s 582ms, eta: 13m 15s 066ms\n",
      "\u001b[32m2024-07-24T13:37:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1420/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4595, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5744, train/total_loss: 0.4595, train/total_loss/avg: 0.5744, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1420, iterations: 1420, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 030ms, time_since_start: 40m 43s 613ms, eta: 13m 28s 685ms\n",
      "\u001b[32m2024-07-24T13:37:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1430/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4576, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5734, train/total_loss: 0.4576, train/total_loss/avg: 0.5734, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1430, iterations: 1430, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 338ms, time_since_start: 40m 55s 951ms, eta: 12m 32s 515ms\n",
      "\u001b[32m2024-07-24T13:37:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1440/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4463, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5722, train/total_loss: 0.4463, train/total_loss/avg: 0.5722, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1440, iterations: 1440, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 123ms, time_since_start: 41m 09s 075ms, eta: 13m 06s 358ms\n",
      "\u001b[32m2024-07-24T13:38:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4448, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5713, train/total_loss: 0.4448, train/total_loss/avg: 0.5713, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1450, iterations: 1450, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 316ms, time_since_start: 41m 21s 392ms, eta: 12m 04s 852ms\n",
      "\u001b[32m2024-07-24T13:38:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1460/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4420, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5702, train/total_loss: 0.4420, train/total_loss/avg: 0.5702, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1460, iterations: 1460, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 150ms, time_since_start: 41m 34s 542ms, eta: 12m 39s 834ms\n",
      "\u001b[32m2024-07-24T13:38:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1470/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4420, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5694, train/total_loss: 0.4420, train/total_loss/avg: 0.5694, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1470, iterations: 1470, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 314ms, time_since_start: 41m 46s 857ms, eta: 11m 38s 354ms\n",
      "\u001b[32m2024-07-24T13:38:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1480/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4348, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5685, train/total_loss: 0.4348, train/total_loss/avg: 0.5685, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1480, iterations: 1480, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 134ms, time_since_start: 41m 59s 991ms, eta: 12m 10s 784ms\n",
      "\u001b[32m2024-07-24T13:38:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1490/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4348, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5675, train/total_loss: 0.4348, train/total_loss/avg: 0.5675, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1490, iterations: 1490, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 336ms, time_since_start: 42m 12s 328ms, eta: 11m 13s 227ms\n",
      "\u001b[32m2024-07-24T13:39:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4348, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5665, train/total_loss: 0.4348, train/total_loss/avg: 0.5665, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1500, iterations: 1500, max_updates: 2000, lr: 0.00002, ups: 0.91, time: 11s 593ms, time_since_start: 42m 23s 921ms, eta: 10m 20s 244ms\n",
      "\u001b[32m2024-07-24T13:39:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:39:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:39:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:39:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:39:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:39:18 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:39:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:39:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:39:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8263, val/total_loss: 0.8263, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.5026, val/hateful_memes/roc_auc: 0.6749, num_updates: 1500, epoch: 15, iterations: 1500, max_updates: 2000, val_time: 32s 284ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:39:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1510/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4348, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5658, train/total_loss: 0.4348, train/total_loss/avg: 0.5658, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1510, iterations: 1510, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 031ms, time_since_start: 43m 09s 246ms, eta: 11m 23s 262ms\n",
      "\u001b[32m2024-07-24T13:40:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1520/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4348, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5647, train/total_loss: 0.4348, train/total_loss/avg: 0.5647, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1520, iterations: 1520, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 838ms, time_since_start: 43m 23s 085ms, eta: 11m 50s 771ms\n",
      "\u001b[32m2024-07-24T13:40:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1530/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4325, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5637, train/total_loss: 0.4325, train/total_loss/avg: 0.5637, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1530, iterations: 1530, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 795ms, time_since_start: 43m 35s 880ms, eta: 10m 43s 496ms\n",
      "\u001b[32m2024-07-24T13:40:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1540/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4303, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5626, train/total_loss: 0.4303, train/total_loss/avg: 0.5626, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1540, iterations: 1540, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 207ms, time_since_start: 43m 49s 088ms, eta: 10m 50s 096ms\n",
      "\u001b[32m2024-07-24T13:40:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4226, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5614, train/total_loss: 0.4226, train/total_loss/avg: 0.5614, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1550, iterations: 1550, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 725ms, time_since_start: 44m 01s 814ms, eta: 10m 12s 755ms\n",
      "\u001b[32m2024-07-24T13:40:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1560/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4197, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5603, train/total_loss: 0.4197, train/total_loss/avg: 0.5603, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1560, iterations: 1560, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 191ms, time_since_start: 44m 15s 006ms, eta: 10m 21s 059ms\n",
      "\u001b[32m2024-07-24T13:41:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1570/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4189, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5593, train/total_loss: 0.4189, train/total_loss/avg: 0.5593, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1570, iterations: 1570, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 600ms, time_since_start: 44m 27s 607ms, eta: 09m 39s 756ms\n",
      "\u001b[32m2024-07-24T13:41:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1580/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4189, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5582, train/total_loss: 0.4189, train/total_loss/avg: 0.5582, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1580, iterations: 1580, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 082ms, time_since_start: 44m 40s 689ms, eta: 09m 47s 914ms\n",
      "\u001b[32m2024-07-24T13:41:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1590/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4189, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5574, train/total_loss: 0.4189, train/total_loss/avg: 0.5574, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1590, iterations: 1590, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 752ms, time_since_start: 44m 53s 441ms, eta: 09m 19s 448ms\n",
      "\u001b[32m2024-07-24T13:41:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4189, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5567, train/total_loss: 0.4189, train/total_loss/avg: 0.5567, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1600, iterations: 1600, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 151ms, time_since_start: 45m 06s 592ms, eta: 09m 22s 864ms\n",
      "\u001b[32m2024-07-24T13:41:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:41:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:41:57 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:41:57 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:41:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:42:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:42:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:42:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8351, val/total_loss: 0.8351, val/hateful_memes/accuracy: 0.6200, val/hateful_memes/binary_f1: 0.5226, val/hateful_memes/roc_auc: 0.6653, num_updates: 1600, epoch: 15, iterations: 1600, max_updates: 2000, val_time: 22s 520ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:42:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1610/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4189, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5556, train/total_loss: 0.4189, train/total_loss/avg: 0.5556, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1610, iterations: 1610, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 975ms, time_since_start: 45m 42s 096ms, eta: 09m 01s 477ms\n",
      "\u001b[32m2024-07-24T13:42:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1620/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4104, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5543, train/total_loss: 0.4104, train/total_loss/avg: 0.5543, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1620, iterations: 1620, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 280ms, time_since_start: 45m 55s 376ms, eta: 08m 59s 976ms\n",
      "\u001b[32m2024-07-24T13:42:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1630/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4097, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5534, train/total_loss: 0.4097, train/total_loss/avg: 0.5534, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1630, iterations: 1630, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 819ms, time_since_start: 46m 09s 196ms, eta: 09m 07s 100ms\n",
      "\u001b[32m2024-07-24T13:43:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1640/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4097, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5525, train/total_loss: 0.4097, train/total_loss/avg: 0.5525, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1640, iterations: 1640, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 306ms, time_since_start: 46m 22s 502ms, eta: 08m 32s 556ms\n",
      "\u001b[32m2024-07-24T13:43:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4087, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5514, train/total_loss: 0.4087, train/total_loss/avg: 0.5514, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1650, iterations: 1650, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 907ms, time_since_start: 46m 36s 409ms, eta: 08m 40s 834ms\n",
      "\u001b[32m2024-07-24T13:43:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1660/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4087, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5506, train/total_loss: 0.4087, train/total_loss/avg: 0.5506, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1660, iterations: 1660, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 417ms, time_since_start: 46m 49s 827ms, eta: 08m 08s 136ms\n",
      "\u001b[32m2024-07-24T13:43:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1670/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4034, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5496, train/total_loss: 0.4034, train/total_loss/avg: 0.5496, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1670, iterations: 1670, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 850ms, time_since_start: 47m 03s 678ms, eta: 08m 09s 076ms\n",
      "\u001b[32m2024-07-24T13:43:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1680/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4034, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5488, train/total_loss: 0.4034, train/total_loss/avg: 0.5488, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1680, iterations: 1680, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 362ms, time_since_start: 47m 17s 041ms, eta: 07m 37s 543ms\n",
      "\u001b[32m2024-07-24T13:44:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1690/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4034, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5480, train/total_loss: 0.4034, train/total_loss/avg: 0.5480, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1690, iterations: 1690, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 897ms, time_since_start: 47m 30s 939ms, eta: 07m 40s 987ms\n",
      "\u001b[32m2024-07-24T13:44:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4034, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5472, train/total_loss: 0.4034, train/total_loss/avg: 0.5472, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1700, iterations: 1700, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 515ms, time_since_start: 47m 44s 454ms, eta: 07m 13s 832ms\n",
      "\u001b[32m2024-07-24T13:44:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:44:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:44:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:44:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:44:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:44:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:44:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:44:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8456, val/total_loss: 0.8456, val/hateful_memes/accuracy: 0.6280, val/hateful_memes/binary_f1: 0.5463, val/hateful_memes/roc_auc: 0.6729, num_updates: 1700, epoch: 16, iterations: 1700, max_updates: 2000, val_time: 21s 958ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:45:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1710/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4034, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5465, train/total_loss: 0.4034, train/total_loss/avg: 0.5465, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1710, iterations: 1710, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 640ms, time_since_start: 48m 20s 060ms, eta: 07m 03s 257ms\n",
      "\u001b[32m2024-07-24T13:45:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1720/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5456, train/total_loss: 0.4056, train/total_loss/avg: 0.5456, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1720, iterations: 1720, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 809ms, time_since_start: 48m 32s 870ms, eta: 06m 23s 771ms\n",
      "\u001b[32m2024-07-24T13:45:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1730/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5450, train/total_loss: 0.4056, train/total_loss/avg: 0.5450, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1730, iterations: 1730, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 042ms, time_since_start: 48m 46s 913ms, eta: 06m 45s 698ms\n",
      "\u001b[32m2024-07-24T13:45:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1740/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5440, train/total_loss: 0.4056, train/total_loss/avg: 0.5440, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1740, iterations: 1740, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 152ms, time_since_start: 49m 065ms, eta: 06m 05s 905ms\n",
      "\u001b[32m2024-07-24T13:45:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5432, train/total_loss: 0.4056, train/total_loss/avg: 0.5432, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1750, iterations: 1750, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 133ms, time_since_start: 49m 14s 199ms, eta: 06m 18s 083ms\n",
      "\u001b[32m2024-07-24T13:46:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1760/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5421, train/total_loss: 0.4056, train/total_loss/avg: 0.5421, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1760, iterations: 1760, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 956ms, time_since_start: 49m 27s 156ms, eta: 05m 32s 734ms\n",
      "\u001b[32m2024-07-24T13:46:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1770/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5413, train/total_loss: 0.4056, train/total_loss/avg: 0.5413, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1770, iterations: 1770, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 993ms, time_since_start: 49m 41s 150ms, eta: 05m 44s 375ms\n",
      "\u001b[32m2024-07-24T13:46:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1780/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4084, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5406, train/total_loss: 0.4084, train/total_loss/avg: 0.5406, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1780, iterations: 1780, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 889ms, time_since_start: 49m 54s 039ms, eta: 05m 03s 425ms\n",
      "\u001b[32m2024-07-24T13:46:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1790/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4058, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5398, train/total_loss: 0.4058, train/total_loss/avg: 0.5398, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1790, iterations: 1790, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 367ms, time_since_start: 50m 08s 407ms, eta: 05m 22s 844ms\n",
      "\u001b[32m2024-07-24T13:47:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5389, train/total_loss: 0.4056, train/total_loss/avg: 0.5389, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1800, iterations: 1800, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 969ms, time_since_start: 50m 21s 377ms, eta: 04m 37s 553ms\n",
      "\u001b[32m2024-07-24T13:47:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:47:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:47:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:47:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:47:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:47:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:47:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:47:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8776, val/total_loss: 0.8776, val/hateful_memes/accuracy: 0.6260, val/hateful_memes/binary_f1: 0.5383, val/hateful_memes/roc_auc: 0.6558, num_updates: 1800, epoch: 17, iterations: 1800, max_updates: 2000, val_time: 23s 613ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:47:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1810/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4058, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5382, train/total_loss: 0.4058, train/total_loss/avg: 0.5382, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1810, iterations: 1810, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 345ms, time_since_start: 50m 59s 344ms, eta: 04m 51s 645ms\n",
      "\u001b[32m2024-07-24T13:47:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1820/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4058, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5374, train/total_loss: 0.4058, train/total_loss/avg: 0.5374, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1820, iterations: 1820, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 825ms, time_since_start: 51m 12s 170ms, eta: 04m 07s 022ms\n",
      "\u001b[32m2024-07-24T13:48:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1830/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5365, train/total_loss: 0.4056, train/total_loss/avg: 0.5365, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1830, iterations: 1830, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 489ms, time_since_start: 51m 24s 660ms, eta: 03m 47s 191ms\n",
      "\u001b[32m2024-07-24T13:48:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1840/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4058, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5360, train/total_loss: 0.4058, train/total_loss/avg: 0.5360, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1840, iterations: 1840, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 859ms, time_since_start: 51m 38s 519ms, eta: 03m 57s 280ms\n",
      "\u001b[32m2024-07-24T13:48:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4058, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5351, train/total_loss: 0.4058, train/total_loss/avg: 0.5351, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1850, iterations: 1850, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 531ms, time_since_start: 51m 51s 051ms, eta: 03m 21s 136ms\n",
      "\u001b[32m2024-07-24T13:48:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1860/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5341, train/total_loss: 0.4056, train/total_loss/avg: 0.5341, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1860, iterations: 1860, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 712ms, time_since_start: 52m 04s 764ms, eta: 03m 25s 409ms\n",
      "\u001b[32m2024-07-24T13:48:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1870/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5332, train/total_loss: 0.4056, train/total_loss/avg: 0.5332, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1870, iterations: 1870, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 677ms, time_since_start: 52m 17s 442ms, eta: 02m 56s 351ms\n",
      "\u001b[32m2024-07-24T13:49:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1880/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3997, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5324, train/total_loss: 0.3997, train/total_loss/avg: 0.5324, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1880, iterations: 1880, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 773ms, time_since_start: 52m 31s 215ms, eta: 02m 56s 849ms\n",
      "\u001b[32m2024-07-24T13:49:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1890/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3945, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5314, train/total_loss: 0.3945, train/total_loss/avg: 0.5314, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1890, iterations: 1890, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 083ms, time_since_start: 52m 44s 299ms, eta: 02m 33s 997ms\n",
      "\u001b[32m2024-07-24T13:49:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3882, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5307, train/total_loss: 0.3882, train/total_loss/avg: 0.5307, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1900, iterations: 1900, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 717ms, time_since_start: 52m 58s 016ms, eta: 02m 26s 773ms\n",
      "\u001b[32m2024-07-24T13:49:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:49:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:49:49 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:49:49 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:49:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:49:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:49:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:49:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.9125, val/total_loss: 0.9125, val/hateful_memes/accuracy: 0.6120, val/hateful_memes/binary_f1: 0.4921, val/hateful_memes/roc_auc: 0.6482, num_updates: 1900, epoch: 18, iterations: 1900, max_updates: 2000, val_time: 16s 402ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:50:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1910/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3825, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5299, train/total_loss: 0.3825, train/total_loss/avg: 0.5299, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1910, iterations: 1910, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 454ms, time_since_start: 53m 27s 880ms, eta: 02m 09s 563ms\n",
      "\u001b[32m2024-07-24T13:50:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1920/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3786, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5291, train/total_loss: 0.3786, train/total_loss/avg: 0.5291, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1920, iterations: 1920, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 747ms, time_since_start: 53m 41s 628ms, eta: 01m 57s 679ms\n",
      "\u001b[32m2024-07-24T13:50:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1930/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3783, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5282, train/total_loss: 0.3783, train/total_loss/avg: 0.5282, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1930, iterations: 1930, max_updates: 2000, lr: 0., ups: 0.91, time: 11s 345ms, time_since_start: 53m 52s 974ms, eta: 01m 24s 980ms\n",
      "\u001b[32m2024-07-24T13:50:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1940/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3773, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5273, train/total_loss: 0.3773, train/total_loss/avg: 0.5273, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1940, iterations: 1940, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 789ms, time_since_start: 54m 06s 764ms, eta: 01m 28s 531ms\n",
      "\u001b[32m2024-07-24T13:51:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3772, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5266, train/total_loss: 0.3772, train/total_loss/avg: 0.5266, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1950, iterations: 1950, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 832ms, time_since_start: 54m 19s 596ms, eta: 01m 08s 655ms\n",
      "\u001b[32m2024-07-24T13:51:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1960/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3772, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5258, train/total_loss: 0.3772, train/total_loss/avg: 0.5258, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1960, iterations: 1960, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 793ms, time_since_start: 54m 33s 390ms, eta: 59s 037ms\n",
      "\u001b[32m2024-07-24T13:51:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1970/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3752, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5249, train/total_loss: 0.3752, train/total_loss/avg: 0.5249, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1970, iterations: 1970, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 773ms, time_since_start: 54m 46s 164ms, eta: 41s 002ms\n",
      "\u001b[32m2024-07-24T13:51:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1980/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3752, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5243, train/total_loss: 0.3752, train/total_loss/avg: 0.5243, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1980, iterations: 1980, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 636ms, time_since_start: 54m 59s 800ms, eta: 29s 181ms\n",
      "\u001b[32m2024-07-24T13:51:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1990/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3749, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5233, train/total_loss: 0.3749, train/total_loss/avg: 0.5233, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1990, iterations: 1990, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 712ms, time_since_start: 55m 12s 512ms, eta: 13s 602ms\n",
      "\u001b[32m2024-07-24T13:52:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-24T13:52:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:52:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:52:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:52:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3709, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5225, train/total_loss: 0.3709, train/total_loss/avg: 0.5225, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 2000, iterations: 2000, max_updates: 2000, lr: 0., ups: 0.37, time: 27s 653ms, time_since_start: 55m 40s 166ms, eta: 0ms\n",
      "\u001b[32m2024-07-24T13:52:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:52:22 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:52:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:52:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:52:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:52:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:52:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:52:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.9113, val/total_loss: 0.9113, val/hateful_memes/accuracy: 0.6280, val/hateful_memes/binary_f1: 0.5206, val/hateful_memes/roc_auc: 0.6488, num_updates: 2000, epoch: 19, iterations: 2000, max_updates: 2000, val_time: 27s 922ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:52:50 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2024-07-24T13:52:50 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2024-07-24T13:52:50 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2024-07-24T13:53:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2024-07-24T13:53:02 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1500\n",
      "\u001b[32m2024-07-24T13:53:02 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1500\n",
      "\u001b[32m2024-07-24T13:53:02 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 15\n",
      "\u001b[32m2024-07-24T13:53:04 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2024-07-24T13:53:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:08<00:00,  1.22s/it]\n",
      "\u001b[32m2024-07-24T13:53:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:53:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:53:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8263, val/total_loss: 0.8263, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.5026, val/hateful_memes/roc_auc: 0.6749\n",
      "\u001b[32m2024-07-24T13:53:13 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 56m 30s 908ms\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.147 MB of 0.147 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/label_smoothing_cross_entropy █▇█▇█▇▇█▇▆▇▇▇▅▆▆▆▅▅▄▅▅▅▃▃▃▄▄▂▃▂▂▂▂▂▂▃▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               train/learning_rate ▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                  train/total_loss █▇█▇█▇▇█▇▆▇▇▇▅▆▆▆▅▅▄▅▅▅▃▃▃▄▄▂▃▂▂▂▂▂▂▃▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        val/hateful_memes/accuracy ▁▁▃▆▅▄▅▆▆▇▇▅▆▇▇███▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       val/hateful_memes/binary_f1 ▁▁▅█▆▄▆█▇▇█▆▇▇▇▇██▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/label_smoothing_cross_entropy ▃▃▂▃▅▃▅▂▃▃▁▄▂▃▇▄▇▁█▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         val/hateful_memes/roc_auc ▁▁▃▅▅▅▇▇▇▇▇▇▇████▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    val/total_loss ▃▃▂▃▅▃▅▂▃▃▁▄▂▃▇▄▇▁█▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/label_smoothing_cross_entropy 0.34889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                  train/total_loss 0.34889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               trainer/global_step 1500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        val/hateful_memes/accuracy 0.616\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       val/hateful_memes/binary_f1 0.50259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/label_smoothing_cross_entropy 0.93743\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         val/hateful_memes/roc_auc 0.67488\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    val/total_loss 0.93743\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhateful_memes_lr7e5_lb.1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/o9ljzyvo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240724_125643-o9ljzyvo/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "Exception ignored in: <function WandbLogger.__del__ at 0x7feed7152f70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf/utils/logger.py\", line 457, in __del__\n",
      "    self._wandb.finish()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 4262, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 390, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2100, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2108, in _finish\n",
      "    tel.feature.finish = True\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
      "    self._run._telemetry_callback(self._obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 798, in _telemetry_callback\n",
      "    self._telemetry_flush()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 809, in _telemetry_flush\n",
      "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 101, in _publish_telemetry\n",
      "    self._publish(rec)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_roberta/defaults.yaml \\\n",
    "model=visual_roberta \\\n",
    "dataset=hateful_memes \\\n",
    "training.log_interval=10 \\\n",
    "training.seed=2024 \\\n",
    "training.batch_size=80 \\\n",
    "training.evaluation_interval=100 \\\n",
    "training.experiment_name=hateful_memes_lr7e5_lb.1 \\\n",
    "training.max_updates=2000 \\\n",
    "optimizer.params.lr=7e-5 \\\n",
    "training.fp16=True \\\n",
    "scheduler.params.num_warmup_steps=200 \\\n",
    "checkpoint.max_to_keep=1 \\\n",
    "training.wandb.enabled=True \\\n",
    "run_type=train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_roberta/defaults.yaml\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_roberta\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 10\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.seed to 2024\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 80\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 100\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.experiment_name to hateful_memes_lr7e5_lb.1\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 2000\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 7e-5\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.fp16 to True\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 200\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option training.wandb.enabled to True\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_roberta/defaults.yaml', 'model=visual_roberta', 'dataset=hateful_memes', 'training.log_interval=10', 'training.seed=2024', 'training.batch_size=80', 'training.evaluation_interval=100', 'training.experiment_name=hateful_memes_lr7e5_lb.1', 'training.max_updates=2000', 'optimizer.params.lr=7e-5', 'training.fp16=True', 'scheduler.params.num_warmup_steps=200', 'checkpoint.max_to_keep=1', 'training.wandb.enabled=True', 'run_type=train_val'])\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf_cli.run: \u001b[0mTorch version: 1.11.0+cu102\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf_cli.run: \u001b[0mUsing seed 2024\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/zeus/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/zeus/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/zeus/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at /home/zeus/.cache/huggingface/transformers/dfe8f1ad04cb25b61a647e3d13620f9bf0a0f51d277897b232a5735297134132.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zeus/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-24T12:56:34 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2024-07-24T12:56:34 | torch.distributed.nn.jit.instantiator: \u001b[0mCreated a temporary directory at /tmp/tmpe259sirv\n",
      "\u001b[32m2024-07-24T12:56:34 | torch.distributed.nn.jit.instantiator: \u001b[0mWriting /tmp/tmpe259sirv/_remote_module_non_sriptable.py\n",
      "HI BRO {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': [{'type': 'label_smoothing_cross_entropy', 'params': {'label_smoothing': 0.1}}], 'model': 'visual_roberta'}\n",
      "SELFIE {'roberta_model_name': 'roberta-base', 'training_head_type': 'classification', 'visual_embedding_dim': 2048, 'special_visual_initialize': True, 'embedding_strategy': 'plain', 'bypass_transformer': False, 'output_attentions': False, 'max_position_embeddings': 514, 'output_hidden_states': False, 'random_initialize': False, 'freeze_base': False, 'finetune_lr_multiplier': 1, 'type_vocab_size': 1, 'pooler_strategy': 'default', 'vocab_size': 50265, 'zerobias': False, 'num_labels': 2, 'losses': [{'type': 'label_smoothing_cross_entropy', 'params': {'label_smoothing': 0.1}}], 'model': 'visual_roberta'}\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    {\n",
      "      \"params\": {\n",
      "        \"label_smoothing\": 0.1\n",
      "      },\n",
      "      \"type\": \"label_smoothing_cross_entropy\"\n",
      "    }\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "SELF ROBERT CONFIG RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    {\n",
      "      \"params\": {\n",
      "        \"label_smoothing\": 0.1\n",
      "      },\n",
      "      \"type\": \"label_smoothing_cross_entropy\"\n",
      "    }\n",
      "  ],\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model\": \"visual_roberta\",\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"roberta_model_name\": \"roberta-base\",\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 50265,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "roberta-base\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/zeus/.cache/torch/mmf/distributed_-1/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VisualRobertaBase: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing VisualRobertaBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualRobertaBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualRobertaBase were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'roberta.embeddings.position_embeddings_visual.weight', 'roberta.embeddings.projection.weight', 'roberta.embeddings.token_type_embeddings_visual.weight', 'roberta.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-07-24T12:56:42 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2024-07-24T12:56:42 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikrant17\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/wandb/run-20240724_125643-o9ljzyvo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhateful_memes_lr7e5_lb.1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/o9ljzyvo\u001b[0m\n",
      "\u001b[32m2024-07-24T12:56:46 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2024-07-24T12:56:46 | mmf.trainers.mmf_trainer: \u001b[0mVisualRoberta(\n",
      "  (model): VisualRobertaForClassification(\n",
      "    (roberta): VisualRobertaBase(\n",
      "      (embeddings): RobertaVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(1, 768)\n",
      "        (position_embeddings_visual): Embedding(514, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): LabelSmoothingCrossEntropyLoss()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2024-07-24T12:56:46 | mmf.utils.general: \u001b[0mTotal Parameters: 127208450. Trained Parameters: 127208450\n",
      "\u001b[32m2024-07-24T12:56:46 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2024-07-24T12:57:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.7760, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.7760, train/total_loss: 0.7760, train/total_loss/avg: 0.7760, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 10, iterations: 10, max_updates: 2000, lr: 0., ups: 0.34, time: 29s 246ms, time_since_start: 33s 455ms, eta: 01h 43m 47s 476ms\n",
      "\u001b[32m2024-07-24T12:57:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6741, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.7251, train/total_loss: 0.6741, train/total_loss/avg: 0.7251, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 20, iterations: 20, max_updates: 2000, lr: 0.00001, ups: 0.43, time: 23s 005ms, time_since_start: 56s 460ms, eta: 01h 21m 13s 881ms\n",
      "\u001b[32m2024-07-24T12:58:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 30/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6930, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.7144, train/total_loss: 0.6930, train/total_loss/avg: 0.7144, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 30, iterations: 30, max_updates: 2000, lr: 0.00001, ups: 0.36, time: 28s 464ms, time_since_start: 01m 24s 924ms, eta: 01h 39m 59s 944ms\n",
      "\u001b[32m2024-07-24T12:58:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 40/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6834, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.7066, train/total_loss: 0.6834, train/total_loss/avg: 0.7066, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 40, iterations: 40, max_updates: 2000, lr: 0.00001, ups: 0.48, time: 21s 237ms, time_since_start: 01m 46s 162ms, eta: 01h 14m 13s 868ms\n",
      "\u001b[32m2024-07-24T12:58:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6889, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.7031, train/total_loss: 0.6889, train/total_loss/avg: 0.7031, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 50, iterations: 50, max_updates: 2000, lr: 0.00002, ups: 0.42, time: 24s 646ms, time_since_start: 02m 10s 808ms, eta: 01h 25m 42s 536ms\n",
      "\u001b[32m2024-07-24T12:59:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 60/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6834, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6928, train/total_loss: 0.6834, train/total_loss/avg: 0.6928, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 60, iterations: 60, max_updates: 2000, lr: 0.00002, ups: 0.53, time: 19s 497ms, time_since_start: 02m 30s 305ms, eta: 01h 07m 27s 205ms\n",
      "\u001b[32m2024-07-24T12:59:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 70/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6889, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6934, train/total_loss: 0.6889, train/total_loss/avg: 0.6934, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 70, iterations: 70, max_updates: 2000, lr: 0.00002, ups: 0.36, time: 28s 443ms, time_since_start: 02m 58s 749ms, eta: 01h 37m 53s 945ms\n",
      "\u001b[32m2024-07-24T13:00:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 80/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6834, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6899, train/total_loss: 0.6834, train/total_loss/avg: 0.6899, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 80, iterations: 80, max_updates: 2000, lr: 0.00003, ups: 0.43, time: 23s 943ms, time_since_start: 03m 22s 693ms, eta: 01h 21m 58s 917ms\n",
      "\u001b[32m2024-07-24T13:00:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 90/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6834, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6883, train/total_loss: 0.6834, train/total_loss/avg: 0.6883, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 90, iterations: 90, max_updates: 2000, lr: 0.00003, ups: 0.36, time: 28s 669ms, time_since_start: 03m 51s 362ms, eta: 01h 37m 39s 094ms\n",
      "\u001b[32m2024-07-24T13:00:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6784, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6873, train/total_loss: 0.6784, train/total_loss/avg: 0.6873, max mem: 13359.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 1, num_updates: 100, iterations: 100, max_updates: 2000, lr: 0.00003, ups: 0.40, time: 25s 720ms, time_since_start: 04m 17s 083ms, eta: 01h 27m 09s 048ms\n",
      "\u001b[32m2024-07-24T13:00:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:00:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:01:26 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:01:26 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:01:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:01:30 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:01:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:01:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:01:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.6948, val/total_loss: 0.6948, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5358, num_updates: 100, epoch: 1, iterations: 100, max_updates: 2000, val_time: 51s 042ms, best_update: 100, best_iteration: 100, best_val/hateful_memes/roc_auc: 0.535776\n",
      "\u001b[32m2024-07-24T13:02:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 110/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6784, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6841, train/total_loss: 0.6784, train/total_loss/avg: 0.6841, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 110, iterations: 110, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 098ms, time_since_start: 05m 20s 232ms, eta: 40m 46s 736ms\n",
      "\u001b[32m2024-07-24T13:02:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 120/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6784, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6840, train/total_loss: 0.6784, train/total_loss/avg: 0.6840, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 120, iterations: 120, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 776ms, time_since_start: 05m 33s 008ms, eta: 42m 50s 116ms\n",
      "\u001b[32m2024-07-24T13:02:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 130/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6784, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6816, train/total_loss: 0.6784, train/total_loss/avg: 0.6816, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 130, iterations: 130, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 928ms, time_since_start: 05m 45s 936ms, eta: 43m 06s 796ms\n",
      "\u001b[32m2024-07-24T13:02:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 140/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6782, train/total_loss: 0.6758, train/total_loss/avg: 0.6782, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 140, iterations: 140, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 941ms, time_since_start: 05m 58s 878ms, eta: 42m 55s 668ms\n",
      "\u001b[32m2024-07-24T13:02:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6784, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6786, train/total_loss: 0.6784, train/total_loss/avg: 0.6786, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 150, iterations: 150, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 837ms, time_since_start: 06m 11s 715ms, eta: 42m 21s 102ms\n",
      "\u001b[32m2024-07-24T13:03:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 160/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6763, train/total_loss: 0.6758, train/total_loss/avg: 0.6763, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 160, iterations: 160, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 927ms, time_since_start: 06m 24s 643ms, eta: 42m 25s 131ms\n",
      "\u001b[32m2024-07-24T13:03:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 170/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6751, train/total_loss: 0.6758, train/total_loss/avg: 0.6751, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 170, iterations: 170, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 950ms, time_since_start: 06m 37s 593ms, eta: 42m 15s 788ms\n",
      "\u001b[32m2024-07-24T13:03:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 180/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6741, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6737, train/total_loss: 0.6741, train/total_loss/avg: 0.6737, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 180, iterations: 180, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 976ms, time_since_start: 06m 50s 569ms, eta: 42m 07s 003ms\n",
      "\u001b[32m2024-07-24T13:03:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 190/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6741, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6731, train/total_loss: 0.6741, train/total_loss/avg: 0.6731, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 190, iterations: 190, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 937ms, time_since_start: 07m 03s 507ms, eta: 41m 45s 581ms\n",
      "\u001b[32m2024-07-24T13:03:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6741, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6735, train/total_loss: 0.6741, train/total_loss/avg: 0.6735, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 200, iterations: 200, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 021ms, time_since_start: 07m 16s 528ms, eta: 41m 47s 851ms\n",
      "\u001b[32m2024-07-24T13:03:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:03:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:04:07 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:04:07 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:04:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:04:10 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:04:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:04:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:04:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.6986, val/total_loss: 0.6986, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5394, num_updates: 200, epoch: 2, iterations: 200, max_updates: 2000, val_time: 31s 837ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.539368\n",
      "\u001b[32m2024-07-24T13:04:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 210/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6740, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6735, train/total_loss: 0.6740, train/total_loss/avg: 0.6735, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 2, num_updates: 210, iterations: 210, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 991ms, time_since_start: 08m 01s 364ms, eta: 41m 28s 178ms\n",
      "\u001b[32m2024-07-24T13:04:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 220/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6740, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6741, train/total_loss: 0.6740, train/total_loss/avg: 0.6741, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 220, iterations: 220, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 489ms, time_since_start: 08m 13s 853ms, eta: 39m 38s 727ms\n",
      "\u001b[32m2024-07-24T13:05:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 230/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6740, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6746, train/total_loss: 0.6740, train/total_loss/avg: 0.6746, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 230, iterations: 230, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 592ms, time_since_start: 08m 26s 446ms, eta: 39m 44s 854ms\n",
      "\u001b[32m2024-07-24T13:05:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 240/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6655, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6742, train/total_loss: 0.6655, train/total_loss/avg: 0.6742, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 240, iterations: 240, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 614ms, time_since_start: 08m 40s 060ms, eta: 42m 43s 813ms\n",
      "\u001b[32m2024-07-24T13:05:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6655, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6738, train/total_loss: 0.6655, train/total_loss/avg: 0.6738, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 250, iterations: 250, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 675ms, time_since_start: 08m 52s 735ms, eta: 39m 33s 426ms\n",
      "\u001b[32m2024-07-24T13:05:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 260/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6655, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6729, train/total_loss: 0.6655, train/total_loss/avg: 0.6729, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 260, iterations: 260, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 833ms, time_since_start: 09m 06s 568ms, eta: 42m 55s 437ms\n",
      "\u001b[32m2024-07-24T13:06:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 270/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6639, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6721, train/total_loss: 0.6639, train/total_loss/avg: 0.6721, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 270, iterations: 270, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 771ms, time_since_start: 09m 19s 339ms, eta: 39m 24s 101ms\n",
      "\u001b[32m2024-07-24T13:06:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 280/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6632, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6712, train/total_loss: 0.6632, train/total_loss/avg: 0.6712, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 280, iterations: 280, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 775ms, time_since_start: 09m 33s 115ms, eta: 42m 15s 208ms\n",
      "\u001b[32m2024-07-24T13:06:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 290/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6632, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6713, train/total_loss: 0.6632, train/total_loss/avg: 0.6713, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 290, iterations: 290, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 889ms, time_since_start: 09m 46s 004ms, eta: 39m 18s 407ms\n",
      "\u001b[32m2024-07-24T13:06:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6550, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6703, train/total_loss: 0.6550, train/total_loss/avg: 0.6703, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 300, iterations: 300, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 838ms, time_since_start: 09m 59s 843ms, eta: 41m 57s 274ms\n",
      "\u001b[32m2024-07-24T13:06:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:06:42 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:06:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:06:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:06:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:06:54 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:07:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:07:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:07:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7083, val/total_loss: 0.7083, val/hateful_memes/accuracy: 0.5400, val/hateful_memes/binary_f1: 0.3429, val/hateful_memes/roc_auc: 0.5788, num_updates: 300, epoch: 3, iterations: 300, max_updates: 2000, val_time: 30s 929ms, best_update: 300, best_iteration: 300, best_val/hateful_memes/roc_auc: 0.578804\n",
      "\u001b[32m2024-07-24T13:07:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 310/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6550, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6685, train/total_loss: 0.6550, train/total_loss/avg: 0.6685, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 310, iterations: 310, max_updates: 2000, lr: 0.00007, ups: 0.77, time: 13s 283ms, time_since_start: 10m 44s 064ms, eta: 40m 02s 141ms\n",
      "\u001b[32m2024-07-24T13:07:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 320/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6517, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6678, train/total_loss: 0.6517, train/total_loss/avg: 0.6678, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 3, num_updates: 320, iterations: 320, max_updates: 2000, lr: 0.00007, ups: 0.83, time: 12s 853ms, time_since_start: 10m 56s 917ms, eta: 38m 30s 498ms\n",
      "\u001b[32m2024-07-24T13:07:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 330/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6506, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6672, train/total_loss: 0.6506, train/total_loss/avg: 0.6672, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 330, iterations: 330, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 429ms, time_since_start: 11m 09s 347ms, eta: 37m 01s 036ms\n",
      "\u001b[32m2024-07-24T13:08:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 340/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6526, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6668, train/total_loss: 0.6526, train/total_loss/avg: 0.6668, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 340, iterations: 340, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 955ms, time_since_start: 11m 22s 302ms, eta: 38m 21s 200ms\n",
      "\u001b[32m2024-07-24T13:08:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6506, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6652, train/total_loss: 0.6506, train/total_loss/avg: 0.6652, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 350, iterations: 350, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 750ms, time_since_start: 11m 35s 053ms, eta: 37m 31s 159ms\n",
      "\u001b[32m2024-07-24T13:08:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 360/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6526, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6653, train/total_loss: 0.6526, train/total_loss/avg: 0.6653, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 360, iterations: 360, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 860ms, time_since_start: 11m 47s 914ms, eta: 37m 36s 773ms\n",
      "\u001b[32m2024-07-24T13:08:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 370/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6506, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6630, train/total_loss: 0.6506, train/total_loss/avg: 0.6630, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 370, iterations: 370, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 687ms, time_since_start: 12m 601ms, eta: 36m 52s 767ms\n",
      "\u001b[32m2024-07-24T13:08:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 380/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6506, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6617, train/total_loss: 0.6506, train/total_loss/avg: 0.6617, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 380, iterations: 380, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 082ms, time_since_start: 12m 13s 683ms, eta: 37m 47s 682ms\n",
      "\u001b[32m2024-07-24T13:09:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 390/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6496, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6609, train/total_loss: 0.6496, train/total_loss/avg: 0.6609, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 390, iterations: 390, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 015ms, time_since_start: 12m 26s 698ms, eta: 37m 22s 120ms\n",
      "\u001b[32m2024-07-24T13:09:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6484, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6598, train/total_loss: 0.6484, train/total_loss/avg: 0.6598, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 400, iterations: 400, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 882ms, time_since_start: 12m 39s 581ms, eta: 36m 45s 516ms\n",
      "\u001b[32m2024-07-24T13:09:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:09:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:09:30 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:09:30 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:09:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:09:34 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:09:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:09:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:09:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7066, val/total_loss: 0.7066, val/hateful_memes/accuracy: 0.5920, val/hateful_memes/binary_f1: 0.5785, val/hateful_memes/roc_auc: 0.6231, num_updates: 400, epoch: 4, iterations: 400, max_updates: 2000, val_time: 33s 556ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.623056\n",
      "\u001b[32m2024-07-24T13:10:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 410/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6472, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6584, train/total_loss: 0.6472, train/total_loss/avg: 0.6584, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 410, iterations: 410, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 646ms, time_since_start: 13m 26s 792ms, eta: 38m 41s 696ms\n",
      "\u001b[32m2024-07-24T13:10:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 420/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6460, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6574, train/total_loss: 0.6460, train/total_loss/avg: 0.6574, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 4, num_updates: 420, iterations: 420, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 001ms, time_since_start: 13m 39s 794ms, eta: 36m 38s 015ms\n",
      "\u001b[32m2024-07-24T13:10:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 430/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6401, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6556, train/total_loss: 0.6401, train/total_loss/avg: 0.6556, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 430, iterations: 430, max_updates: 2000, lr: 0.00006, ups: 0.91, time: 11s 595ms, time_since_start: 13m 51s 390ms, eta: 32m 27s 994ms\n",
      "\u001b[32m2024-07-24T13:10:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 440/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6281, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6550, train/total_loss: 0.6281, train/total_loss/avg: 0.6550, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 440, iterations: 440, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 541ms, time_since_start: 14m 03s 931ms, eta: 34m 53s 349ms\n",
      "\u001b[32m2024-07-24T13:10:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6272, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6535, train/total_loss: 0.6272, train/total_loss/avg: 0.6535, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 450, iterations: 450, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 021ms, time_since_start: 14m 16s 952ms, eta: 35m 59s 617ms\n",
      "\u001b[32m2024-07-24T13:11:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 460/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6165, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6521, train/total_loss: 0.6165, train/total_loss/avg: 0.6521, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 460, iterations: 460, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 671ms, time_since_start: 14m 29s 623ms, eta: 34m 47s 937ms\n",
      "\u001b[32m2024-07-24T13:11:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 470/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6165, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6516, train/total_loss: 0.6165, train/total_loss/avg: 0.6516, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 470, iterations: 470, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 026ms, time_since_start: 14m 42s 650ms, eta: 35m 32s 590ms\n",
      "\u001b[32m2024-07-24T13:11:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 480/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6165, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6530, train/total_loss: 0.6165, train/total_loss/avg: 0.6530, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 480, iterations: 480, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 628ms, time_since_start: 14m 55s 278ms, eta: 34m 13s 866ms\n",
      "\u001b[32m2024-07-24T13:11:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 490/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6160, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6516, train/total_loss: 0.6160, train/total_loss/avg: 0.6516, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 490, iterations: 490, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 207ms, time_since_start: 15m 08s 486ms, eta: 35m 33s 946ms\n",
      "\u001b[32m2024-07-24T13:12:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6155, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6506, train/total_loss: 0.6155, train/total_loss/avg: 0.6506, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 500, iterations: 500, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 666ms, time_since_start: 15m 21s 153ms, eta: 33m 53s 006ms\n",
      "\u001b[32m2024-07-24T13:12:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:12:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:12:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:12:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:12:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:12:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:12:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:12:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7492, val/total_loss: 0.7492, val/hateful_memes/accuracy: 0.5720, val/hateful_memes/binary_f1: 0.4368, val/hateful_memes/roc_auc: 0.6207, num_updates: 500, epoch: 5, iterations: 500, max_updates: 2000, val_time: 21s 807ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.623056\n",
      "\u001b[32m2024-07-24T13:12:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 510/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6160, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6505, train/total_loss: 0.6160, train/total_loss/avg: 0.6505, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 510, iterations: 510, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 564ms, time_since_start: 15m 56s 532ms, eta: 36m 02s 549ms\n",
      "\u001b[32m2024-07-24T13:12:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 520/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6160, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6505, train/total_loss: 0.6160, train/total_loss/avg: 0.6505, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 520, iterations: 520, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 677ms, time_since_start: 16m 09s 209ms, eta: 33m 27s 543ms\n",
      "\u001b[32m2024-07-24T13:13:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 530/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6160, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6505, train/total_loss: 0.6160, train/total_loss/avg: 0.6505, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 5, num_updates: 530, iterations: 530, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 117ms, time_since_start: 16m 22s 327ms, eta: 34m 23s 305ms\n",
      "\u001b[32m2024-07-24T13:13:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 540/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6160, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6504, train/total_loss: 0.6160, train/total_loss/avg: 0.6504, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 540, iterations: 540, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 227ms, time_since_start: 16m 34s 555ms, eta: 31m 50s 205ms\n",
      "\u001b[32m2024-07-24T13:13:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6165, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6500, train/total_loss: 0.6165, train/total_loss/avg: 0.6500, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 550, iterations: 550, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 407ms, time_since_start: 16m 46s 962ms, eta: 32m 05s 059ms\n",
      "\u001b[32m2024-07-24T13:13:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 560/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6165, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6496, train/total_loss: 0.6165, train/total_loss/avg: 0.6496, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 560, iterations: 560, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 171ms, time_since_start: 17m 134ms, eta: 33m 49s 496ms\n",
      "\u001b[32m2024-07-24T13:13:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 570/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6492, train/total_loss: 0.6254, train/total_loss/avg: 0.6492, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 570, iterations: 570, max_updates: 2000, lr: 0.00006, ups: 0.83, time: 12s 346ms, time_since_start: 17m 12s 480ms, eta: 31m 29s 074ms\n",
      "\u001b[32m2024-07-24T13:14:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 580/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6480, train/total_loss: 0.6254, train/total_loss/avg: 0.6480, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 580, iterations: 580, max_updates: 2000, lr: 0.00006, ups: 0.77, time: 13s 339ms, time_since_start: 17m 25s 820ms, eta: 33m 46s 811ms\n",
      "\u001b[32m2024-07-24T13:14:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 590/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6193, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6475, train/total_loss: 0.6193, train/total_loss/avg: 0.6475, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 590, iterations: 590, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 516ms, time_since_start: 17m 38s 336ms, eta: 31m 28s 346ms\n",
      "\u001b[32m2024-07-24T13:14:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6472, train/total_loss: 0.6254, train/total_loss/avg: 0.6472, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 600, iterations: 600, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 503ms, time_since_start: 17m 51s 839ms, eta: 33m 42s 779ms\n",
      "\u001b[32m2024-07-24T13:14:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:14:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:14:42 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:14:42 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:14:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:14:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:14:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:14:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7273, val/total_loss: 0.7273, val/hateful_memes/accuracy: 0.5480, val/hateful_memes/binary_f1: 0.2614, val/hateful_memes/roc_auc: 0.6123, num_updates: 600, epoch: 6, iterations: 600, max_updates: 2000, val_time: 22s 787ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.623056\n",
      "\u001b[32m2024-07-24T13:15:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 610/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6463, train/total_loss: 0.6254, train/total_loss/avg: 0.6463, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 610, iterations: 610, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 898ms, time_since_start: 18m 27s 532ms, eta: 31m 58s 340ms\n",
      "\u001b[32m2024-07-24T13:15:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 620/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6261, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6462, train/total_loss: 0.6261, train/total_loss/avg: 0.6462, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 620, iterations: 620, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 569ms, time_since_start: 18m 41s 102ms, eta: 33m 23s 713ms\n",
      "\u001b[32m2024-07-24T13:15:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 630/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6261, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6452, train/total_loss: 0.6261, train/total_loss/avg: 0.6452, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 630, iterations: 630, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 573ms, time_since_start: 18m 53s 676ms, eta: 30m 43s 156ms\n",
      "\u001b[32m2024-07-24T13:15:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 640/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6261, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6453, train/total_loss: 0.6261, train/total_loss/avg: 0.6453, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 6, num_updates: 640, iterations: 640, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 485ms, time_since_start: 19m 06s 161ms, eta: 30m 16s 840ms\n",
      "\u001b[32m2024-07-24T13:16:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6261, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6442, train/total_loss: 0.6261, train/total_loss/avg: 0.6442, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 650, iterations: 650, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 115ms, time_since_start: 19m 18s 277ms, eta: 29m 10s 134ms\n",
      "\u001b[32m2024-07-24T13:16:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 660/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6263, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6440, train/total_loss: 0.6263, train/total_loss/avg: 0.6440, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 660, iterations: 660, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 809ms, time_since_start: 19m 32s 086ms, eta: 33m 041ms\n",
      "\u001b[32m2024-07-24T13:16:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 670/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6261, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6431, train/total_loss: 0.6261, train/total_loss/avg: 0.6431, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 670, iterations: 670, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 661ms, time_since_start: 19m 44s 747ms, eta: 30m 01s 795ms\n",
      "\u001b[32m2024-07-24T13:16:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 680/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6416, train/total_loss: 0.6254, train/total_loss/avg: 0.6416, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 680, iterations: 680, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 801ms, time_since_start: 19m 58s 549ms, eta: 32m 29s 377ms\n",
      "\u001b[32m2024-07-24T13:16:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 690/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6403, train/total_loss: 0.6254, train/total_loss/avg: 0.6403, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 690, iterations: 690, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 690ms, time_since_start: 20m 11s 240ms, eta: 29m 38s 819ms\n",
      "\u001b[32m2024-07-24T13:17:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6391, train/total_loss: 0.6254, train/total_loss/avg: 0.6391, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 700, iterations: 700, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 689ms, time_since_start: 20m 24s 929ms, eta: 31m 44s 174ms\n",
      "\u001b[32m2024-07-24T13:17:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:17:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:17:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:17:16 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:17:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:17:19 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:17:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:17:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:17:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7438, val/total_loss: 0.7438, val/hateful_memes/accuracy: 0.5820, val/hateful_memes/binary_f1: 0.4079, val/hateful_memes/roc_auc: 0.6531, num_updates: 700, epoch: 7, iterations: 700, max_updates: 2000, val_time: 33s 222ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.653120\n",
      "\u001b[32m2024-07-24T13:17:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 710/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6254, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6395, train/total_loss: 0.6254, train/total_loss/avg: 0.6395, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 710, iterations: 710, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 543ms, time_since_start: 21m 11s 702ms, eta: 31m 09s 372ms\n",
      "\u001b[32m2024-07-24T13:18:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 720/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6193, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6386, train/total_loss: 0.6193, train/total_loss/avg: 0.6386, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 720, iterations: 720, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 698ms, time_since_start: 21m 25s 401ms, eta: 31m 16s 148ms\n",
      "\u001b[32m2024-07-24T13:18:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 730/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.6040, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6381, train/total_loss: 0.6040, train/total_loss/avg: 0.6381, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 730, iterations: 730, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 786ms, time_since_start: 21m 38s 187ms, eta: 28m 57s 546ms\n",
      "\u001b[32m2024-07-24T13:18:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 740/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5939, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6372, train/total_loss: 0.5939, train/total_loss/avg: 0.6372, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 7, num_updates: 740, iterations: 740, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 623ms, time_since_start: 21m 51s 811ms, eta: 30m 36s 674ms\n",
      "\u001b[32m2024-07-24T13:18:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5838, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6360, train/total_loss: 0.5838, train/total_loss/avg: 0.6360, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 750, iterations: 750, max_updates: 2000, lr: 0.00005, ups: 0.91, time: 11s 854ms, time_since_start: 22m 03s 665ms, eta: 26m 25s 577ms\n",
      "\u001b[32m2024-07-24T13:18:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 760/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5838, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6354, train/total_loss: 0.5838, train/total_loss/avg: 0.6354, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 760, iterations: 760, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 017ms, time_since_start: 22m 16s 682ms, eta: 28m 47s 114ms\n",
      "\u001b[32m2024-07-24T13:19:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 770/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5829, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6346, train/total_loss: 0.5829, train/total_loss/avg: 0.6346, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 770, iterations: 770, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 403ms, time_since_start: 22m 30s 086ms, eta: 29m 23s 995ms\n",
      "\u001b[32m2024-07-24T13:19:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 780/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5829, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6338, train/total_loss: 0.5829, train/total_loss/avg: 0.6338, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 780, iterations: 780, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 852ms, time_since_start: 22m 42s 938ms, eta: 27m 57s 801ms\n",
      "\u001b[32m2024-07-24T13:19:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 790/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5768, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6322, train/total_loss: 0.5768, train/total_loss/avg: 0.6322, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 790, iterations: 790, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 461ms, time_since_start: 22m 56s 400ms, eta: 29m 02s 854ms\n",
      "\u001b[32m2024-07-24T13:19:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5768, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6318, train/total_loss: 0.5768, train/total_loss/avg: 0.6318, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 800, iterations: 800, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 057ms, time_since_start: 23m 09s 457ms, eta: 27m 56s 562ms\n",
      "\u001b[32m2024-07-24T13:19:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:19:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:19:59 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:19:59 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:20:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:20:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:20:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:20:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7178, val/total_loss: 0.7178, val/hateful_memes/accuracy: 0.5920, val/hateful_memes/binary_f1: 0.5660, val/hateful_memes/roc_auc: 0.6520, num_updates: 800, epoch: 8, iterations: 800, max_updates: 2000, val_time: 21s 903ms, best_update: 700, best_iteration: 700, best_val/hateful_memes/roc_auc: 0.653120\n",
      "\u001b[32m2024-07-24T13:20:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 810/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6311, train/total_loss: 0.5758, train/total_loss/avg: 0.6311, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 810, iterations: 810, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 736ms, time_since_start: 23m 45s 104ms, eta: 29m 09s 014ms\n",
      "\u001b[32m2024-07-24T13:20:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 820/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6307, train/total_loss: 0.5758, train/total_loss/avg: 0.6307, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 820, iterations: 820, max_updates: 2000, lr: 0.00005, ups: 0.83, time: 12s 818ms, time_since_start: 23m 57s 923ms, eta: 26m 58s 459ms\n",
      "\u001b[32m2024-07-24T13:20:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 830/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5758, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6303, train/total_loss: 0.5758, train/total_loss/avg: 0.6303, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 830, iterations: 830, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 489ms, time_since_start: 24m 11s 412ms, eta: 28m 08s 735ms\n",
      "\u001b[32m2024-07-24T13:21:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 840/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5751, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6293, train/total_loss: 0.5751, train/total_loss/avg: 0.6293, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 840, iterations: 840, max_updates: 2000, lr: 0.00005, ups: 0.77, time: 13s 043ms, time_since_start: 24m 24s 456ms, eta: 26m 59s 012ms\n",
      "\u001b[32m2024-07-24T13:21:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5734, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6284, train/total_loss: 0.5734, train/total_loss/avg: 0.6284, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 8, num_updates: 850, iterations: 850, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 354ms, time_since_start: 24m 37s 810ms, eta: 27m 23s 274ms\n",
      "\u001b[32m2024-07-24T13:21:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 860/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5705, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6275, train/total_loss: 0.5705, train/total_loss/avg: 0.6275, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 860, iterations: 860, max_updates: 2000, lr: 0.00004, ups: 0.91, time: 11s 556ms, time_since_start: 24m 49s 367ms, eta: 23m 29s 713ms\n",
      "\u001b[32m2024-07-24T13:21:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 870/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5704, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6264, train/total_loss: 0.5704, train/total_loss/avg: 0.6264, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 870, iterations: 870, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 990ms, time_since_start: 25m 02s 358ms, eta: 26m 10s 734ms\n",
      "\u001b[32m2024-07-24T13:21:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 880/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5704, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6254, train/total_loss: 0.5704, train/total_loss/avg: 0.6254, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 880, iterations: 880, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 905ms, time_since_start: 25m 15s 264ms, eta: 25m 46s 576ms\n",
      "\u001b[32m2024-07-24T13:22:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 890/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5704, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6247, train/total_loss: 0.5704, train/total_loss/avg: 0.6247, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 890, iterations: 890, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 991ms, time_since_start: 25m 28s 255ms, eta: 25m 43s 030ms\n",
      "\u001b[32m2024-07-24T13:22:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5704, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6239, train/total_loss: 0.5704, train/total_loss/avg: 0.6239, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 900, iterations: 900, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 166ms, time_since_start: 25m 41s 422ms, eta: 25m 49s 716ms\n",
      "\u001b[32m2024-07-24T13:22:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:22:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:22:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:22:32 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:22:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:22:36 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:22:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:22:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:22:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7499, val/total_loss: 0.7499, val/hateful_memes/accuracy: 0.5860, val/hateful_memes/binary_f1: 0.5012, val/hateful_memes/roc_auc: 0.6587, num_updates: 900, epoch: 9, iterations: 900, max_updates: 2000, val_time: 33s 512ms, best_update: 900, best_iteration: 900, best_val/hateful_memes/roc_auc: 0.658736\n",
      "\u001b[32m2024-07-24T13:23:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 910/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5692, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6222, train/total_loss: 0.5692, train/total_loss/avg: 0.6222, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 910, iterations: 910, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 349ms, time_since_start: 26m 28s 292ms, eta: 25m 56s 919ms\n",
      "\u001b[32m2024-07-24T13:23:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 920/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5514, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6213, train/total_loss: 0.5514, train/total_loss/avg: 0.6213, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 920, iterations: 920, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 125ms, time_since_start: 26m 41s 417ms, eta: 25m 16s 786ms\n",
      "\u001b[32m2024-07-24T13:23:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 930/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5513, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6203, train/total_loss: 0.5513, train/total_loss/avg: 0.6203, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 930, iterations: 930, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 970ms, time_since_start: 26m 54s 387ms, eta: 24m 44s 949ms\n",
      "\u001b[32m2024-07-24T13:23:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 940/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5513, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6197, train/total_loss: 0.5513, train/total_loss/avg: 0.6197, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 940, iterations: 940, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 010ms, time_since_start: 27m 07s 398ms, eta: 24m 35s 624ms\n",
      "\u001b[32m2024-07-24T13:24:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5514, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6191, train/total_loss: 0.5514, train/total_loss/avg: 0.6191, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 950, iterations: 950, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 012ms, time_since_start: 27m 20s 410ms, eta: 24m 21s 982ms\n",
      "\u001b[32m2024-07-24T13:24:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 960/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5513, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6181, train/total_loss: 0.5513, train/total_loss/avg: 0.6181, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 9, num_updates: 960, iterations: 960, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 775ms, time_since_start: 27m 33s 186ms, eta: 23m 41s 709ms\n",
      "\u001b[32m2024-07-24T13:24:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 970/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5444, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6165, train/total_loss: 0.5444, train/total_loss/avg: 0.6165, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 970, iterations: 970, max_updates: 2000, lr: 0.00004, ups: 0.91, time: 11s 774ms, time_since_start: 27m 44s 961ms, eta: 21m 37s 696ms\n",
      "\u001b[32m2024-07-24T13:24:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 980/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5441, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6152, train/total_loss: 0.5441, train/total_loss/avg: 0.6152, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 980, iterations: 980, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 228ms, time_since_start: 27m 58s 190ms, eta: 24m 03s 785ms\n",
      "\u001b[32m2024-07-24T13:24:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 990/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5441, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6140, train/total_loss: 0.5441, train/total_loss/avg: 0.6140, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 990, iterations: 990, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 848ms, time_since_start: 28m 11s 038ms, eta: 23m 08s 486ms\n",
      "\u001b[32m2024-07-24T13:25:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-24T13:25:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:25:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:25:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:25:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5441, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6134, train/total_loss: 0.5441, train/total_loss/avg: 0.6134, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1000, iterations: 1000, max_updates: 2000, lr: 0.00004, ups: 0.37, time: 27s 670ms, time_since_start: 28m 38s 709ms, eta: 49m 20s 767ms\n",
      "\u001b[32m2024-07-24T13:25:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:25:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:25:29 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:25:29 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:25:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:25:36 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:25:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:25:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:25:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7385, val/total_loss: 0.7385, val/hateful_memes/accuracy: 0.6060, val/hateful_memes/binary_f1: 0.5343, val/hateful_memes/roc_auc: 0.6612, num_updates: 1000, epoch: 10, iterations: 1000, max_updates: 2000, val_time: 33s 043ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.661152\n",
      "\u001b[32m2024-07-24T13:26:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1010/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5420, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6127, train/total_loss: 0.5420, train/total_loss/avg: 0.6127, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1010, iterations: 1010, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 047ms, time_since_start: 29m 24s 802ms, eta: 23m 02s 136ms\n",
      "\u001b[32m2024-07-24T13:26:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1020/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5385, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6119, train/total_loss: 0.5385, train/total_loss/avg: 0.6119, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1020, iterations: 1020, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 296ms, time_since_start: 29m 38s 099ms, eta: 23m 14s 317ms\n",
      "\u001b[32m2024-07-24T13:26:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1030/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5370, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6105, train/total_loss: 0.5370, train/total_loss/avg: 0.6105, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1030, iterations: 1030, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 905ms, time_since_start: 29m 51s 004ms, eta: 22m 19s 423ms\n",
      "\u001b[32m2024-07-24T13:26:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1040/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5370, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6103, train/total_loss: 0.5370, train/total_loss/avg: 0.6103, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1040, iterations: 1040, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 278ms, time_since_start: 30m 04s 282ms, eta: 22m 43s 950ms\n",
      "\u001b[32m2024-07-24T13:26:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5370, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6099, train/total_loss: 0.5370, train/total_loss/avg: 0.6099, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1050, iterations: 1050, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 756ms, time_since_start: 30m 17s 039ms, eta: 21m 36s 719ms\n",
      "\u001b[32m2024-07-24T13:27:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1060/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5297, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6090, train/total_loss: 0.5297, train/total_loss/avg: 0.6090, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1060, iterations: 1060, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 147ms, time_since_start: 30m 30s 186ms, eta: 22m 02s 331ms\n",
      "\u001b[32m2024-07-24T13:27:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1070/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5370, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6091, train/total_loss: 0.5370, train/total_loss/avg: 0.6091, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 10, num_updates: 1070, iterations: 1070, max_updates: 2000, lr: 0.00004, ups: 0.91, time: 11s 290ms, time_since_start: 30m 41s 477ms, eta: 18m 43s 507ms\n",
      "\u001b[32m2024-07-24T13:27:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1080/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5297, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6083, train/total_loss: 0.5297, train/total_loss/avg: 0.6083, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1080, iterations: 1080, max_updates: 2000, lr: 0.00004, ups: 0.77, time: 13s 804ms, time_since_start: 30m 55s 281ms, eta: 22m 38s 900ms\n",
      "\u001b[32m2024-07-24T13:27:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1090/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5297, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6076, train/total_loss: 0.5297, train/total_loss/avg: 0.6076, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1090, iterations: 1090, max_updates: 2000, lr: 0.00004, ups: 0.83, time: 12s 756ms, time_since_start: 31m 08s 037ms, eta: 20m 42s 100ms\n",
      "\u001b[32m2024-07-24T13:28:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5276, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6061, train/total_loss: 0.5276, train/total_loss/avg: 0.6061, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1100, iterations: 1100, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 992ms, time_since_start: 31m 22s 030ms, eta: 22m 27s 468ms\n",
      "\u001b[32m2024-07-24T13:28:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:28:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:28:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:28:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:28:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:28:16 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:28:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:28:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:28:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.7320, val/total_loss: 0.7320, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.5605, val/hateful_memes/roc_auc: 0.6633, num_updates: 1100, epoch: 11, iterations: 1100, max_updates: 2000, val_time: 32s 835ms, best_update: 1100, best_iteration: 1100, best_val/hateful_memes/roc_auc: 0.663296\n",
      "\u001b[32m2024-07-24T13:28:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1110/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5276, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6053, train/total_loss: 0.5276, train/total_loss/avg: 0.6053, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1110, iterations: 1110, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 312ms, time_since_start: 32m 08s 188ms, eta: 21m 07s 762ms\n",
      "\u001b[32m2024-07-24T13:29:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1120/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5251, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6045, train/total_loss: 0.5251, train/total_loss/avg: 0.6045, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1120, iterations: 1120, max_updates: 2000, lr: 0.00003, ups: 0.71, time: 14s 050ms, time_since_start: 32m 22s 239ms, eta: 22m 03s 028ms\n",
      "\u001b[32m2024-07-24T13:29:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1130/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5251, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6037, train/total_loss: 0.5251, train/total_loss/avg: 0.6037, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1130, iterations: 1130, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 809ms, time_since_start: 32m 35s 048ms, eta: 19m 52s 420ms\n",
      "\u001b[32m2024-07-24T13:29:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1140/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5251, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6030, train/total_loss: 0.5251, train/total_loss/avg: 0.6030, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1140, iterations: 1140, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 810ms, time_since_start: 32m 48s 858ms, eta: 21m 10s 815ms\n",
      "\u001b[32m2024-07-24T13:29:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5184, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6022, train/total_loss: 0.5184, train/total_loss/avg: 0.6022, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1150, iterations: 1150, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 705ms, time_since_start: 33m 01s 564ms, eta: 19m 15s 568ms\n",
      "\u001b[32m2024-07-24T13:29:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1160/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5184, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6018, train/total_loss: 0.5184, train/total_loss/avg: 0.6018, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1160, iterations: 1160, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 770ms, time_since_start: 33m 15s 334ms, eta: 20m 37s 662ms\n",
      "\u001b[32m2024-07-24T13:30:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1170/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5184, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6010, train/total_loss: 0.5184, train/total_loss/avg: 0.6010, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 11, num_updates: 1170, iterations: 1170, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 766ms, time_since_start: 33m 28s 100ms, eta: 18m 53s 750ms\n",
      "\u001b[32m2024-07-24T13:30:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1180/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5184, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5996, train/total_loss: 0.5184, train/total_loss/avg: 0.5996, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1180, iterations: 1180, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 335ms, time_since_start: 33m 40s 435ms, eta: 18m 02s 293ms\n",
      "\u001b[32m2024-07-24T13:30:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1190/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5184, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5985, train/total_loss: 0.5184, train/total_loss/avg: 0.5985, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1190, iterations: 1190, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 947ms, time_since_start: 33m 54s 383ms, eta: 20m 08s 822ms\n",
      "\u001b[32m2024-07-24T13:30:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5149, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5971, train/total_loss: 0.5149, train/total_loss/avg: 0.5971, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1200, iterations: 1200, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 333ms, time_since_start: 34m 07s 716ms, eta: 19m 01s 369ms\n",
      "\u001b[32m2024-07-24T13:30:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:30:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:30:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:30:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:30:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:31:02 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:31:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:31:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:31:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8504, val/total_loss: 0.8504, val/hateful_memes/accuracy: 0.5820, val/hateful_memes/binary_f1: 0.4367, val/hateful_memes/roc_auc: 0.6638, num_updates: 1200, epoch: 12, iterations: 1200, max_updates: 2000, val_time: 31s 566ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.663840\n",
      "\u001b[32m2024-07-24T13:31:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1210/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5115, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5957, train/total_loss: 0.5115, train/total_loss/avg: 0.5957, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1210, iterations: 1210, max_updates: 2000, lr: 0.00003, ups: 0.71, time: 14s 562ms, time_since_start: 34m 53s 854ms, eta: 20m 30s 946ms\n",
      "\u001b[32m2024-07-24T13:31:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1220/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5112, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5943, train/total_loss: 0.5112, train/total_loss/avg: 0.5943, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1220, iterations: 1220, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 345ms, time_since_start: 35m 07s 199ms, eta: 18m 33s 851ms\n",
      "\u001b[32m2024-07-24T13:32:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1230/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5112, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5933, train/total_loss: 0.5112, train/total_loss/avg: 0.5933, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1230, iterations: 1230, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 949ms, time_since_start: 35m 21s 149ms, eta: 19m 09s 324ms\n",
      "\u001b[32m2024-07-24T13:32:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1240/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5105, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5923, train/total_loss: 0.5105, train/total_loss/avg: 0.5923, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1240, iterations: 1240, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 462ms, time_since_start: 35m 34s 612ms, eta: 18m 14s 803ms\n",
      "\u001b[32m2024-07-24T13:32:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.5081, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5913, train/total_loss: 0.5081, train/total_loss/avg: 0.5913, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1250, iterations: 1250, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 916ms, time_since_start: 35m 48s 529ms, eta: 18m 36s 815ms\n",
      "\u001b[32m2024-07-24T13:32:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1260/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4936, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5905, train/total_loss: 0.4936, train/total_loss/avg: 0.5905, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1260, iterations: 1260, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 494ms, time_since_start: 36m 02s 023ms, eta: 17m 48s 483ms\n",
      "\u001b[32m2024-07-24T13:32:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1270/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4713, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5895, train/total_loss: 0.4713, train/total_loss/avg: 0.5895, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1270, iterations: 1270, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 893ms, time_since_start: 36m 15s 916ms, eta: 18m 05s 194ms\n",
      "\u001b[32m2024-07-24T13:33:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1280/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4697, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5885, train/total_loss: 0.4697, train/total_loss/avg: 0.5885, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 12, num_updates: 1280, iterations: 1280, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 258ms, time_since_start: 36m 29s 175ms, eta: 17m 01s 420ms\n",
      "\u001b[32m2024-07-24T13:33:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1290/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4695, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5872, train/total_loss: 0.4695, train/total_loss/avg: 0.5872, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1290, iterations: 1290, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 222ms, time_since_start: 36m 41s 397ms, eta: 15m 28s 547ms\n",
      "\u001b[32m2024-07-24T13:33:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4695, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5859, train/total_loss: 0.4695, train/total_loss/avg: 0.5859, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1300, iterations: 1300, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 923ms, time_since_start: 36m 54s 321ms, eta: 16m 07s 943ms\n",
      "\u001b[32m2024-07-24T13:33:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:33:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:33:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:33:45 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:33:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:33:48 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:34:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:34:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:34:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8187, val/total_loss: 0.8187, val/hateful_memes/accuracy: 0.5980, val/hateful_memes/binary_f1: 0.4885, val/hateful_memes/roc_auc: 0.6647, num_updates: 1300, epoch: 13, iterations: 1300, max_updates: 2000, val_time: 41s 224ms, best_update: 1300, best_iteration: 1300, best_val/hateful_memes/roc_auc: 0.664704\n",
      "\u001b[32m2024-07-24T13:34:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1310/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4682, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5848, train/total_loss: 0.4682, train/total_loss/avg: 0.5848, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1310, iterations: 1310, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 553ms, time_since_start: 37m 49s 106ms, eta: 16m 40s 658ms\n",
      "\u001b[32m2024-07-24T13:34:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1320/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4639, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5833, train/total_loss: 0.4639, train/total_loss/avg: 0.5833, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1320, iterations: 1320, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 805ms, time_since_start: 38m 01s 912ms, eta: 15m 31s 754ms\n",
      "\u001b[32m2024-07-24T13:34:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1330/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4639, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5825, train/total_loss: 0.4639, train/total_loss/avg: 0.5825, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1330, iterations: 1330, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 205ms, time_since_start: 38m 15s 117ms, eta: 15m 46s 703ms\n",
      "\u001b[32m2024-07-24T13:35:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1340/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4639, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5817, train/total_loss: 0.4639, train/total_loss/avg: 0.5817, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1340, iterations: 1340, max_updates: 2000, lr: 0.00003, ups: 0.83, time: 12s 712ms, time_since_start: 38m 27s 829ms, eta: 14m 57s 730ms\n",
      "\u001b[32m2024-07-24T13:35:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4595, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5808, train/total_loss: 0.4595, train/total_loss/avg: 0.5808, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1350, iterations: 1350, max_updates: 2000, lr: 0.00003, ups: 0.77, time: 13s 279ms, time_since_start: 38m 41s 108ms, eta: 15m 23s 560ms\n",
      "\u001b[32m2024-07-24T13:35:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1360/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4595, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5800, train/total_loss: 0.4595, train/total_loss/avg: 0.5800, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1360, iterations: 1360, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 766ms, time_since_start: 38m 53s 875ms, eta: 14m 34s 261ms\n",
      "\u001b[32m2024-07-24T13:35:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1370/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4576, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5790, train/total_loss: 0.4576, train/total_loss/avg: 0.5790, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1370, iterations: 1370, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 283ms, time_since_start: 39m 07s 158ms, eta: 14m 55s 428ms\n",
      "\u001b[32m2024-07-24T13:36:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1380/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4595, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5782, train/total_loss: 0.4595, train/total_loss/avg: 0.5782, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1380, iterations: 1380, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 749ms, time_since_start: 39m 19s 908ms, eta: 14m 05s 804ms\n",
      "\u001b[32m2024-07-24T13:36:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1390/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4576, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5770, train/total_loss: 0.4576, train/total_loss/avg: 0.5770, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 13, num_updates: 1390, iterations: 1390, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 551ms, time_since_start: 39m 32s 460ms, eta: 13m 39s 266ms\n",
      "\u001b[32m2024-07-24T13:36:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4576, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5760, train/total_loss: 0.4576, train/total_loss/avg: 0.5760, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1400, iterations: 1400, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 811ms, time_since_start: 39m 45s 271ms, eta: 13m 42s 481ms\n",
      "\u001b[32m2024-07-24T13:36:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:36:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:36:36 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:36:36 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:36:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:36:40 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:36:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:37:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:37:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8292, val/total_loss: 0.8292, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.5200, val/hateful_memes/roc_auc: 0.6734, num_updates: 1400, epoch: 14, iterations: 1400, max_updates: 2000, val_time: 32s 709ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.673392\n",
      "\u001b[32m2024-07-24T13:37:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1410/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4595, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5755, train/total_loss: 0.4595, train/total_loss/avg: 0.5755, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1410, iterations: 1410, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 594ms, time_since_start: 40m 30s 582ms, eta: 13m 15s 066ms\n",
      "\u001b[32m2024-07-24T13:37:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1420/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4595, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5744, train/total_loss: 0.4595, train/total_loss/avg: 0.5744, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1420, iterations: 1420, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 030ms, time_since_start: 40m 43s 613ms, eta: 13m 28s 685ms\n",
      "\u001b[32m2024-07-24T13:37:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1430/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4576, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5734, train/total_loss: 0.4576, train/total_loss/avg: 0.5734, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1430, iterations: 1430, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 338ms, time_since_start: 40m 55s 951ms, eta: 12m 32s 515ms\n",
      "\u001b[32m2024-07-24T13:37:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1440/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4463, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5722, train/total_loss: 0.4463, train/total_loss/avg: 0.5722, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1440, iterations: 1440, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 123ms, time_since_start: 41m 09s 075ms, eta: 13m 06s 358ms\n",
      "\u001b[32m2024-07-24T13:38:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4448, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5713, train/total_loss: 0.4448, train/total_loss/avg: 0.5713, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1450, iterations: 1450, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 316ms, time_since_start: 41m 21s 392ms, eta: 12m 04s 852ms\n",
      "\u001b[32m2024-07-24T13:38:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1460/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4420, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5702, train/total_loss: 0.4420, train/total_loss/avg: 0.5702, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1460, iterations: 1460, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 150ms, time_since_start: 41m 34s 542ms, eta: 12m 39s 834ms\n",
      "\u001b[32m2024-07-24T13:38:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1470/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4420, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5694, train/total_loss: 0.4420, train/total_loss/avg: 0.5694, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1470, iterations: 1470, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 314ms, time_since_start: 41m 46s 857ms, eta: 11m 38s 354ms\n",
      "\u001b[32m2024-07-24T13:38:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1480/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4348, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5685, train/total_loss: 0.4348, train/total_loss/avg: 0.5685, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1480, iterations: 1480, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 134ms, time_since_start: 41m 59s 991ms, eta: 12m 10s 784ms\n",
      "\u001b[32m2024-07-24T13:38:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1490/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4348, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5675, train/total_loss: 0.4348, train/total_loss/avg: 0.5675, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 14, num_updates: 1490, iterations: 1490, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 336ms, time_since_start: 42m 12s 328ms, eta: 11m 13s 227ms\n",
      "\u001b[32m2024-07-24T13:39:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4348, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5665, train/total_loss: 0.4348, train/total_loss/avg: 0.5665, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1500, iterations: 1500, max_updates: 2000, lr: 0.00002, ups: 0.91, time: 11s 593ms, time_since_start: 42m 23s 921ms, eta: 10m 20s 244ms\n",
      "\u001b[32m2024-07-24T13:39:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:39:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:39:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:39:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:39:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:39:18 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2024-07-24T13:39:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:39:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:39:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8263, val/total_loss: 0.8263, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.5026, val/hateful_memes/roc_auc: 0.6749, num_updates: 1500, epoch: 15, iterations: 1500, max_updates: 2000, val_time: 32s 284ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:39:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1510/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4348, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5658, train/total_loss: 0.4348, train/total_loss/avg: 0.5658, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1510, iterations: 1510, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 031ms, time_since_start: 43m 09s 246ms, eta: 11m 23s 262ms\n",
      "\u001b[32m2024-07-24T13:40:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1520/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4348, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5647, train/total_loss: 0.4348, train/total_loss/avg: 0.5647, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1520, iterations: 1520, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 838ms, time_since_start: 43m 23s 085ms, eta: 11m 50s 771ms\n",
      "\u001b[32m2024-07-24T13:40:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1530/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4325, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5637, train/total_loss: 0.4325, train/total_loss/avg: 0.5637, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1530, iterations: 1530, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 795ms, time_since_start: 43m 35s 880ms, eta: 10m 43s 496ms\n",
      "\u001b[32m2024-07-24T13:40:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1540/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4303, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5626, train/total_loss: 0.4303, train/total_loss/avg: 0.5626, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1540, iterations: 1540, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 207ms, time_since_start: 43m 49s 088ms, eta: 10m 50s 096ms\n",
      "\u001b[32m2024-07-24T13:40:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4226, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5614, train/total_loss: 0.4226, train/total_loss/avg: 0.5614, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1550, iterations: 1550, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 725ms, time_since_start: 44m 01s 814ms, eta: 10m 12s 755ms\n",
      "\u001b[32m2024-07-24T13:40:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1560/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4197, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5603, train/total_loss: 0.4197, train/total_loss/avg: 0.5603, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1560, iterations: 1560, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 191ms, time_since_start: 44m 15s 006ms, eta: 10m 21s 059ms\n",
      "\u001b[32m2024-07-24T13:41:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1570/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4189, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5593, train/total_loss: 0.4189, train/total_loss/avg: 0.5593, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1570, iterations: 1570, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 600ms, time_since_start: 44m 27s 607ms, eta: 09m 39s 756ms\n",
      "\u001b[32m2024-07-24T13:41:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1580/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4189, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5582, train/total_loss: 0.4189, train/total_loss/avg: 0.5582, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1580, iterations: 1580, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 082ms, time_since_start: 44m 40s 689ms, eta: 09m 47s 914ms\n",
      "\u001b[32m2024-07-24T13:41:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1590/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4189, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5574, train/total_loss: 0.4189, train/total_loss/avg: 0.5574, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1590, iterations: 1590, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 752ms, time_since_start: 44m 53s 441ms, eta: 09m 19s 448ms\n",
      "\u001b[32m2024-07-24T13:41:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4189, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5567, train/total_loss: 0.4189, train/total_loss/avg: 0.5567, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 15, num_updates: 1600, iterations: 1600, max_updates: 2000, lr: 0.00002, ups: 0.77, time: 13s 151ms, time_since_start: 45m 06s 592ms, eta: 09m 22s 864ms\n",
      "\u001b[32m2024-07-24T13:41:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:41:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:41:57 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:41:57 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:41:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:42:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:42:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:42:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8351, val/total_loss: 0.8351, val/hateful_memes/accuracy: 0.6200, val/hateful_memes/binary_f1: 0.5226, val/hateful_memes/roc_auc: 0.6653, num_updates: 1600, epoch: 15, iterations: 1600, max_updates: 2000, val_time: 22s 520ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:42:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1610/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4189, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5556, train/total_loss: 0.4189, train/total_loss/avg: 0.5556, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1610, iterations: 1610, max_updates: 2000, lr: 0.00002, ups: 0.83, time: 12s 975ms, time_since_start: 45m 42s 096ms, eta: 09m 01s 477ms\n",
      "\u001b[32m2024-07-24T13:42:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1620/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4104, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5543, train/total_loss: 0.4104, train/total_loss/avg: 0.5543, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1620, iterations: 1620, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 280ms, time_since_start: 45m 55s 376ms, eta: 08m 59s 976ms\n",
      "\u001b[32m2024-07-24T13:42:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1630/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4097, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5534, train/total_loss: 0.4097, train/total_loss/avg: 0.5534, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1630, iterations: 1630, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 819ms, time_since_start: 46m 09s 196ms, eta: 09m 07s 100ms\n",
      "\u001b[32m2024-07-24T13:43:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1640/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4097, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5525, train/total_loss: 0.4097, train/total_loss/avg: 0.5525, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1640, iterations: 1640, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 306ms, time_since_start: 46m 22s 502ms, eta: 08m 32s 556ms\n",
      "\u001b[32m2024-07-24T13:43:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4087, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5514, train/total_loss: 0.4087, train/total_loss/avg: 0.5514, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1650, iterations: 1650, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 907ms, time_since_start: 46m 36s 409ms, eta: 08m 40s 834ms\n",
      "\u001b[32m2024-07-24T13:43:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1660/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4087, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5506, train/total_loss: 0.4087, train/total_loss/avg: 0.5506, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1660, iterations: 1660, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 417ms, time_since_start: 46m 49s 827ms, eta: 08m 08s 136ms\n",
      "\u001b[32m2024-07-24T13:43:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1670/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4034, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5496, train/total_loss: 0.4034, train/total_loss/avg: 0.5496, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1670, iterations: 1670, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 850ms, time_since_start: 47m 03s 678ms, eta: 08m 09s 076ms\n",
      "\u001b[32m2024-07-24T13:43:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1680/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4034, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5488, train/total_loss: 0.4034, train/total_loss/avg: 0.5488, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1680, iterations: 1680, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 362ms, time_since_start: 47m 17s 041ms, eta: 07m 37s 543ms\n",
      "\u001b[32m2024-07-24T13:44:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1690/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4034, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5480, train/total_loss: 0.4034, train/total_loss/avg: 0.5480, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1690, iterations: 1690, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 897ms, time_since_start: 47m 30s 939ms, eta: 07m 40s 987ms\n",
      "\u001b[32m2024-07-24T13:44:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4034, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5472, train/total_loss: 0.4034, train/total_loss/avg: 0.5472, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1700, iterations: 1700, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 515ms, time_since_start: 47m 44s 454ms, eta: 07m 13s 832ms\n",
      "\u001b[32m2024-07-24T13:44:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:44:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:44:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:44:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:44:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:44:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:44:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:44:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8456, val/total_loss: 0.8456, val/hateful_memes/accuracy: 0.6280, val/hateful_memes/binary_f1: 0.5463, val/hateful_memes/roc_auc: 0.6729, num_updates: 1700, epoch: 16, iterations: 1700, max_updates: 2000, val_time: 21s 958ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:45:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1710/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4034, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5465, train/total_loss: 0.4034, train/total_loss/avg: 0.5465, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 16, num_updates: 1710, iterations: 1710, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 640ms, time_since_start: 48m 20s 060ms, eta: 07m 03s 257ms\n",
      "\u001b[32m2024-07-24T13:45:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1720/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5456, train/total_loss: 0.4056, train/total_loss/avg: 0.5456, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1720, iterations: 1720, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 809ms, time_since_start: 48m 32s 870ms, eta: 06m 23s 771ms\n",
      "\u001b[32m2024-07-24T13:45:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1730/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5450, train/total_loss: 0.4056, train/total_loss/avg: 0.5450, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1730, iterations: 1730, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 042ms, time_since_start: 48m 46s 913ms, eta: 06m 45s 698ms\n",
      "\u001b[32m2024-07-24T13:45:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1740/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5440, train/total_loss: 0.4056, train/total_loss/avg: 0.5440, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1740, iterations: 1740, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 152ms, time_since_start: 49m 065ms, eta: 06m 05s 905ms\n",
      "\u001b[32m2024-07-24T13:45:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5432, train/total_loss: 0.4056, train/total_loss/avg: 0.5432, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1750, iterations: 1750, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 133ms, time_since_start: 49m 14s 199ms, eta: 06m 18s 083ms\n",
      "\u001b[32m2024-07-24T13:46:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1760/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5421, train/total_loss: 0.4056, train/total_loss/avg: 0.5421, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1760, iterations: 1760, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 956ms, time_since_start: 49m 27s 156ms, eta: 05m 32s 734ms\n",
      "\u001b[32m2024-07-24T13:46:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1770/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5413, train/total_loss: 0.4056, train/total_loss/avg: 0.5413, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1770, iterations: 1770, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 993ms, time_since_start: 49m 41s 150ms, eta: 05m 44s 375ms\n",
      "\u001b[32m2024-07-24T13:46:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1780/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4084, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5406, train/total_loss: 0.4084, train/total_loss/avg: 0.5406, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1780, iterations: 1780, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 889ms, time_since_start: 49m 54s 039ms, eta: 05m 03s 425ms\n",
      "\u001b[32m2024-07-24T13:46:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1790/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4058, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5398, train/total_loss: 0.4058, train/total_loss/avg: 0.5398, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1790, iterations: 1790, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 367ms, time_since_start: 50m 08s 407ms, eta: 05m 22s 844ms\n",
      "\u001b[32m2024-07-24T13:47:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5389, train/total_loss: 0.4056, train/total_loss/avg: 0.5389, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1800, iterations: 1800, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 969ms, time_since_start: 50m 21s 377ms, eta: 04m 37s 553ms\n",
      "\u001b[32m2024-07-24T13:47:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:47:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:47:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:47:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:47:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:47:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:47:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:47:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8776, val/total_loss: 0.8776, val/hateful_memes/accuracy: 0.6260, val/hateful_memes/binary_f1: 0.5383, val/hateful_memes/roc_auc: 0.6558, num_updates: 1800, epoch: 17, iterations: 1800, max_updates: 2000, val_time: 23s 613ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:47:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1810/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4058, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5382, train/total_loss: 0.4058, train/total_loss/avg: 0.5382, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 17, num_updates: 1810, iterations: 1810, max_updates: 2000, lr: 0.00001, ups: 0.71, time: 14s 345ms, time_since_start: 50m 59s 344ms, eta: 04m 51s 645ms\n",
      "\u001b[32m2024-07-24T13:47:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1820/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4058, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5374, train/total_loss: 0.4058, train/total_loss/avg: 0.5374, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1820, iterations: 1820, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 825ms, time_since_start: 51m 12s 170ms, eta: 04m 07s 022ms\n",
      "\u001b[32m2024-07-24T13:48:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1830/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5365, train/total_loss: 0.4056, train/total_loss/avg: 0.5365, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1830, iterations: 1830, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 489ms, time_since_start: 51m 24s 660ms, eta: 03m 47s 191ms\n",
      "\u001b[32m2024-07-24T13:48:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1840/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4058, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5360, train/total_loss: 0.4058, train/total_loss/avg: 0.5360, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1840, iterations: 1840, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 859ms, time_since_start: 51m 38s 519ms, eta: 03m 57s 280ms\n",
      "\u001b[32m2024-07-24T13:48:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4058, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5351, train/total_loss: 0.4058, train/total_loss/avg: 0.5351, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1850, iterations: 1850, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 531ms, time_since_start: 51m 51s 051ms, eta: 03m 21s 136ms\n",
      "\u001b[32m2024-07-24T13:48:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1860/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5341, train/total_loss: 0.4056, train/total_loss/avg: 0.5341, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1860, iterations: 1860, max_updates: 2000, lr: 0.00001, ups: 0.77, time: 13s 712ms, time_since_start: 52m 04s 764ms, eta: 03m 25s 409ms\n",
      "\u001b[32m2024-07-24T13:48:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1870/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.4056, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5332, train/total_loss: 0.4056, train/total_loss/avg: 0.5332, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1870, iterations: 1870, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 12s 677ms, time_since_start: 52m 17s 442ms, eta: 02m 56s 351ms\n",
      "\u001b[32m2024-07-24T13:49:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1880/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3997, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5324, train/total_loss: 0.3997, train/total_loss/avg: 0.5324, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1880, iterations: 1880, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 773ms, time_since_start: 52m 31s 215ms, eta: 02m 56s 849ms\n",
      "\u001b[32m2024-07-24T13:49:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1890/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3945, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5314, train/total_loss: 0.3945, train/total_loss/avg: 0.5314, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1890, iterations: 1890, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 083ms, time_since_start: 52m 44s 299ms, eta: 02m 33s 997ms\n",
      "\u001b[32m2024-07-24T13:49:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3882, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5307, train/total_loss: 0.3882, train/total_loss/avg: 0.5307, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1900, iterations: 1900, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 717ms, time_since_start: 52m 58s 016ms, eta: 02m 26s 773ms\n",
      "\u001b[32m2024-07-24T13:49:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:49:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:49:49 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:49:49 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:49:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:49:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:49:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:49:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.9125, val/total_loss: 0.9125, val/hateful_memes/accuracy: 0.6120, val/hateful_memes/binary_f1: 0.4921, val/hateful_memes/roc_auc: 0.6482, num_updates: 1900, epoch: 18, iterations: 1900, max_updates: 2000, val_time: 16s 402ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:50:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1910/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3825, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5299, train/total_loss: 0.3825, train/total_loss/avg: 0.5299, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1910, iterations: 1910, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 454ms, time_since_start: 53m 27s 880ms, eta: 02m 09s 563ms\n",
      "\u001b[32m2024-07-24T13:50:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1920/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3786, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5291, train/total_loss: 0.3786, train/total_loss/avg: 0.5291, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 18, num_updates: 1920, iterations: 1920, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 747ms, time_since_start: 53m 41s 628ms, eta: 01m 57s 679ms\n",
      "\u001b[32m2024-07-24T13:50:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1930/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3783, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5282, train/total_loss: 0.3783, train/total_loss/avg: 0.5282, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1930, iterations: 1930, max_updates: 2000, lr: 0., ups: 0.91, time: 11s 345ms, time_since_start: 53m 52s 974ms, eta: 01m 24s 980ms\n",
      "\u001b[32m2024-07-24T13:50:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1940/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3773, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5273, train/total_loss: 0.3773, train/total_loss/avg: 0.5273, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1940, iterations: 1940, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 789ms, time_since_start: 54m 06s 764ms, eta: 01m 28s 531ms\n",
      "\u001b[32m2024-07-24T13:51:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3772, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5266, train/total_loss: 0.3772, train/total_loss/avg: 0.5266, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1950, iterations: 1950, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 832ms, time_since_start: 54m 19s 596ms, eta: 01m 08s 655ms\n",
      "\u001b[32m2024-07-24T13:51:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1960/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3772, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5258, train/total_loss: 0.3772, train/total_loss/avg: 0.5258, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1960, iterations: 1960, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 793ms, time_since_start: 54m 33s 390ms, eta: 59s 037ms\n",
      "\u001b[32m2024-07-24T13:51:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1970/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3752, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5249, train/total_loss: 0.3752, train/total_loss/avg: 0.5249, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1970, iterations: 1970, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 773ms, time_since_start: 54m 46s 164ms, eta: 41s 002ms\n",
      "\u001b[32m2024-07-24T13:51:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1980/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3752, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5243, train/total_loss: 0.3752, train/total_loss/avg: 0.5243, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1980, iterations: 1980, max_updates: 2000, lr: 0., ups: 0.77, time: 13s 636ms, time_since_start: 54m 59s 800ms, eta: 29s 181ms\n",
      "\u001b[32m2024-07-24T13:51:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1990/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3749, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5233, train/total_loss: 0.3749, train/total_loss/avg: 0.5233, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 1990, iterations: 1990, max_updates: 2000, lr: 0., ups: 0.83, time: 12s 712ms, time_since_start: 55m 12s 512ms, eta: 13s 602ms\n",
      "\u001b[32m2024-07-24T13:52:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2024-07-24T13:52:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:52:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:52:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:52:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, train/hateful_memes/label_smoothing_cross_entropy: 0.3709, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5225, train/total_loss: 0.3709, train/total_loss/avg: 0.5225, max mem: 13427.0, experiment: hateful_memes_lr7e5_lb.1, epoch: 19, num_updates: 2000, iterations: 2000, max_updates: 2000, lr: 0., ups: 0.37, time: 27s 653ms, time_since_start: 55m 40s 166ms, eta: 0ms\n",
      "\u001b[32m2024-07-24T13:52:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2024-07-24T13:52:22 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2024-07-24T13:52:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:52:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:52:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2024-07-24T13:52:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2024-07-24T13:52:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2024-07-24T13:52:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.9113, val/total_loss: 0.9113, val/hateful_memes/accuracy: 0.6280, val/hateful_memes/binary_f1: 0.5206, val/hateful_memes/roc_auc: 0.6488, num_updates: 2000, epoch: 19, iterations: 2000, max_updates: 2000, val_time: 27s 922ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.674880\n",
      "\u001b[32m2024-07-24T13:52:50 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2024-07-24T13:52:50 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2024-07-24T13:52:50 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2024-07-24T13:53:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2024-07-24T13:53:02 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1500\n",
      "\u001b[32m2024-07-24T13:53:02 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1500\n",
      "\u001b[32m2024-07-24T13:53:02 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 15\n",
      "\u001b[32m2024-07-24T13:53:04 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2024-07-24T13:53:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:08<00:00,  1.22s/it]\n",
      "\u001b[32m2024-07-24T13:53:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 7\n",
      "\u001b[32m2024-07-24T13:53:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-24T13:53:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, val/hateful_memes/label_smoothing_cross_entropy: 0.8263, val/total_loss: 0.8263, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.5026, val/hateful_memes/roc_auc: 0.6749\n",
      "\u001b[32m2024-07-24T13:53:13 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 56m 30s 908ms\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.147 MB of 0.147 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/label_smoothing_cross_entropy █▇█▇█▇▇█▇▆▇▇▇▅▆▆▆▅▅▄▅▅▅▃▃▃▄▄▂▃▂▂▂▂▂▂▃▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               train/learning_rate ▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                  train/total_loss █▇█▇█▇▇█▇▆▇▇▇▅▆▆▆▅▅▄▅▅▅▃▃▃▄▄▂▃▂▂▂▂▂▂▃▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        val/hateful_memes/accuracy ▁▁▃▆▅▄▅▆▆▇▇▅▆▇▇███▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       val/hateful_memes/binary_f1 ▁▁▅█▆▄▆█▇▇█▆▇▇▇▇██▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/label_smoothing_cross_entropy ▃▃▂▃▅▃▅▂▃▃▁▄▂▃▇▄▇▁█▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         val/hateful_memes/roc_auc ▁▁▃▅▅▅▇▇▇▇▇▇▇████▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    val/total_loss ▃▃▂▃▅▃▅▂▃▃▁▄▂▃▇▄▇▁█▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/hateful_memes/label_smoothing_cross_entropy 0.34889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                  train/total_loss 0.34889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               trainer/global_step 1500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        val/hateful_memes/accuracy 0.616\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       val/hateful_memes/binary_f1 0.50259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/hateful_memes/label_smoothing_cross_entropy 0.93743\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         val/hateful_memes/roc_auc 0.67488\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    val/total_loss 0.93743\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhateful_memes_lr7e5_lb.1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf/runs/o9ljzyvo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/vikrant17/mmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240724_125643-o9ljzyvo/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "Exception ignored in: <function WandbLogger.__del__ at 0x7feed7152f70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/teamspace/studios/this_studio/hateful-memes/mmf/utils/logger.py\", line 457, in __del__\n",
      "    self._wandb.finish()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 4262, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 390, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2100, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 2108, in _finish\n",
      "    tel.feature.finish = True\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
      "    self._run._telemetry_callback(self._obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 798, in _telemetry_callback\n",
      "    self._telemetry_flush()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 809, in _telemetry_flush\n",
      "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 101, in _publish_telemetry\n",
      "    self._publish(rec)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_roberta/defaults.yaml \\\n",
    "model=visual_roberta \\\n",
    "dataset=hateful_memes \\\n",
    "training.log_interval=10 \\\n",
    "training.seed=2024 \\\n",
    "training.batch_size=80 \\\n",
    "training.evaluation_interval=100 \\\n",
    "training.experiment_name=hateful_memes_lr7e5_lb.1 \\\n",
    "training.max_updates=2000 \\\n",
    "optimizer.params.lr=7e-5 \\\n",
    "training.fp16=True \\\n",
    "scheduler.params.num_warmup_steps=200 \\\n",
    "checkpoint.max_to_keep=1 \\\n",
    "training.wandb.enabled=True \\\n",
    "run_type=train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 22699.88 MB\n",
      "Allocated Memory: 0.00 MB\n",
      "Free Memory: 22699.88 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Ensure CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get CUDA device properties\n",
    "    device = torch.device('cuda')\n",
    "    cuda = torch.cuda.get_device_properties(device)\n",
    "    total_memory = cuda.total_memory / (1024 ** 2)  # Convert to MB\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "    free_memory = total_memory - allocated_memory  # Calculate free memory\n",
    "    \n",
    "    print(f\"Total Memory: {total_memory:.2f} MB\")\n",
    "    print(f\"Allocated Memory: {allocated_memory:.2f} MB\")\n",
    "    print(f\"Free Memory: {free_memory:.2f} MB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "\u001b[32m2024-07-17T22:42:08 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/defaults.yaml\n",
      "\u001b[32m2024-07-17T22:42:08 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2024-07-17T22:42:08 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "\u001b[32m2024-07-17T22:42:08 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2024-07-17T22:42:08 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes'])\n",
      "\u001b[32m2024-07-17T22:42:08 | mmf_cli.run: \u001b[0mTorch version: 1.11.0+cu102\n",
      "\u001b[32m2024-07-17T22:42:08 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA GeForce GTX 1650\n",
      "\u001b[32m2024-07-17T22:42:08 | mmf_cli.run: \u001b[0mUsing seed 8523002\n",
      "\u001b[32m2024-07-17T22:42:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/vikrant/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/vikrant/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/vikrant/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/vikrant/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/vikrant/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "\u001b[32m2024-07-17T22:42:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-17T22:42:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-17T22:42:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-17T22:42:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2024-07-17T22:42:11 | torch.distributed.nn.jit.instantiator: \u001b[0mCreated a temporary directory at /tmp/tmp2vjnw_8w\n",
      "\u001b[32m2024-07-17T22:42:11 | torch.distributed.nn.jit.instantiator: \u001b[0mWriting /tmp/tmp2vjnw_8w/_remote_module_non_sriptable.py\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bert_model_name\": \"bert-base-uncased\",\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model\": \"visual_bert\",\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 30522,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/vikrant/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-07-17T22:42:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2024-07-17T22:42:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2024-07-17T22:42:20 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2024-07-17T22:42:20 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2024-07-17T22:42:20 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2024-07-17T22:42:20 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vikrant/miniconda3/envs/mmf/bin/mmf_run\", line 8, in <module>\n",
      "    sys.exit(run())\n",
      "  File \"/home/vikrant/mmf/mmf_cli/run.py\", line 133, in run\n",
      "    main(configuration, predict=predict)\n",
      "  File \"/home/vikrant/mmf/mmf_cli/run.py\", line 56, in main\n",
      "    trainer.train()\n",
      "  File \"/home/vikrant/mmf/mmf/trainers/mmf_trainer.py\", line 145, in train\n",
      "    self.training_loop()\n",
      "  File \"/home/vikrant/mmf/mmf/trainers/core/training_loop.py\", line 33, in training_loop\n",
      "    self.run_training_epoch()\n",
      "  File \"/home/vikrant/mmf/mmf/trainers/core/training_loop.py\", line 91, in run_training_epoch\n",
      "    report = self.run_training_batch(batch, num_batches_for_this_update)\n",
      "  File \"/home/vikrant/mmf/mmf/trainers/core/training_loop.py\", line 166, in run_training_batch\n",
      "    report = self._forward(batch)\n",
      "  File \"/home/vikrant/mmf/mmf/trainers/core/training_loop.py\", line 200, in _forward\n",
      "    model_output = self.model(prepared_batch)\n",
      "  File \"/home/vikrant/mmf/mmf/models/base_model.py\", line 311, in __call__\n",
      "    model_output = super().__call__(sample_list, *args, **kwargs)\n",
      "  File \"/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/vikrant/mmf/mmf/models/visual_bert.py\", line 586, in forward\n",
      "    getattr_torchscriptable(sample_list, \"masked_lm_labels\", None),\n",
      "  File \"/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/vikrant/mmf/mmf/models/visual_bert.py\", line 366, in forward\n",
      "    image_text_alignment,\n",
      "  File \"/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/vikrant/mmf/mmf/models/visual_bert.py\", line 144, in forward\n",
      "    encoded_layers = self.encoder(embedding_output, extended_attention_mask)\n",
      "  File \"/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/vikrant/mmf/mmf/modules/hf_layers.py\", line 361, in forward\n",
      "    encoder_attention_mask,\n",
      "  File \"/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/vikrant/mmf/mmf/modules/hf_layers.py\", line 305, in forward\n",
      "    hidden_states, attention_mask, head_mask\n",
      "  File \"/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/vikrant/mmf/mmf/modules/hf_layers.py\", line 269, in forward\n",
      "    encoder_attention_mask,\n",
      "  File \"/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/vikrant/mmf/mmf/modules/hf_layers.py\", line 229, in forward\n",
      "    context_layer = torch.matmul(attention_probs, value_layer)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 9.46 GiB already allocated; 0 bytes free; 9.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/defaults.yaml \\\n",
    "model=visual_bert \\\n",
    "dataset=hateful_memes \\\n",
    "# checkpoint.resume_pretrained=true \\\n",
    "# checkpoint.resume_zoo=visual_bert.pretrained.coco \\\n",
    "run_type=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "`val/hateful_memes/cross_entropy: 0.7202, val/total_loss: 0.7202, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.4683`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a pretrained visual bert COCO tuned on hateful memes dataset with configuration provided in paper as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/defaults.yaml\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'checkpoint.resume_pretrained=False', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco', 'run_type=val'])\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf_cli.run: \u001b[0mTorch version: 1.11.0+cu102\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA GeForce GTX 1650\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf_cli.run: \u001b[0mUsing seed 1217446\n",
      "\u001b[32m2024-07-17T22:39:00 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/vikrant/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/vikrant/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/vikrant/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/vikrant/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/vikrant/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "\u001b[32m2024-07-17T22:39:05 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-17T22:39:05 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-17T22:39:05 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-17T22:39:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2024-07-17T22:39:05 | torch.distributed.nn.jit.instantiator: \u001b[0mCreated a temporary directory at /tmp/tmp7uph9ldh\n",
      "\u001b[32m2024-07-17T22:39:05 | torch.distributed.nn.jit.instantiator: \u001b[0mWriting /tmp/tmp7uph9ldh/_remote_module_non_sriptable.py\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bert_model_name\": \"bert-base-uncased\",\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model\": \"visual_bert\",\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 30522,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/vikrant/.cache/torch/mmf/distributed_-1/tmpn4eka66k\n",
      "Downloading: 100%|███████████████████████████| 440M/440M [00:31<00:00, 13.9MB/s]\n",
      "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /home/vikrant/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "creating metadata file for /home/vikrant/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/vikrant/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-07-17T22:39:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2024-07-17T22:39:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2024-07-17T22:39:58 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.finetuned.hateful_memes_from_coco.tar.gz to /home/vikrant/.cache/torch/mmf/data/models/visual_bert.finetuned.hateful_memes.from_coco/visual_bert.finetuned.hateful_memes_from_coco.tar.gz ]\n",
      "Downloading visual_bert.finetuned.hateful_memes_from_coco.tar.gz: 100%|█| 414M/4     \n",
      "[ Starting checksum for visual_bert.finetuned.hateful_memes_from_coco.tar.gz]\n",
      "[ Checksum successful for visual_bert.finetuned.hateful_memes_from_coco.tar.gz]\n",
      "Unpacking visual_bert.finetuned.hateful_memes_from_coco.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-17T22:40:47 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-17T22:40:47 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-17T22:40:47 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-17T22:40:47 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-17T22:40:48 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
      "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
      "Unexpected keys if any: []\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-17T22:40:48 | py.warnings: \u001b[0m/home/vikrant/mmf/mmf/utils/checkpoint.py:345: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-17T22:40:48 | py.warnings: \u001b[0m/home/vikrant/mmf/mmf/utils/checkpoint.py:345: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-17T22:40:48 | py.warnings: \u001b[0m/home/vikrant/mmf/mmf/utils/checkpoint.py:394: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-17T22:40:48 | py.warnings: \u001b[0m/home/vikrant/mmf/mmf/utils/checkpoint.py:394: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2024-07-17T22:40:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2024-07-17T22:40:48 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2024-07-17T22:40:48 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2024-07-17T22:40:48 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2024-07-17T22:40:48 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2024-07-17T22:40:48 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2024-07-17T22:40:48 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2024-07-17T22:40:48 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2024-07-17T22:40:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:21<00:00,  2.68s/it]\n",
      "\u001b[32m2024-07-17T22:41:09 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 8\n",
      "\u001b[32m2024-07-17T22:41:09 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-17T22:41:11 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 1.7758, val/total_loss: 1.7758, val/hateful_memes/accuracy: 0.6180, val/hateful_memes/binary_f1: 0.4852, val/hateful_memes/roc_auc: 0.7196\n",
      "\u001b[32m2024-07-17T22:41:11 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01m 12s 687ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/defaults.yaml \\\n",
    "model=visual_bert \\\n",
    "dataset=hateful_memes \\\n",
    "checkpoint.resume_pretrained=False \\\n",
    "checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco \\\n",
    "run_type=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/defaults.yaml\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/home/vikrant/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco', 'run_type=val'])\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf_cli.run: \u001b[0mTorch version: 1.11.0+cu102\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA GeForce GTX 1650\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf_cli.run: \u001b[0mUsing seed 16634648\n",
      "\u001b[32m2024-07-16T17:37:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/vikrant/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/vikrant/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/vikrant/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/vikrant/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/vikrant/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "\u001b[32m2024-07-16T17:37:19 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-16T17:37:19 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-16T17:37:19 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2024-07-16T17:37:19 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2024-07-16T17:37:19 | torch.distributed.nn.jit.instantiator: \u001b[0mCreated a temporary directory at /tmp/tmp4omg8bxg\n",
      "\u001b[32m2024-07-16T17:37:19 | torch.distributed.nn.jit.instantiator: \u001b[0mWriting /tmp/tmp4omg8bxg/_remote_module_non_sriptable.py\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bert_model_name\": \"bert-base-uncased\",\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model\": \"visual_bert\",\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 30522,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/vikrant/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.bias', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-07-16T17:37:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2024-07-16T17:37:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2024-07-16T17:37:27 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-16T17:37:28 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-16T17:37:28 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-16T17:37:28 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-07-16T17:37:28 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2024-07-16T17:37:28 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:16<00:00,  2.05s/it]\n",
      "\u001b[32m2024-07-16T17:37:44 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 8\n",
      "\u001b[32m2024-07-16T17:37:44 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2024-07-16T17:37:46 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 0.7218, val/total_loss: 0.7218, val/hateful_memes/accuracy: 0.5700, val/hateful_memes/binary_f1: 0.3768, val/hateful_memes/roc_auc: 0.5288\n",
      "\u001b[32m2024-07-16T17:37:46 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 19s 192ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/defaults.yaml model=visual_bert dataset=hateful_memes checkpoint.resume_pretrained=True checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco run_type=val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
